[
  {
    "objectID": "In-class_Ex/In-class_Ex08.html",
    "href": "In-class_Ex/In-class_Ex08.html",
    "title": "Hands on Exercise 9",
    "section": "",
    "text": "Geographically weighted regression (GWR) is a spatial statistical technique that takes non-stationary variables into consideration (e.g., climate; demographic factors; physical environment characteristics) and models the local relationships between these independent variables and an outcome of interest (also known as dependent variable). In this hands-on exercise, you will learn how to build hedonic pricing models by using GWR methods. The dependent variable is the resale prices of condominium in 2015. The independent variables are divided into either structural and locational.\n\n\n\nTwo data sets will be used in this model building exercise, they are:\n\nURA Master Plan subzone boundary in shapefile format (i.e. MP14_SUBZONE_WEB_PL)\ncondo_resale_2015 in csv format (i.e. condo_resale_2015.csv)\n\n\n\n\nBefore we get started, it is important for us to install the necessary R packages into R and launch these R packages into R environment.\nThe R packages needed for this exercise are as follows:\n\nR package for building OLS and performing diagnostics tests\n\nolsrr\n\nR package for calibrating geographical weighted family of models\n\nGWmodel\n\nR package for multivariate data visualisation and analysis\n\ncorrplot\n\nSpatial data handling\n\nsf\n\nAttribute data handling\n\ntidyverse, especially readr, ggplot2 and dplyr\n\nChoropleth mapping\n\ntmap\n\n\nThe code chunks below installs and launches these R packages into R environment.\n\npacman::p_load(dplyr, olsrr, corrplot, cowplot, sf, spdep, GWmodel, tmap, tidyverse, gtsummary, readr, ggpubr)\n\n\n\n\nGWmodel package provides a collection of localised spatial statistical methods, namely: GW summary statistics, GW principal components analysis, GW discriminant analysis and various forms of GW regression; some of which are provided in basic and robust (outlier resistant) forms. Commonly, outputs or parameters of the GWmodel are mapped to provide a useful exploratory tool, which can often precede (and direct) a more traditional or sophisticated statistical analysis.\n\n\n\n\n\nThe geospatial data used in this hands-on exercise is called MP14_SUBZONE_WEB_PL. It is in ESRI shapefile format. The shapefile consists of URA Master Plan 2014’s planning subzone boundaries. Polygon features are used to represent these geographic boundaries. The GIS data is in svy21 projected coordinates systems.\nThe code chunk below is used to import MP_SUBZONE_WEB_PL shapefile by using st_read() of sf packages.\n\nmpsz = st_read(dsn = \"data/geospatial\", layer = \"MP14_SUBZONE_WEB_PL\")\n\nReading layer `MP14_SUBZONE_WEB_PL' from data source \n  `C:\\Users\\kwekm\\Desktop\\SMU Year 3 Semester 2\\IS415 Geospatial Analytics and Applications\\KMRCrazyDuck\\IS415-KMR\\In-class_Ex\\Data\\Geospatial' \n  using driver `ESRI Shapefile'\nSimple feature collection with 323 features and 15 fields\nGeometry type: MULTIPOLYGON\nDimension:     XY\nBounding box:  xmin: 2667.538 ymin: 15748.72 xmax: 56396.44 ymax: 50256.33\nProjected CRS: SVY21\n\n\nThe report above shows that the R object used to contain the imported MP14_SUBZONE_WEB_PL shapefile is called mpsz and it is a simple feature object. The geometry type is multipolygon. it is also important to note that mpsz simple feature object does not have EPSG information.\n\n\n\nThe code chunk below updates the newly imported mpsz with the correct ESPG code (i.e. 3414)\n\nmpsz_svy21 <- st_transform(mpsz, 3414)\n\nAfter transforming the projection metadata, you can varify the projection of the newly transformed mpsz_svy21 by using st_crs() of sf package.\nThe code chunk below will be used to varify the newly transformed mpsz_svy21.\n\nst_crs(mpsz_svy21)\n\nCoordinate Reference System:\n  User input: EPSG:3414 \n  wkt:\nPROJCRS[\"SVY21 / Singapore TM\",\n    BASEGEOGCRS[\"SVY21\",\n        DATUM[\"SVY21\",\n            ELLIPSOID[\"WGS 84\",6378137,298.257223563,\n                LENGTHUNIT[\"metre\",1]]],\n        PRIMEM[\"Greenwich\",0,\n            ANGLEUNIT[\"degree\",0.0174532925199433]],\n        ID[\"EPSG\",4757]],\n    CONVERSION[\"Singapore Transverse Mercator\",\n        METHOD[\"Transverse Mercator\",\n            ID[\"EPSG\",9807]],\n        PARAMETER[\"Latitude of natural origin\",1.36666666666667,\n            ANGLEUNIT[\"degree\",0.0174532925199433],\n            ID[\"EPSG\",8801]],\n        PARAMETER[\"Longitude of natural origin\",103.833333333333,\n            ANGLEUNIT[\"degree\",0.0174532925199433],\n            ID[\"EPSG\",8802]],\n        PARAMETER[\"Scale factor at natural origin\",1,\n            SCALEUNIT[\"unity\",1],\n            ID[\"EPSG\",8805]],\n        PARAMETER[\"False easting\",28001.642,\n            LENGTHUNIT[\"metre\",1],\n            ID[\"EPSG\",8806]],\n        PARAMETER[\"False northing\",38744.572,\n            LENGTHUNIT[\"metre\",1],\n            ID[\"EPSG\",8807]]],\n    CS[Cartesian,2],\n        AXIS[\"northing (N)\",north,\n            ORDER[1],\n            LENGTHUNIT[\"metre\",1]],\n        AXIS[\"easting (E)\",east,\n            ORDER[2],\n            LENGTHUNIT[\"metre\",1]],\n    USAGE[\n        SCOPE[\"Cadastre, engineering survey, topographic mapping.\"],\n        AREA[\"Singapore - onshore and offshore.\"],\n        BBOX[1.13,103.59,1.47,104.07]],\n    ID[\"EPSG\",3414]]\n\n\nNotice that the EPSG: is indicated as 3414 now.\nNext, you will reveal the extent of mpsz_svy21 by using st_bbox() of sf package\n\n\n\n\n\n\nThe condo_resale_2015 is in csv file format. The codes chunk below uses read_csv() function of readr package to import condo_resale_2015 into R as a tibble data frame called condo_resale.\n\ncondo_resale = read_csv(\"data/aspatial/Condo_resale_2015.csv\")\n\nAfter importing the data file into R, it is important for us to examine if the data file has been imported correctly.\nThe codes chunks below uses glimpse() to display the data structure of will do the job.\n\nglimpse(condo_resale)\n\nRows: 1,436\nColumns: 23\n$ LATITUDE             <dbl> 1.287145, 1.328698, 1.313727, 1.308563, 1.321437,…\n$ LONGITUDE            <dbl> 103.7802, 103.8123, 103.7971, 103.8247, 103.9505,…\n$ POSTCODE             <dbl> 118635, 288420, 267833, 258380, 467169, 466472, 3…\n$ SELLING_PRICE        <dbl> 3000000, 3880000, 3325000, 4250000, 1400000, 1320…\n$ AREA_SQM             <dbl> 309, 290, 248, 127, 145, 139, 218, 141, 165, 168,…\n$ AGE                  <dbl> 30, 32, 33, 7, 28, 22, 24, 24, 27, 31, 17, 22, 6,…\n$ PROX_CBD             <dbl> 7.941259, 6.609797, 6.898000, 4.038861, 11.783402…\n$ PROX_CHILDCARE       <dbl> 0.16597932, 0.28027246, 0.42922669, 0.39473543, 0…\n$ PROX_ELDERLYCARE     <dbl> 2.5198118, 1.9333338, 0.5021395, 1.9910316, 1.121…\n$ PROX_URA_GROWTH_AREA <dbl> 6.618741, 7.505109, 6.463887, 4.906512, 6.410632,…\n$ PROX_HAWKER_MARKET   <dbl> 1.76542207, 0.54507614, 0.37789301, 1.68259969, 0…\n$ PROX_KINDERGARTEN    <dbl> 0.05835552, 0.61592412, 0.14120309, 0.38200076, 0…\n$ PROX_MRT             <dbl> 0.5607188, 0.6584461, 0.3053433, 0.6910183, 0.528…\n$ PROX_PARK            <dbl> 1.1710446, 0.1992269, 0.2779886, 0.9832843, 0.116…\n$ PROX_PRIMARY_SCH     <dbl> 1.6340256, 0.9747834, 1.4715016, 1.4546324, 0.709…\n$ PROX_TOP_PRIMARY_SCH <dbl> 3.3273195, 0.9747834, 1.4715016, 2.3006394, 0.709…\n$ PROX_SHOPPING_MALL   <dbl> 2.2102717, 2.9374279, 1.2256850, 0.3525671, 1.307…\n$ PROX_SUPERMARKET     <dbl> 0.9103958, 0.5900617, 0.4135583, 0.4162219, 0.581…\n$ PROX_BUS_STOP        <dbl> 0.10336166, 0.28673408, 0.28504777, 0.29872340, 0…\n$ NO_Of_UNITS          <dbl> 18, 20, 27, 30, 30, 31, 32, 32, 32, 32, 34, 34, 3…\n$ FAMILY_FRIENDLY      <dbl> 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0…\n$ FREEHOLD             <dbl> 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1…\n$ LEASEHOLD_99YR       <dbl> 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0…\n\n\n\nhead(condo_resale$LONGITUDE) #see the data in XCOORD column\n\n[1] 103.7802 103.8123 103.7971 103.8247 103.9505 103.9386\n\n\n\nhead(condo_resale$LATITUDE) #see the data in YCOORD column\n\n[1] 1.287145 1.328698 1.313727 1.308563 1.321437 1.314198\n\n\nNext, summary() of base R is used to display the summary statistics of condo_resale tibble data frame.\n\nsummary(condo_resale)\n\n    LATITUDE       LONGITUDE        POSTCODE      SELLING_PRICE     \n Min.   :1.240   Min.   :103.7   Min.   : 18965   Min.   :  540000  \n 1st Qu.:1.309   1st Qu.:103.8   1st Qu.:259849   1st Qu.: 1100000  \n Median :1.328   Median :103.8   Median :469298   Median : 1383222  \n Mean   :1.334   Mean   :103.8   Mean   :440439   Mean   : 1751211  \n 3rd Qu.:1.357   3rd Qu.:103.9   3rd Qu.:589486   3rd Qu.: 1950000  \n Max.   :1.454   Max.   :104.0   Max.   :828833   Max.   :18000000  \n    AREA_SQM          AGE           PROX_CBD       PROX_CHILDCARE    \n Min.   : 34.0   Min.   : 0.00   Min.   : 0.3869   Min.   :0.004927  \n 1st Qu.:103.0   1st Qu.: 5.00   1st Qu.: 5.5574   1st Qu.:0.174481  \n Median :121.0   Median :11.00   Median : 9.3567   Median :0.258135  \n Mean   :136.5   Mean   :12.14   Mean   : 9.3254   Mean   :0.326313  \n 3rd Qu.:156.0   3rd Qu.:18.00   3rd Qu.:12.6661   3rd Qu.:0.368293  \n Max.   :619.0   Max.   :37.00   Max.   :19.1804   Max.   :3.465726  \n PROX_ELDERLYCARE  PROX_URA_GROWTH_AREA PROX_HAWKER_MARKET PROX_KINDERGARTEN \n Min.   :0.05451   Min.   :0.2145       Min.   :0.05182    Min.   :0.004927  \n 1st Qu.:0.61254   1st Qu.:3.1643       1st Qu.:0.55245    1st Qu.:0.276345  \n Median :0.94179   Median :4.6186       Median :0.90842    Median :0.413385  \n Mean   :1.05351   Mean   :4.5981       Mean   :1.27987    Mean   :0.458903  \n 3rd Qu.:1.35122   3rd Qu.:5.7550       3rd Qu.:1.68578    3rd Qu.:0.578474  \n Max.   :3.94916   Max.   :9.1554       Max.   :5.37435    Max.   :2.229045  \n    PROX_MRT         PROX_PARK       PROX_PRIMARY_SCH  PROX_TOP_PRIMARY_SCH\n Min.   :0.05278   Min.   :0.02906   Min.   :0.07711   Min.   :0.07711     \n 1st Qu.:0.34646   1st Qu.:0.26211   1st Qu.:0.44024   1st Qu.:1.34451     \n Median :0.57430   Median :0.39926   Median :0.63505   Median :1.88213     \n Mean   :0.67316   Mean   :0.49802   Mean   :0.75471   Mean   :2.27347     \n 3rd Qu.:0.84844   3rd Qu.:0.65592   3rd Qu.:0.95104   3rd Qu.:2.90954     \n Max.   :3.48037   Max.   :2.16105   Max.   :3.92899   Max.   :6.74819     \n PROX_SHOPPING_MALL PROX_SUPERMARKET PROX_BUS_STOP       NO_Of_UNITS    \n Min.   :0.0000     Min.   :0.0000   Min.   :0.001595   Min.   :  18.0  \n 1st Qu.:0.5258     1st Qu.:0.3695   1st Qu.:0.098356   1st Qu.: 188.8  \n Median :0.9357     Median :0.5687   Median :0.151710   Median : 360.0  \n Mean   :1.0455     Mean   :0.6141   Mean   :0.193974   Mean   : 409.2  \n 3rd Qu.:1.3994     3rd Qu.:0.7862   3rd Qu.:0.220466   3rd Qu.: 590.0  \n Max.   :3.4774     Max.   :2.2441   Max.   :2.476639   Max.   :1703.0  \n FAMILY_FRIENDLY     FREEHOLD      LEASEHOLD_99YR  \n Min.   :0.0000   Min.   :0.0000   Min.   :0.0000  \n 1st Qu.:0.0000   1st Qu.:0.0000   1st Qu.:0.0000  \n Median :0.0000   Median :0.0000   Median :0.0000  \n Mean   :0.4868   Mean   :0.4227   Mean   :0.4882  \n 3rd Qu.:1.0000   3rd Qu.:1.0000   3rd Qu.:1.0000  \n Max.   :1.0000   Max.   :1.0000   Max.   :1.0000  \n\n\n\n\n\nCurrently, the condo_resale tibble data frame is Aspatial. We will convert it to a sf object. The code chunk below converts condo_resale data frame into a simple feature data frame by using st_as_sf() of sf packages.\n\ncondo_resale.sf <- st_as_sf(condo_resale,\n                            coords = c(\"LONGITUDE\", \"LATITUDE\"),\n                            crs=4326) %>%\n  st_transform(crs=3414)\n\nNotice that st_transform() of sf package is used to convert the coordinates from wgs84 (i.e. crs:4326) to svy21 (i.e. crs=3414).\nNext, head() is used to list the content of condo_resale.sf object.\n\nhead(condo_resale.sf)\n\nSimple feature collection with 6 features and 21 fields\nGeometry type: POINT\nDimension:     XY\nBounding box:  xmin: 22085.12 ymin: 29951.54 xmax: 41042.56 ymax: 34546.2\nProjected CRS: SVY21 / Singapore TM\n# A tibble: 6 × 22\n  POSTCODE SELLI…¹ AREA_…²   AGE PROX_…³ PROX_…⁴ PROX_…⁵ PROX_…⁶ PROX_…⁷ PROX_…⁸\n     <dbl>   <dbl>   <dbl> <dbl>   <dbl>   <dbl>   <dbl>   <dbl>   <dbl>   <dbl>\n1   118635 3000000     309    30    7.94   0.166   2.52     6.62   1.77   0.0584\n2   288420 3880000     290    32    6.61   0.280   1.93     7.51   0.545  0.616 \n3   267833 3325000     248    33    6.90   0.429   0.502    6.46   0.378  0.141 \n4   258380 4250000     127     7    4.04   0.395   1.99     4.91   1.68   0.382 \n5   467169 1400000     145    28   11.8    0.119   1.12     6.41   0.565  0.461 \n6   466472 1320000     139    22   10.3    0.125   0.789    5.09   0.781  0.0994\n# … with 12 more variables: PROX_MRT <dbl>, PROX_PARK <dbl>,\n#   PROX_PRIMARY_SCH <dbl>, PROX_TOP_PRIMARY_SCH <dbl>,\n#   PROX_SHOPPING_MALL <dbl>, PROX_SUPERMARKET <dbl>, PROX_BUS_STOP <dbl>,\n#   NO_Of_UNITS <dbl>, FAMILY_FRIENDLY <dbl>, FREEHOLD <dbl>,\n#   LEASEHOLD_99YR <dbl>, geometry <POINT [m]>, and abbreviated variable names\n#   ¹​SELLING_PRICE, ²​AREA_SQM, ³​PROX_CBD, ⁴​PROX_CHILDCARE, ⁵​PROX_ELDERLYCARE,\n#   ⁶​PROX_URA_GROWTH_AREA, ⁷​PROX_HAWKER_MARKET, ⁸​PROX_KINDERGARTEN\n\n\nNotice that the output is in point feature data frame.\n\n\n\n\nIn the section, you will learn how to use statistical graphics functions of ggplot2 package to perform EDA.\n\n\nWe can plot the distribution of SELLING_PRICE by using appropriate Exploratory Data Analysis (EDA) as shown in the code chunk below.\n\nggplot(data=condo_resale.sf, aes(x=`SELLING_PRICE`)) +\n  geom_histogram(bins=20, color=\"black\", fill=\"light blue\")\n\n\n\n\nThe figure above reveals a right skewed distribution. This means that more condominium units were transacted at relative lower prices.\nStatistically, the skewed dsitribution can be normalised by using log transformation. The code chunk below is used to derive a new variable called LOG_SELLING_PRICE by using a log transformation on the variable SELLING_PRICE. It is performed using mutate() of dplyr package.\n\ncondo_resale.sf <- condo_resale.sf %>%\n  mutate(`LOG_SELLING_PRICE` = log(SELLING_PRICE))\n\nNow, you can plot the LOG_SELLING_PRICE using the code chunk below.\n\nggplot(data=condo_resale.sf, aes(x=`LOG_SELLING_PRICE`)) +\n  geom_histogram(bins=20, color=\"black\", fill=\"light blue\")\n\n\n\n\nNotice that the distribution is relatively less skewed after the transformation.\n\n\n\nIn this section, you will learn how to draw a small multiple histograms (also known as trellis plot) by using ggarrange() of ggpubr package.\nThe code chunk below is used to create 12 histograms. Then, ggarrange() is used to organised these histogram into a 3 columns by 4 rows small multiple plot.\n\nAREA_SQM <- ggplot(data=condo_resale.sf, aes(x= `AREA_SQM`)) + \n  geom_histogram(bins=20, color=\"black\", fill=\"light blue\")\n\nAGE <- ggplot(data=condo_resale.sf, aes(x= `AGE`)) +\n  geom_histogram(bins=20, color=\"black\", fill=\"light blue\")\n\nPROX_CBD <- ggplot(data=condo_resale.sf, aes(x= `PROX_CBD`)) +\n  geom_histogram(bins=20, color=\"black\", fill=\"light blue\")\n\nPROX_CHILDCARE <- ggplot(data=condo_resale.sf, aes(x= `PROX_CHILDCARE`)) + \n  geom_histogram(bins=20, color=\"black\", fill=\"light blue\")\n\nPROX_ELDERLYCARE <- ggplot(data=condo_resale.sf, aes(x= `PROX_ELDERLYCARE`)) +\n  geom_histogram(bins=20, color=\"black\", fill=\"light blue\")\n\nPROX_URA_GROWTH_AREA <- ggplot(data=condo_resale.sf, \n                               aes(x= `PROX_URA_GROWTH_AREA`)) +\n  geom_histogram(bins=20, color=\"black\", fill=\"light blue\")\n\nPROX_HAWKER_MARKET <- ggplot(data=condo_resale.sf, aes(x= `PROX_HAWKER_MARKET`)) +\n  geom_histogram(bins=20, color=\"black\", fill=\"light blue\")\n\nPROX_KINDERGARTEN <- ggplot(data=condo_resale.sf, aes(x= `PROX_KINDERGARTEN`)) +\n  geom_histogram(bins=20, color=\"black\", fill=\"light blue\")\n\nPROX_MRT <- ggplot(data=condo_resale.sf, aes(x= `PROX_MRT`)) +\n  geom_histogram(bins=20, color=\"black\", fill=\"light blue\")\n\nPROX_PARK <- ggplot(data=condo_resale.sf, aes(x= `PROX_PARK`)) +\n  geom_histogram(bins=20, color=\"black\", fill=\"light blue\")\n\nPROX_PRIMARY_SCH <- ggplot(data=condo_resale.sf, aes(x= `PROX_PRIMARY_SCH`)) +\n  geom_histogram(bins=20, color=\"black\", fill=\"light blue\")\n\nPROX_TOP_PRIMARY_SCH <- ggplot(data=condo_resale.sf, \n                               aes(x= `PROX_TOP_PRIMARY_SCH`)) +\n  geom_histogram(bins=20, color=\"black\", fill=\"light blue\")\n\nggarrange(AREA_SQM, AGE, PROX_CBD, PROX_CHILDCARE, PROX_ELDERLYCARE,\n          PROX_URA_GROWTH_AREA, PROX_HAWKER_MARKET, PROX_KINDERGARTEN, PROX_MRT,\n          PROX_PARK, PROX_PRIMARY_SCH, PROX_TOP_PRIMARY_SCH,\n          ncol = 3, nrow = 4)\n\n\n\n\n\n\n\nLastly, we want to reveal the geospatial distribution condominium resale prices in Singapore. The map will be prepared by using tmap package.\nFirst, we will turn on the interactive mode of tmap by using the code chunk below\n\ntmap_mode(\"view\")\n\nNext, the code chunks below is used to create an interactive point symbol map.\n\nsf_use_s2(FALSE)\n\ntm_shape(mpsz_svy21)+\n  tm_polygons() +\ntm_shape(condo_resale.sf) +  \n  tm_dots(col = \"SELLING_PRICE\",\n          alpha = 0.6,\n          style=\"quantile\") +\n  tm_view(set.zoom.limits = c(11,14))\n\n\n\n\n\n\n\ntmap_mode(\"plot\")"
  },
  {
    "objectID": "In-class_Ex/In-class_Ex02.html",
    "href": "In-class_Ex/In-class_Ex02.html",
    "title": "In-class Exercise 2: Geospatial Data Wrangling",
    "section": "",
    "text": "Water is an important resource to mankind. Clean and accessible water is critical to human health. It provides a healthy environment, a sustainable economy, reduces poverty and ensures peace and security. Yet over 40% of the global population does not have access to sufficient clean water. By 2025, 1.8 billion people will be living in countries or regions with absolute water scarcity, according to UN-Water. The lack of water poses a major threat to several sectors, including food security. Agriculture uses about 70% of the world’s accessible freshwater.\nDeveloping countries are most affected by water shortages and poor water quality. Up to 80% of illnesses in the developing world are linked to inadequate water and sanitation. Despite technological advancement, providing clean water to the rural community is still a major development issues in many countries globally, especially countries in the Africa continent.\nTo address the issue of providing clean and sustainable water supply to the rural community, a global Water Point Data Exchange (WPdx) project has been initiated. The main aim of this initiative is to collect water point related data from rural areas at the water point or small water scheme level and share the data via WPdx Data Repository, a cloud-based data library. What is so special of this project is that data are collected based on WPDx Data Standard.\n\n\nGeospatial analytics hold tremendous potential to address complex problems facing society. In this study, you are tasked to apply appropriate geospatial data wrangling methods to prepare the data for water point mapping study. For the purpose of this study, Nigeria will be used as the study country.\n\n\n\nFor the purpose of this assignment, data from WPdx Global Data Repositories will be used. There are two versions of the data. They are: WPdx-Basic and WPdx+. You are required to use WPdx+ data set.\n\n\n\nNigeria Level-2 Administrative Boundary (also known as Local Government Area) polygon features GIS data will be used in this take-home exercise. The data can be downloaded either from The Humanitarian Data Exchange portal or geoBoundaries.\n\n\n\nThe specific tasks of this take-home exercise are as follows:\n\nUsing appropriate sf method, import the shapefile into R and save it in a simple feature data frame format. Note that there are three Projected Coordinate Systems of Nigeria, they are: EPSG: 26391, 26392, and 26303. You can use any one of them.\nUsing appropriate tidyr and dplyr methods, derive the proportion of functional and non-functional water point at LGA level.\nCombining the geospatial and aspatial data frame into simple feature data frame.\nVisualising the distribution of water point by using appropriate analytical visualisation methods."
  },
  {
    "objectID": "In-class_Ex/In-class_Ex02.html#installing-and-loading-packages",
    "href": "In-class_Ex/In-class_Ex02.html#installing-and-loading-packages",
    "title": "In-class Exercise 2: Geospatial Data Wrangling",
    "section": "2.1 Installing and Loading Packages",
    "text": "2.1 Installing and Loading Packages\nFirstly, the code below will check if pacman has been installed. If it has not been installed, R will download and install it, before activating it for use during this session.\n\nif (!require('pacman', character.only = T)){\n  install.packages('pacman')\n}\nlibrary('pacman')\n\nNext, pacman assists us by helping us load R packages that we require, sf, tidyverse and funModeling.\n\npacman::p_load(sf, tidyverse, funModeling, readr, dplyr, knitr, tidyr)"
  },
  {
    "objectID": "In-class_Ex/In-class_Ex02.html#geoboundaries-nigeria-level-2-administrative-boundary-dataset",
    "href": "In-class_Ex/In-class_Ex02.html#geoboundaries-nigeria-level-2-administrative-boundary-dataset",
    "title": "In-class Exercise 2: Geospatial Data Wrangling",
    "section": "2.2 geoBoundaries Nigeria Level-2 Administrative Boundary Dataset",
    "text": "2.2 geoBoundaries Nigeria Level-2 Administrative Boundary Dataset\n\n2.2.1 Importing geoBoundaries Nigeria Level-2 Administrative Boundary Dataset\nIn the code below, dsn specifies the filepath where the dataset is located and layer provides the filename of the dataset excluding the file extension.\n\ngbnigeria = st_read(dsn = \"data/Geospatial\", layer = \"geoBoundaries-NGA-ADM2\")\n\nReading layer `geoBoundaries-NGA-ADM2' from data source \n  `C:\\Users\\kwekm\\Desktop\\SMU Year 3 Semester 2\\IS415 Geospatial Analytics and Applications\\KMRCrazyDuck\\IS415-KMR\\In-class_Ex\\Data\\Geospatial' \n  using driver `ESRI Shapefile'\nSimple feature collection with 774 features and 5 fields\nGeometry type: MULTIPOLYGON\nDimension:     XY\nBounding box:  xmin: 2.668534 ymin: 4.273007 xmax: 14.67882 ymax: 13.89442\nGeodetic CRS:  WGS 84\n\n\nFrom the above message, it tells us that the dataset contains multipolygon features, containing 774 multipolygon features and 5 fields in the gbnigeria simple feature data frame and is in the WGS84 geographic coordinates system.\nLet us check the other dataset from Humanitarian data exchange.\n\nnigeria = st_read(dsn = \"data/Geospatial\", layer = \"nga_admbnda_adm2_osgof_20190417\")\n\nReading layer `nga_admbnda_adm2_osgof_20190417' from data source \n  `C:\\Users\\kwekm\\Desktop\\SMU Year 3 Semester 2\\IS415 Geospatial Analytics and Applications\\KMRCrazyDuck\\IS415-KMR\\In-class_Ex\\Data\\Geospatial' \n  using driver `ESRI Shapefile'\nSimple feature collection with 774 features and 16 fields\nGeometry type: MULTIPOLYGON\nDimension:     XY\nBounding box:  xmin: 2.668534 ymin: 4.273007 xmax: 14.67882 ymax: 13.89442\nGeodetic CRS:  WGS 84\n\n\nFrom the above message, it tells us that the dataset contains multipolygon features, containing 774 multipolygon features and 16 fields in the gbnigeria simple feature data frame and is in the WGS84 geographic coordinates system.\nBy comparing both datasets, the dataset from Humanitarian Data Exchange is more favourable as we can tell which state the LGA area belongs too which will be beneficial for our analysis\n\n\n2.2.2 Checking the Coordinate Reference System\nIn the code below, we will check if the Coordinate Reference System has been specified correctly.\n\nst_crs(nigeria)\n\nCoordinate Reference System:\n  User input: WGS 84 \n  wkt:\nGEOGCRS[\"WGS 84\",\n    DATUM[\"World Geodetic System 1984\",\n        ELLIPSOID[\"WGS 84\",6378137,298.257223563,\n            LENGTHUNIT[\"metre\",1]]],\n    PRIMEM[\"Greenwich\",0,\n        ANGLEUNIT[\"degree\",0.0174532925199433]],\n    CS[ellipsoidal,2],\n        AXIS[\"latitude\",north,\n            ORDER[1],\n            ANGLEUNIT[\"degree\",0.0174532925199433]],\n        AXIS[\"longitude\",east,\n            ORDER[2],\n            ANGLEUNIT[\"degree\",0.0174532925199433]],\n    ID[\"EPSG\",4326]]\n\n\nAs seen above, the file has been configured correctly, having a WGS84 Geographic Coordinate System which maps to EPSG:4326.\n\n\n2.2.3 Converting the Coordinate Reference System\nIn the code below, we will convert the Geographic Coordinate Reference System from WGS84 to EPSG:26391 Projected Coordinate System.\n\nnigeria26391 <- st_transform(nigeria, crs = 26391)\n\n\nst_crs(nigeria26391)\n\nCoordinate Reference System:\n  User input: EPSG:26391 \n  wkt:\nPROJCRS[\"Minna / Nigeria West Belt\",\n    BASEGEOGCRS[\"Minna\",\n        DATUM[\"Minna\",\n            ELLIPSOID[\"Clarke 1880 (RGS)\",6378249.145,293.465,\n                LENGTHUNIT[\"metre\",1]]],\n        PRIMEM[\"Greenwich\",0,\n            ANGLEUNIT[\"degree\",0.0174532925199433]],\n        ID[\"EPSG\",4263]],\n    CONVERSION[\"Nigeria West Belt\",\n        METHOD[\"Transverse Mercator\",\n            ID[\"EPSG\",9807]],\n        PARAMETER[\"Latitude of natural origin\",4,\n            ANGLEUNIT[\"degree\",0.0174532925199433],\n            ID[\"EPSG\",8801]],\n        PARAMETER[\"Longitude of natural origin\",4.5,\n            ANGLEUNIT[\"degree\",0.0174532925199433],\n            ID[\"EPSG\",8802]],\n        PARAMETER[\"Scale factor at natural origin\",0.99975,\n            SCALEUNIT[\"unity\",1],\n            ID[\"EPSG\",8805]],\n        PARAMETER[\"False easting\",230738.26,\n            LENGTHUNIT[\"metre\",1],\n            ID[\"EPSG\",8806]],\n        PARAMETER[\"False northing\",0,\n            LENGTHUNIT[\"metre\",1],\n            ID[\"EPSG\",8807]]],\n    CS[Cartesian,2],\n        AXIS[\"(E)\",east,\n            ORDER[1],\n            LENGTHUNIT[\"metre\",1]],\n        AXIS[\"(N)\",north,\n            ORDER[2],\n            LENGTHUNIT[\"metre\",1]],\n    USAGE[\n        SCOPE[\"Engineering survey, topographic mapping.\"],\n        AREA[\"Nigeria - onshore west of 6°30'E, onshore and offshore shelf.\"],\n        BBOX[3.57,2.69,13.9,6.5]],\n    ID[\"EPSG\",26391]]\n\n\nAfter running the code, we can confirm that the data frame has been converted to EPSG:26391 Projected Coordinate System."
  },
  {
    "objectID": "In-class_Ex/In-class_Ex02.html#wpdx-aspatial-data",
    "href": "In-class_Ex/In-class_Ex02.html#wpdx-aspatial-data",
    "title": "In-class Exercise 2: Geospatial Data Wrangling",
    "section": "2.3 WPdx + Aspatial Data",
    "text": "2.3 WPdx + Aspatial Data\n\n2.3.1 Importing WPdx + Aspatial Data\nSince WPdx+ data set is in csv format, we will use read_csv() of readr package to import Water_Point_Data_Exchange_-_PlusWPdx.csv and output it to an R object called wpdx.\n\nwpdx <- read_csv(\"Data/Aspatial/Water_Point_Data_Exchange.csv\") %>%\n  filter(`#clean_country_name` == \"Nigeria\")\n\n\nlist(wpdx)\n\n[[1]]\n# A tibble: 95,008 × 70\n   row_id `#source`      #lat_…¹ #lon_…² #repo…³ #stat…⁴ #wate…⁵ #wate…⁶ #wate…⁷\n    <dbl> <chr>            <dbl>   <dbl> <chr>   <chr>   <chr>   <chr>   <chr>  \n 1 429068 GRID3             7.98    5.12 08/29/… Unknown <NA>    <NA>    Tapsta…\n 2 222071 Federal Minis…    6.96    3.60 08/16/… Yes     Boreho… Well    Mechan…\n 3 160612 WaterAid          6.49    7.93 12/04/… Yes     Boreho… Well    Hand P…\n 4 160669 WaterAid          6.73    7.65 12/04/… Yes     Boreho… Well    <NA>   \n 5 160642 WaterAid          6.78    7.66 12/04/… Yes     Boreho… Well    Hand P…\n 6 160628 WaterAid          6.96    7.78 12/04/… Yes     Boreho… Well    Hand P…\n 7 160632 WaterAid          7.02    7.84 12/04/… Yes     Boreho… Well    Hand P…\n 8 642747 Living Water …    7.33    8.98 10/03/… Yes     Boreho… Well    Mechan…\n 9 642456 Living Water …    7.17    9.11 10/03/… Yes     Boreho… Well    Hand P…\n10 641347 Living Water …    7.20    9.22 03/28/… Yes     Boreho… Well    Hand P…\n# … with 94,998 more rows, 61 more variables: `#water_tech_category` <chr>,\n#   `#facility_type` <chr>, `#clean_country_name` <chr>, `#clean_adm1` <chr>,\n#   `#clean_adm2` <chr>, `#clean_adm3` <chr>, `#clean_adm4` <chr>,\n#   `#install_year` <dbl>, `#installer` <chr>, `#rehab_year` <lgl>,\n#   `#rehabilitator` <lgl>, `#management_clean` <chr>, `#status_clean` <chr>,\n#   `#pay` <chr>, `#fecal_coliform_presence` <chr>,\n#   `#fecal_coliform_value` <dbl>, `#subjective_quality` <chr>, …\n\n\nOur output shows our wpdx tibble data frame consists of 97,478 rows and 74 columns. The useful fields we would be paying attention to is the #lat_deg and #lon_deg columns, which are in the decimal degree format. By viewing the Data Standard on wpdx’s website, we know that the latitude and longitude is in the wgs84 Geographic Coordinate System.\n\n\n2.3.2 Creating a Simple Feature Data Frame from an Aspatial Data Frame\nAs the geometry is available in wkt in the column New Georeferenced Column, we can use st_as_sfc() to import the geomtry\n\nwpdx$Geometry <- st_as_sfc(wpdx$`New Georeferenced Column`)\n\nAs there is no spatial data information, firstly, we assign the original projection when converting the tibble dataframe to sf. The original is wgs84 which is EPSG:4326.\n\nwpdx_sf <- st_sf(wpdx, crs=4326)\nwpdx_sf\n\nSimple feature collection with 95008 features and 70 fields\nGeometry type: POINT\nDimension:     XY\nBounding box:  xmin: 2.707441 ymin: 4.301812 xmax: 14.21828 ymax: 13.86568\nGeodetic CRS:  WGS 84\n# A tibble: 95,008 × 71\n   row_id `#source`      #lat_…¹ #lon_…² #repo…³ #stat…⁴ #wate…⁵ #wate…⁶ #wate…⁷\n *  <dbl> <chr>            <dbl>   <dbl> <chr>   <chr>   <chr>   <chr>   <chr>  \n 1 429068 GRID3             7.98    5.12 08/29/… Unknown <NA>    <NA>    Tapsta…\n 2 222071 Federal Minis…    6.96    3.60 08/16/… Yes     Boreho… Well    Mechan…\n 3 160612 WaterAid          6.49    7.93 12/04/… Yes     Boreho… Well    Hand P…\n 4 160669 WaterAid          6.73    7.65 12/04/… Yes     Boreho… Well    <NA>   \n 5 160642 WaterAid          6.78    7.66 12/04/… Yes     Boreho… Well    Hand P…\n 6 160628 WaterAid          6.96    7.78 12/04/… Yes     Boreho… Well    Hand P…\n 7 160632 WaterAid          7.02    7.84 12/04/… Yes     Boreho… Well    Hand P…\n 8 642747 Living Water …    7.33    8.98 10/03/… Yes     Boreho… Well    Mechan…\n 9 642456 Living Water …    7.17    9.11 10/03/… Yes     Boreho… Well    Hand P…\n10 641347 Living Water …    7.20    9.22 03/28/… Yes     Boreho… Well    Hand P…\n# … with 94,998 more rows, 62 more variables: `#water_tech_category` <chr>,\n#   `#facility_type` <chr>, `#clean_country_name` <chr>, `#clean_adm1` <chr>,\n#   `#clean_adm2` <chr>, `#clean_adm3` <chr>, `#clean_adm4` <chr>,\n#   `#install_year` <dbl>, `#installer` <chr>, `#rehab_year` <lgl>,\n#   `#rehabilitator` <lgl>, `#management_clean` <chr>, `#status_clean` <chr>,\n#   `#pay` <chr>, `#fecal_coliform_presence` <chr>,\n#   `#fecal_coliform_value` <dbl>, `#subjective_quality` <chr>, …\n\n\nNext, we then convert the projection to the appropriate decimal based projection system.\n\nwpdx_sf <- wpdx_sf %>%\n  st_transform(crs = 26391)"
  },
  {
    "objectID": "In-class_Ex/In-class_Ex02.html#excluding-redundant-fields",
    "href": "In-class_Ex/In-class_Ex02.html#excluding-redundant-fields",
    "title": "In-class Exercise 2: Geospatial Data Wrangling",
    "section": "3.1 Excluding Redundant Fields",
    "text": "3.1 Excluding Redundant Fields\nAs the wpdx sf dataframe consist of many redundant field, we use select() to select the fields which we want to retain.\n\nnigeria26391 <- nigeria26391 %>%\n  select(c(3:4, 8:9))"
  },
  {
    "objectID": "In-class_Ex/In-class_Ex02.html#checking-for-duplicate-name",
    "href": "In-class_Ex/In-class_Ex02.html#checking-for-duplicate-name",
    "title": "In-class Exercise 2: Geospatial Data Wrangling",
    "section": "3.2 Checking for Duplicate Name",
    "text": "3.2 Checking for Duplicate Name\nIt is important to check for duplicate name in the data main data fields. Using duplicated(), we can flag out LGA names that might be duplicated as shown below:\n\nnigeria26391$ADM2_EN[duplicated(nigeria26391$ADM2_EN) == TRUE]\n\n[1] \"Bassa\"    \"Ifelodun\" \"Irepodun\" \"Nasarawa\" \"Obi\"      \"Surulere\"\n\n\nTo reduce duplication of LGA names, we will put the state names behind to make it more specific.\n\nnigeria26391$ADM2_EN[94] <- \"Bassa, Kogi\"\nnigeria26391$ADM2_EN[95] <- \"Bassa, Plateau\"\nnigeria26391$ADM2_EN[304] <- \"Ifelodun, Kwara\"\nnigeria26391$ADM2_EN[305] <- \"Ifelodun, Osun\"\nnigeria26391$ADM2_EN[355] <- \"Irepodun, Kwara\"\nnigeria26391$ADM2_EN[356] <- \"Ireopodun, Osun\"\nnigeria26391$ADM2_EN[519] <- \"Nasarawa, Kano\"\nnigeria26391$ADM2_EN[520] <- \"Nasarawa, Nasarawa\"\nnigeria26391$ADM2_EN[546] <- \"Obi, Benue\"\nnigeria26391$ADM2_EN[547] <- \"Obi, Nasarawa\"\nnigeria26391$ADM2_EN[693] <- \"Surulere, Lagos\"\nnigeria26391$ADM2_EN[694] <- \"Surulere, Oyo\"\n\nLet us check now if the duplication has been resolved.\n\nnigeria26391$ADM2_EN[duplicated(nigeria26391$ADM2_EN) == TRUE]\n\ncharacter(0)"
  },
  {
    "objectID": "In-class_Ex/In-class_Ex02.html#understanding-field-names",
    "href": "In-class_Ex/In-class_Ex02.html#understanding-field-names",
    "title": "In-class Exercise 2: Geospatial Data Wrangling",
    "section": "4.1 Understanding Field Names",
    "text": "4.1 Understanding Field Names\nFirst, let us have a look at the #status_clean column which stores the information about Functional and Non-Functional data points. The code below returns all values that were used in the column.\n\nfreq(data = wpdx_sf,\n     input = '#status_clean')\n\n\n\n\n                     #status_clean frequency percentage cumulative_perc\n1                       Functional     45883      48.29           48.29\n2                   Non-Functional     29385      30.93           79.22\n3                             <NA>     10656      11.22           90.44\n4      Functional but needs repair      4579       4.82           95.26\n5 Non-Functional due to dry season      2403       2.53           97.79\n6        Functional but not in use      1686       1.77           99.56\n7         Abandoned/Decommissioned       234       0.25           99.81\n8                        Abandoned       175       0.18           99.99\n9 Non functional due to dry season         7       0.01          100.00\n\n\nAs there might be issues performing mathematical calculations with NA labels, we will rename them to unknown.\nThe code below renames the column #status_clean to status_clean, select only the status_clean for manipulation and then replace all na values to unknown.\n\nwpdx_sf_nga <- wpdx_sf %>%\n  rename(status_clean = '#status_clean') %>%\n  select(status_clean) %>%\n  mutate(status_clean = replace_na(status_clean, \"unknown\"))"
  },
  {
    "objectID": "In-class_Ex/In-class_Ex02.html#filtering-data",
    "href": "In-class_Ex/In-class_Ex02.html#filtering-data",
    "title": "In-class Exercise 2: Geospatial Data Wrangling",
    "section": "4.2 Filtering Data",
    "text": "4.2 Filtering Data\nWith our previous knowledge, we can filter the data to obtain functional proportion counts in each LGA level. We will filter the wpdx_sf_nga dataframes to option functional and non-functional water points.\n\nwpdx_func <- wpdx_sf_nga %>% \n  filter(status_clean %in% \n           c(\"Functional\", \n             \"Functional but not in use\", \n             \"Functional but needs repair\"))\nwpdx_nonfunc <- wpdx_sf_nga %>% \n  filter(status_clean %in%\n          c(\"Abadoned/Decommissioned\", \n            \"Abandoned\",\n            \"Non-Functional due to dry season\",\n            \"Non-Functional\",\n            \"Non functional due to dry season\"))\nwpdx_unknown <- wpdx_sf_nga %>%\n  filter(status_clean == \"unknown\")"
  },
  {
    "objectID": "In-class_Ex/In-class_Ex02.html#point-in-polygon-count",
    "href": "In-class_Ex/In-class_Ex02.html#point-in-polygon-count",
    "title": "In-class Exercise 2: Geospatial Data Wrangling",
    "section": "4.3 Point-in-polygon Count",
    "text": "4.3 Point-in-polygon Count\nUtilising st_intersects() of sf package and lengths, we check where each data point for the water point which fall inside each LGA. We do each calculation separation so we can cross check later to ensure all the values sum to the same total.\n\nnigeria_wp <- nigeria26391 %>%\n  mutate(`total_wp` = lengths(\n    st_intersects(nigeria26391, wpdx_sf_nga))) %>%\n  mutate(`wp_functional` = lengths(\n    st_intersects(nigeria26391, wpdx_func))) %>%\n  mutate(`wp_nonfunctional` = lengths(\n    st_intersects(nigeria26391, wpdx_nonfunc))) %>%\n  mutate(`wp_unknown` = lengths(\n    st_intersects(nigeria26391, wpdx_unknown)))"
  },
  {
    "objectID": "In-class_Ex/In-class_Ex02.html#saving-the-analytical-data-in-rds-format",
    "href": "In-class_Ex/In-class_Ex02.html#saving-the-analytical-data-in-rds-format",
    "title": "In-class Exercise 2: Geospatial Data Wrangling",
    "section": "4.4 Saving the Analytical Data in rds format",
    "text": "4.4 Saving the Analytical Data in rds format\nIn order to retain the sf data structure for subsequent analysis, we should save the sf dataframe into rds format.\n\nwrite_rds(nigeria_wp, \"Data/rds/nigeria_wp.rds\")"
  },
  {
    "objectID": "In-class_Ex/In-class_Ex02.html#plotting-the-distribution-of-total-water-points-by-lga-in-histogram",
    "href": "In-class_Ex/In-class_Ex02.html#plotting-the-distribution-of-total-water-points-by-lga-in-histogram",
    "title": "In-class Exercise 2: Geospatial Data Wrangling",
    "section": "4.5 Plotting the Distribution of Total Water Points by LGA in Histogram",
    "text": "4.5 Plotting the Distribution of Total Water Points by LGA in Histogram\nNext, we will use mutate() of dplyr package to compute the proportion of Functional and Non- water points.\nThis is given by Functional Proportion = Functional Count / Total Count.\n\nggplot(data = nigeria_wp,\n       aes(x = total_wp)) +\n  geom_histogram(bins = 20,\n                 color = \"black\",\n                 fill = \"light blue\") +\n  geom_vline(aes(xintercept = mean(\n    total_wp, na.rm = T)),\n    color = \"red\",\n    linetype = \"dashed\",\n    size = 0.8) +\n  ggtitle(\"Distribution of total water points by LGA\") +\n  xlab(\"No. of water points\") +\n  ylab(\"No of\\nLGAs\") +\n  theme(axis.title.y = element_text(angle = 0))"
  },
  {
    "objectID": "In-class_Ex/In-class_Ex03.html",
    "href": "In-class_Ex/In-class_Ex03.html",
    "title": "In-class Exercise 3: Analytical Mapping",
    "section": "",
    "text": "In this in-class exercise, you will gain hands-on experience on using appropriate R methods to plot analytical maps. For the purpose of this exercise, Nigeria water point data prepared during In-class Exercise 2 will be used.\n\n\n\nBy the end of this in-class exercise, you will be able to use appropriate functions of tmap and tidyverse to perform the following tasks:\n\nImporting geospatial data in rds format into R environment.\nCreating cartographic quality choropleth maps by using appropriate tmap functions.\nCreating rate map\nCreating percentile map\nCreating boxmap"
  },
  {
    "objectID": "In-class_Ex/In-class_Ex03.html#installing-and-loading-packages",
    "href": "In-class_Ex/In-class_Ex03.html#installing-and-loading-packages",
    "title": "In-class Exercise 3: Analytical Mapping",
    "section": "2.1 Installing and Loading Packages",
    "text": "2.1 Installing and Loading Packages\nFirstly, the code below will check if pacman has been installed. If it has not been installed, R will download and install it, before activating it for use during this session.\n\nif (!require('pacman', character.only = T)){\n  install.packages('pacman')\n}\nlibrary('pacman')\n\nNext, pacman assists us by helping us load R packages that we require, sf, tidyverse and tmap.\n\npacman::p_load(sf, tidyverse, tmap)"
  },
  {
    "objectID": "In-class_Ex/In-class_Ex03.html#importing-data",
    "href": "In-class_Ex/In-class_Ex03.html#importing-data",
    "title": "In-class Exercise 3: Analytical Mapping",
    "section": "2.2 Importing Data",
    "text": "2.2 Importing Data\nWe want to import the sf dataframe we have cleaned and prepared earlier in class exercise 02.\n\nNGA_wp <- read_rds(\"data/rds/nigeria_wp.rds\")"
  },
  {
    "objectID": "In-class_Ex/In-class_Ex03.html#visualising-distribution-of-non-functional-water-points",
    "href": "In-class_Ex/In-class_Ex03.html#visualising-distribution-of-non-functional-water-points",
    "title": "In-class Exercise 3: Analytical Mapping",
    "section": "2.3 Visualising Distribution of Non-Functional Water Points",
    "text": "2.3 Visualising Distribution of Non-Functional Water Points\nHere, we will plot 2 maps, p1 which shows the functional water points and p2 by total number of water points for side-by-side visualization.\n\np1 <- tm_shape(NGA_wp) +\n  tm_fill(\"wp_functional\",\n          n = 10,\n          style = \"equal\",\n          palette = \"Blues\") +\n  tm_borders(lwd = 0.1,\n             alpha = 1) +\n  tm_layout(main.title = \"Distribution of functional water points by LGA\",\n            legend.outside = FALSE)\n\n\np2 <- tm_shape(NGA_wp) +\n  tm_fill(\"total_wp\",\n          n = 10,\n          style = \"equal\",\n          palette = \"Blues\") +\n  tm_borders(lwd = 0.1,\n             alpha = 1) +\n  tm_layout(main.title = \"Distribution of total water points by LGA\",\n            legend.outside = FALSE)\n\nUsing the tmap_arrange() function, we can arrange the two maps plotted on a single row for side-by-side comparison.\n\ntmap_arrange(p2, p1, nrow = 1)"
  },
  {
    "objectID": "In-class_Ex/In-class_Ex03.html#deriving-proportion-of-functional-water-points-and-non-functional-water-points",
    "href": "In-class_Ex/In-class_Ex03.html#deriving-proportion-of-functional-water-points-and-non-functional-water-points",
    "title": "In-class Exercise 3: Analytical Mapping",
    "section": "3.1 Deriving Proportion of Functional Water Points and Non-Functional Water Points",
    "text": "3.1 Deriving Proportion of Functional Water Points and Non-Functional Water Points\nWith the code below, we use mutate() to calculate the percentages of functional and nonpfunctional water points.\n\nNGA_wp <- NGA_wp %>%\n  mutate(pct_functional = wp_functional / total_wp) %>%\n  mutate(pct_nonfunctional = wp_nonfunctional / total_wp)"
  },
  {
    "objectID": "In-class_Ex/In-class_Ex03.html#plotting-map-of-rate",
    "href": "In-class_Ex/In-class_Ex03.html#plotting-map-of-rate",
    "title": "In-class Exercise 3: Analytical Mapping",
    "section": "3.2 Plotting Map of Rate",
    "text": "3.2 Plotting Map of Rate\nUtilising tmap, we can specify the NGA_wp dataframe to colour by pct_functional water points.\n\ntm_shape(NGA_wp) +\n  tm_fill(\"pct_functional\",\n          n = 10,\n          style = \"equal\",\n          palette = \"Blues\") +\n  tm_borders(lwd = 0.1,\n             alpha = 1) +\n  tm_layout(main.title = \"Rate map of functional water points by LGA\",\n            legend.outside = FALSE)"
  },
  {
    "objectID": "In-class_Ex/In-class_Ex03.html#percentile-map",
    "href": "In-class_Ex/In-class_Ex03.html#percentile-map",
    "title": "In-class Exercise 3: Analytical Mapping",
    "section": "4.1 Percentile Map",
    "text": "4.1 Percentile Map\nA percentile is a special type of quantile map with the following categories:\n\n0-1%\n1-10%\n10-50%\n50-90%\n90-99%\n99 - 100%\n\nTo create the map, we can set the breakpoints as c(0, 0.01, 0.1, 0.5, 0.9, 0.99, 1). Note that the start and endpoints needs to be included.\n\n4.1.1 Data Preparation\nFirstly, we exclude records with NA using the code below:\n\nNGA_wp <- NGA_wp %>%\n  drop_na()\n\nSecondly, we create a customised classification andextract the values.\n\npercent <- c(0, 0.01, 0.1, 0.5, 0.9, 0.99, 1)\nvar <- NGA_wp[\"pct_functional\"] %>%\n  st_set_geometry(NULL)\nquantile(var[,1], percent)\n\n       0%        1%       10%       50%       90%       99%      100% \n0.0000000 0.0000000 0.2169811 0.4791667 0.8611111 1.0000000 1.0000000 \n\n\n\n\n4.1.2 Function to get variable dataframe\nWith the function below, we can extract a variable out of the sf dataframe as a vector.\n\nget.var <- function(vname, df) {\n  v <- df[vname] %>%\n    st_set_geometry(NULL)\n  v <- unname(v[,1])\n  return(v)\n}\n\n\n\n4.1.3 Percentile Mapping Function\nThis percentmap function allows us to take various inputs and automatically calculate the values and points needed for the percentile map.\nThe use of functions allows us to easily plot percentile maps of other variables flexibly without rewriting the entire code.\n\npercentmap <- function(vnam, df, legtitle = NA, mtitle = \"Percentile Map\"){\n  percent <- c(0, 0.01, 0.1, 0.5, 0.9, 0.99, 1)\n  var <- get.var(vnam, df)\n  bperc <- quantile(var, percent)\n  tm_shape(df) +\n  tm_polygons() +\n  tm_shape(df) +\n    tm_fill(vnam,\n            title = legtitle,\n            breaks = bperc,\n            palette = \"Blues\",\n            labels = c(\"< 1%\", \"1% - 10%\", \"10% - 50%\", \"50% - 90%\", \"90% - 99%\", \"99% - 100%\")) +\n  tm_borders() +\n  tm_layout(main.title = mtitle,\n            title.position = c(\"right\", \"bottom\"))\n  }\n\nPlotting the Percentile Map of functional water points.\n\npercentmap(\"wp_functional\", NGA_wp)"
  },
  {
    "objectID": "In-class_Ex/In-class_Ex04.html",
    "href": "In-class_Ex/In-class_Ex04.html",
    "title": "In-class Exercise 4",
    "section": "",
    "text": "pacman::p_load(maptools, sf, raster, spatstat, tmap)\n\nThings to learn from this code chunk.\nImporting spatial data\n\nchildcare_sf <- st_read(\"data/Geospatial/child-care-services-geojson.geojson\") %>% \n  st_transform(crs = 3414)\n\nReading layer `child-care-services-geojson' from data source \n  `C:\\Users\\kwekm\\Desktop\\SMU Year 3 Semester 2\\IS415 Geospatial Analytics and Applications\\KMRCrazyDuck\\IS415-KMR\\In-class_Ex\\Data\\Geospatial\\child-care-services-geojson.geojson' \n  using driver `GeoJSON'\nSimple feature collection with 1545 features and 2 fields\nGeometry type: POINT\nDimension:     XYZ\nBounding box:  xmin: 103.6824 ymin: 1.248403 xmax: 103.9897 ymax: 1.462134\nz_range:       zmin: 0 zmax: 0\nGeodetic CRS:  WGS 84\n\n\n\nsg_sf <- st_read(dsn = \"data/Geospatial\", layer=\"CostalOutline\")\n\nReading layer `CostalOutline' from data source \n  `C:\\Users\\kwekm\\Desktop\\SMU Year 3 Semester 2\\IS415 Geospatial Analytics and Applications\\KMRCrazyDuck\\IS415-KMR\\In-class_Ex\\Data\\Geospatial' \n  using driver `ESRI Shapefile'\nSimple feature collection with 60 features and 4 fields\nGeometry type: POLYGON\nDimension:     XY\nBounding box:  xmin: 2663.926 ymin: 16357.98 xmax: 56047.79 ymax: 50244.03\nProjected CRS: SVY21\n\n\n\nmpsz_sf <- st_read(dsn = \"data/Geospatial\", \n                layer = \"MP14_SUBZONE_WEB_PL\")\n\nReading layer `MP14_SUBZONE_WEB_PL' from data source \n  `C:\\Users\\kwekm\\Desktop\\SMU Year 3 Semester 2\\IS415 Geospatial Analytics and Applications\\KMRCrazyDuck\\IS415-KMR\\In-class_Ex\\Data\\Geospatial' \n  using driver `ESRI Shapefile'\nSimple feature collection with 323 features and 15 fields\nGeometry type: MULTIPOLYGON\nDimension:     XY\nBounding box:  xmin: 2667.538 ymin: 15748.72 xmax: 56396.44 ymax: 50256.33\nProjected CRS: SVY21\n\n\n\ntmap_mode('view')\ntm_shape(childcare_sf)+\n  tm_dots()\n\n\n\n\n\n\n\ntmap_mode('view')\ntm_shape(childcare_sf)+\n  tm_dots(alph =0.5,\n          size =0.01) +\n  tm_view(set.zoom.limits = c(11,14))\n\n\n\n\n\n\n\ntmap_mode('plot')\n\n\nchildcare <- as_Spatial(childcare_sf)\nmpsz <- as_Spatial(mpsz_sf)\nsg <- as_Spatial(sg_sf)"
  },
  {
    "objectID": "In-class_Ex/In-class_Ex04.html#converting-the-spatial-class-into-generic-sp-format",
    "href": "In-class_Ex/In-class_Ex04.html#converting-the-spatial-class-into-generic-sp-format",
    "title": "In-class Exercise 4",
    "section": "4.5.2 Converting the Spatial* class into generic sp format",
    "text": "4.5.2 Converting the Spatial* class into generic sp format\n\nchildcare_sp <- as(childcare, \"SpatialPoints\")\nsg_sp <- as(sg, \"SpatialPolygons\")"
  },
  {
    "objectID": "In-class_Ex/In-class_Ex04.html#converting-the-generic-sp-format-into-spatstats-ppp-format",
    "href": "In-class_Ex/In-class_Ex04.html#converting-the-generic-sp-format-into-spatstats-ppp-format",
    "title": "In-class Exercise 4",
    "section": "4.5.3 Converting the generic sp format into spatstat’s ppp format",
    "text": "4.5.3 Converting the generic sp format into spatstat’s ppp format\n\nchildcare_ppp <- as(childcare_sp, \"ppp\")\nchildcare_ppp\n\nPlanar point pattern: 1545 points\nwindow: rectangle = [11203.01, 45404.24] x [25667.6, 49300.88] units\n\n\n\nplot(childcare_ppp)"
  },
  {
    "objectID": "In-class_Ex/In-class_Ex05.html",
    "href": "In-class_Ex/In-class_Ex05.html",
    "title": "In-class_Ex05",
    "section": "",
    "text": "pacman::p_load(tidyverse, tmap, sf, sfdep)\n\nImporting data shapefile for study area\n\nstudyArea <- st_read(dsn = \"data/Geospatial\", \n                layer = \"study_area\") %>%\n  st_transform(crs = 3829)\n\nReading layer `study_area' from data source \n  `C:\\Users\\kwekm\\Desktop\\SMU Year 3 Semester 2\\IS415 Geospatial Analytics and Applications\\KMRCrazyDuck\\IS415-KMR\\In-class_Ex\\Data\\Geospatial' \n  using driver `ESRI Shapefile'\nSimple feature collection with 7 features and 7 fields\nGeometry type: POLYGON\nDimension:     XY\nBounding box:  xmin: 121.4836 ymin: 25.00776 xmax: 121.592 ymax: 25.09288\nGeodetic CRS:  TWD97\n\n\nImporting data shapefile for stores\n\nstores <- st_read(dsn = \"data/Geospatial\", \n                layer = \"stores\") %>%\n  st_transform(crs = 3829)\n\nReading layer `stores' from data source \n  `C:\\Users\\kwekm\\Desktop\\SMU Year 3 Semester 2\\IS415 Geospatial Analytics and Applications\\KMRCrazyDuck\\IS415-KMR\\In-class_Ex\\Data\\Geospatial' \n  using driver `ESRI Shapefile'\nSimple feature collection with 1409 features and 4 fields\nGeometry type: POINT\nDimension:     XY\nBounding box:  xmin: 121.4902 ymin: 25.01257 xmax: 121.5874 ymax: 25.08557\nGeodetic CRS:  TWD97"
  },
  {
    "objectID": "In-class_Ex/In-class_Ex05.html#visualizing-the-sf-layer",
    "href": "In-class_Ex/In-class_Ex05.html#visualizing-the-sf-layer",
    "title": "In-class_Ex05",
    "section": "Visualizing the sf layer",
    "text": "Visualizing the sf layer\n\ntmap_mode(\"view\")\ntm_shape(studyArea) +\n  tm_polygons() +\n  tm_shape(stores) +\n  tm_dots(col = \"Name\", \n          size = 0.01,\n          border.col = \"black\",\n          border.lwd = 0.5) +\n  tm_view(set.zoom.limits = c(12, 16))"
  },
  {
    "objectID": "In-class_Ex/In-class_Ex05.html#local-colocation-quotients-lclq",
    "href": "In-class_Ex/In-class_Ex05.html#local-colocation-quotients-lclq",
    "title": "In-class_Ex05",
    "section": "Local Colocation Quotients (LCLQ)",
    "text": "Local Colocation Quotients (LCLQ)\n\nnb <- include_self(\n  st_knn(st_geometry(stores), 6))\n\n\nwt <- st_kernel_weights(nb,\n                        stores, \n                        \"gaussian\",\n                        adaptive = TRUE)\n\n\nFamilyMart <- stores %>%\n  filter(Name == \"Family Mart\")\nA <- FamilyMart$Name\n\n\nSevenEleven <- stores %>%\n  filter(Name == \"7-Eleven\")\nB <- SevenEleven$Name\n\n\nLCLQ <- local_colocation(A, B, nb, wt , 49)\n\n\nLCLQ_stores <- cbind(stores, LCLQ)\n\n\ntmap_mode(\"view\")\ntm_shape(studyArea) +\n  tm_polygons() +\n  tm_shape(LCLQ_stores) +\n  tm_dots(col = \"X7.Eleven\", \n          size = 0.01,\n          border.col = \"black\",\n          border.lwd = 0.5) +\n  tm_view(set.zoom.limits = c(12, 16))"
  },
  {
    "objectID": "In-class_Ex/In-class_Ex06.html",
    "href": "In-class_Ex/In-class_Ex06.html",
    "title": "In-class_Ex06",
    "section": "",
    "text": "Before we get started, we need to ensure that sfdep, sf, tmap and tidyverse packages of R are currently installed in your R.\n\npacman::p_load(sf, sfdep, tmap, tidyverse)"
  },
  {
    "objectID": "In-class_Ex/In-class_Ex06.html#the-data",
    "href": "In-class_Ex/In-class_Ex06.html#the-data",
    "title": "In-class_Ex06",
    "section": "The Data",
    "text": "The Data\nFor the purpose of this in-class exercise, the hunan data sets will be used. There are two data sets in this use case, they are:\n\nHunan, a goaspatial data set in ESRI shapefile format, and\nHunan_2021, an attribute data set in csv format.\n\n\nImporting geospatial data\n\nhunan <- st_read(dsn = \"data/Geospatial\", \n                 layer = \"Hunan\")\n\nReading layer `Hunan' from data source \n  `C:\\Users\\kwekm\\Desktop\\SMU Year 3 Semester 2\\IS415 Geospatial Analytics and Applications\\KMRCrazyDuck\\IS415-KMR\\In-class_Ex\\Data\\Geospatial' \n  using driver `ESRI Shapefile'\nSimple feature collection with 88 features and 7 fields\nGeometry type: POLYGON\nDimension:     XY\nBounding box:  xmin: 108.7831 ymin: 24.6342 xmax: 114.2544 ymax: 30.12812\nGeodetic CRS:  WGS 84\n\n\n\n\nImport csv file into r environment\n\nhunan2012 <- read_csv(\"data/Aspatial/Hunan_2012.csv\")"
  },
  {
    "objectID": "In-class_Ex/In-class_Ex06.html#performing-relational-join",
    "href": "In-class_Ex/In-class_Ex06.html#performing-relational-join",
    "title": "In-class_Ex06",
    "section": "Performing relational join",
    "text": "Performing relational join\nThe code chunk below will be used to update the attribute table of hunan’s SpatialPolygonsDataFrame with the attribute fields of hunan2012 dataframe. This is performed by using left_join() of dplyr package.\n\nhunan_GDPPC <- left_join(hunan,hunan2012)%>%\n  select(1:4, 7, 15)\n\n\ntmap_mode(\"plot\")\ntm_shape(hunan_GDPPC) +\n  tm_fill(\"GDPPC\",\n          style = \"quantile\",\n          palette = \"Blues\",\n          title = \"GDPPC\") +\n    tm_layout(main.title = 'Distribution of GDP per capita by distribution',\n    main.title.position = \"center\",\n    main.title.size = 1.2,\n    legend.height = 0.45,\n    legend.width = 0.35,\n    frame = TRUE) +\ntm_borders(alpha = 0.5) +\ntm_compass(type=\"8star\", size = 2) +\ntm_scale_bar() +\ntm_grid(alpha =0.2)"
  },
  {
    "objectID": "In-class_Ex/In-class_Ex06.html#visualising-regional-development-indicator",
    "href": "In-class_Ex/In-class_Ex06.html#visualising-regional-development-indicator",
    "title": "In-class_Ex06",
    "section": "Visualising Regional Development Indicator",
    "text": "Visualising Regional Development Indicator\nNow, we are going to prepare a basemap and a choropleth map showing the distribution of GDPPC 2012 by using qtm() of tmap package.\n\nbasemap <- tm_shape(hunan_GDPPC) +\n  tm_polygons() +\n  tm_text(\"NAME_3\", size=0.5)\n\ngdppc <- qtm(hunan_GDPPC, \"GDPPC\")\ntmap_arrange(basemap, gdppc, asp=1, ncol=2)"
  },
  {
    "objectID": "In-class_Ex/In-class_Ex06.html#contingency-neighbours-method",
    "href": "In-class_Ex/In-class_Ex06.html#contingency-neighbours-method",
    "title": "In-class_Ex06",
    "section": "Contingency neighbours method",
    "text": "Contingency neighbours method\nIn the code chunk below, st_contiguity() is used to derive a continguity neighbour list by using queen’s method.\n\ncn_queen <-hunan_GDPPC %>%\n  mutate(nb = st_contiguity(geometry),\n         .before = 1)\n\nderive a contiguity neighbour list Using Rook’s method\n\ncn_rook <- hunan_GDPPC %>%\n  mutate(nb =st_contiguity(geometry),\n         queen =FALSE,\n         .before = 1)"
  },
  {
    "objectID": "In-class_Ex/In-class_Ex07.html#importing-the-geospatial-data",
    "href": "In-class_Ex/In-class_Ex07.html#importing-the-geospatial-data",
    "title": "In-class Exercise 7",
    "section": "1.1 Importing the Geospatial Data",
    "text": "1.1 Importing the Geospatial Data\nThe code chunk below uses st_read() of sf package to import Hunan shapefile into R. The imported shapefile will be simple features Object of sf.\n\nhunan <- st_read(dsn = \"Data/Geospatial\", \n                 layer = \"Hunan\")\n\nReading layer `Hunan' from data source \n  `C:\\Users\\kwekm\\Desktop\\SMU Year 3 Semester 2\\IS415 Geospatial Analytics and Applications\\KMRCrazyDuck\\IS415-KMR\\In-class_Ex\\Data\\Geospatial' \n  using driver `ESRI Shapefile'\nSimple feature collection with 88 features and 7 fields\nGeometry type: POLYGON\nDimension:     XY\nBounding box:  xmin: 108.7831 ymin: 24.6342 xmax: 114.2544 ymax: 30.12812\nGeodetic CRS:  WGS 84"
  },
  {
    "objectID": "In-class_Ex/In-class_Ex07.html#importing-csv-file-into-environment",
    "href": "In-class_Ex/In-class_Ex07.html#importing-csv-file-into-environment",
    "title": "In-class Exercise 7",
    "section": "1.2 Importing CSV file into environment",
    "text": "1.2 Importing CSV file into environment\nNext, we will import Hunan_2012.csv into R by using read_csv() of readr package. The output is R dataframe class.\n\nhunan2012 <- read_csv(\"Data/Aspatial/Hunan_2012.csv\")"
  },
  {
    "objectID": "In-class_Ex/In-class_Ex07.html#performing-relational-join",
    "href": "In-class_Ex/In-class_Ex07.html#performing-relational-join",
    "title": "In-class Exercise 7",
    "section": "1.3 Performing relational join",
    "text": "1.3 Performing relational join\nThe code chunk below will be used to update the attribute table of hunan’s SpatialPolygonsDataFrame with the attribute fields of hunan2012 dataframe. This is performed by using left_join() of dplyr package.\nIn order to retain the geospatial properties, the left data frame must be the sf data.frame (i.e. hunan)\n\nhunan_GDPPC <- left_join(hunan,hunan2012)%>%\n  select(1:4, 7, 15)"
  },
  {
    "objectID": "In-class_Ex/In-class_Ex07.html#contiguity-weights-queens-method",
    "href": "In-class_Ex/In-class_Ex07.html#contiguity-weights-queens-method",
    "title": "In-class Exercise 7",
    "section": "3.1 Contiguity weights: Queen’s method",
    "text": "3.1 Contiguity weights: Queen’s method\n\nwm_q <- hunan_GDPPC %>%\n  mutate(nb = st_contiguity(geometry),\n         wt = st_weights(nb,\n                         style = \"W\"),\n         .before = 1)"
  },
  {
    "objectID": "In-class_Ex/In-class_Ex07.html#contiguity-weights-rooks-method",
    "href": "In-class_Ex/In-class_Ex07.html#contiguity-weights-rooks-method",
    "title": "In-class Exercise 7",
    "section": "3.2 Contiguity weights: Rook’s method",
    "text": "3.2 Contiguity weights: Rook’s method\n\nwm_r <- hunan_GDPPC %>%\n  mutate(nb = st_contiguity(geometry),\n  queen = FALSE,\n  wt = st_weights(nb,\n                  style = \"W\"),\n  .before = 1)"
  },
  {
    "objectID": "In-class_Ex/In-class_Ex07.html#computing-global-moran-i",
    "href": "In-class_Ex/In-class_Ex07.html#computing-global-moran-i",
    "title": "In-class Exercise 7",
    "section": "3.3 Computing Global Moran I",
    "text": "3.3 Computing Global Moran I\n\nmoranI <- global_moran(wm_q$GDPPC,\n                        wm_q$nb,\n                        wm_q$wt)"
  },
  {
    "objectID": "In-class_Ex/In-class_Ex07.html#performing-global-moran-is-test",
    "href": "In-class_Ex/In-class_Ex07.html#performing-global-moran-is-test",
    "title": "In-class Exercise 7",
    "section": "3.4 Performing Global Moran I’s Test",
    "text": "3.4 Performing Global Moran I’s Test\n\nglobal_moran_test(wm_q$GDPPC,\n                  wm_q$nb,\n                  wm_q$wt)\n\n\n    Moran I test under randomisation\n\ndata:  x  \nweights: listw    \n\nMoran I statistic standard deviate = 4.7351, p-value = 1.095e-06\nalternative hypothesis: greater\nsample estimates:\nMoran I statistic       Expectation          Variance \n      0.300749970      -0.011494253       0.004348351"
  },
  {
    "objectID": "In-class_Ex/In-class_Ex07.html#performing-global-morans-i-permutation-test",
    "href": "In-class_Ex/In-class_Ex07.html#performing-global-morans-i-permutation-test",
    "title": "In-class Exercise 7",
    "section": "3.5 Performing Global Moran’s I permutation test",
    "text": "3.5 Performing Global Moran’s I permutation test\nTo ensure results stay the same when rendering every time\n\nset.seed(1234)\n\n\nglobal_moran_perm(wm_q$GDPPC,\n                  wm_q$nb,\n                  wm_q$wt,\n                  nsim = 99)\n\n\n    Monte-Carlo simulation of Moran I\n\ndata:  x \nweights: listw  \nnumber of simulations + 1: 100 \n\nstatistic = 0.30075, observed rank = 100, p-value < 2.2e-16\nalternative hypothesis: two.sided\n\n\nIf observation values are small, it is better to use a higher number of simulations"
  },
  {
    "objectID": "In-class_Ex/In-class_Ex07.html#computing-local-morans-i",
    "href": "In-class_Ex/In-class_Ex07.html#computing-local-morans-i",
    "title": "In-class Exercise 7",
    "section": "3.6 Computing local Moran’s I",
    "text": "3.6 Computing local Moran’s I\n\nlisa <- wm_q %>%\n  mutate(local_moran = local_moran(\n    GDPPC, nb, wt, nsim = 99),\n    .before = 1) %>%\n  unnest(local_moran)\n\nlisa\n\nSimple feature collection with 88 features and 20 fields\nGeometry type: POLYGON\nDimension:     XY\nBounding box:  xmin: 108.7831 ymin: 24.6342 xmax: 114.2544 ymax: 30.12812\nGeodetic CRS:  WGS 84\n# A tibble: 88 × 21\n         ii        eii   var_ii    z_ii    p_ii p_ii_…¹ p_fol…² skewn…³ kurtosis\n      <dbl>      <dbl>    <dbl>   <dbl>   <dbl>   <dbl>   <dbl>   <dbl>    <dbl>\n 1 -0.00147  0.00177    4.18e-4 -0.158  0.874      0.82    0.41  -0.812  0.652  \n 2  0.0259   0.00641    1.05e-2  0.190  0.849      0.96    0.48  -1.09   1.89   \n 3 -0.0120  -0.0374     1.02e-1  0.0796 0.937      0.76    0.38   0.824  0.0461 \n 4  0.00102 -0.0000349  4.37e-6  0.506  0.613      0.64    0.32   1.04   1.61   \n 5  0.0148  -0.00340    1.65e-3  0.449  0.654      0.5     0.25   1.64   3.96   \n 6 -0.0388  -0.00339    5.45e-3 -0.480  0.631      0.82    0.41   0.614 -0.264  \n 7  3.37    -0.198      1.41e+0  3.00   0.00266    0.08    0.04   1.46   2.74   \n 8  1.56    -0.265      8.04e-1  2.04   0.0417     0.08    0.04   0.459 -0.519  \n 9  4.42     0.0450     1.79e+0  3.27   0.00108    0.02    0.01   0.746 -0.00582\n10 -0.399   -0.0505     8.59e-2 -1.19   0.234      0.28    0.14  -0.685  0.134  \n# … with 78 more rows, 12 more variables: mean <fct>, median <fct>,\n#   pysal <fct>, nb <nb>, wt <list>, NAME_2 <chr>, ID_3 <int>, NAME_3 <chr>,\n#   ENGTYPE_3 <chr>, County <chr>, GDPPC <dbl>, geometry <POLYGON [°]>, and\n#   abbreviated variable names ¹​p_ii_sim, ²​p_folded_sim, ³​skewness\n\n\nVariable mean and pysal value should be the same. For take home exercise 2, stay with mean"
  },
  {
    "objectID": "In-class_Ex/In-class_Ex07.html#visualising-local-morans-i",
    "href": "In-class_Ex/In-class_Ex07.html#visualising-local-morans-i",
    "title": "In-class Exercise 7",
    "section": "3.7 Visualising local Moran’s I",
    "text": "3.7 Visualising local Moran’s I\n\n3.7.1 Computing ii\n\ntmap_mode(\"plot\")\ntm_shape(lisa) +\n  tm_fill(\"ii\") +\n  tm_borders(alpha = 0.5) +\n  tm_view(set.zoom.limits = c(6,8))\n\n\n\n\n\n\n3.7.2 Computing p_ii\n\ntmap_mode(\"plot\")\ntm_shape(lisa) +\n  tm_fill(\"p_ii\") +\n  tm_borders(alpha = 0.5)\n\n\n\n\nIdeally should use p_ii_sim variable of lisa so that results produced is stable.\n\n\n3.7.3 Visualising the local Moran’s I Map\n\nlisa_sig <- lisa %>%\n  filter(p_ii < 0.05)\ntmap_mode(\"plot\")\ntm_shape(lisa) +\n  tm_polygons() +\n  tm_borders(alpha = 0.5) +\ntm_shape(lisa_sig) +\n  tm_fill(\"mean\") + \n  tm_borders(alpha = 0.4)\n\n\n\n\nFor take home exercise 2, add on to use insignificant on top of LL,HL,LH,HH, no need to use LISA but hot & cold spot areas"
  },
  {
    "objectID": "In-class_Ex/In-class_Ex07.html#visualising-gi",
    "href": "In-class_Ex/In-class_Ex07.html#visualising-gi",
    "title": "In-class Exercise 7",
    "section": "4.1 Visualising Gi*",
    "text": "4.1 Visualising Gi*\n\ntmap_mode(\"view\")\ntm_shape(HCSA) +\n  tm_fill(\"gi_star\") +\n  tm_borders(alpha = 0.5) +\n  tm_view(set.zoom.limits = c(6,8))"
  },
  {
    "objectID": "In-class_Ex/In-class_Ex07.html#visualising-the-p-value-of-hcsa",
    "href": "In-class_Ex/In-class_Ex07.html#visualising-the-p-value-of-hcsa",
    "title": "In-class Exercise 7",
    "section": "4.2 Visualising the p value of HCSA",
    "text": "4.2 Visualising the p value of HCSA\n\ntmap_mode(\"plot\")\ntm_shape(HCSA) +\n  tm_fill(\"p_sim\") +\n  tm_borders(alpha = 0.5)"
  },
  {
    "objectID": "In-class_Ex/In-class_Ex07.html#import-files-of-hunan-gdppc",
    "href": "In-class_Ex/In-class_Ex07.html#import-files-of-hunan-gdppc",
    "title": "In-class Exercise 7",
    "section": "5.1 Import files of Hunan GDPPC",
    "text": "5.1 Import files of Hunan GDPPC\n\nGDPPC <- read_csv(\"Data/Aspatial/Hunan_GDPPC.csv\")\n\n\n5.1.1 Creating a time series cube\n\nGDPPC_st <- spacetime(GDPPC, hunan,\n                      .loc_col = \"County\",\n                      .time_col = \"Year\")\n\nTo construct spacetime cube, we must obtain the location and time\n\nGDPPC_nb <- GDPPC_st %>%\n  activate(\"geometry\") %>%\n  mutate(\n    nb = include_self(st_contiguity(geometry)),\n    wt = st_weights(nb)\n  ) %>%\n  set_nbs(\"nb\") %>%\n  set_wts(\"wt\")"
  },
  {
    "objectID": "In-class_Ex/In-class_Ex07.html#arrange-to-show-significant-hotspot-and-coldspot-areas",
    "href": "In-class_Ex/In-class_Ex07.html#arrange-to-show-significant-hotspot-and-coldspot-areas",
    "title": "In-class Exercise 7",
    "section": "5.2 Arrange to show significant hotspot and coldspot areas",
    "text": "5.2 Arrange to show significant hotspot and coldspot areas"
  },
  {
    "objectID": "In-class_Ex/In-class_Ex07.html#performing-emerging-hotspot-analysis",
    "href": "In-class_Ex/In-class_Ex07.html#performing-emerging-hotspot-analysis",
    "title": "In-class Exercise 7",
    "section": "5.3 Performing Emerging Hotspot Analysis",
    "text": "5.3 Performing Emerging Hotspot Analysis\n\nehsa <- emerging_hotspot_analysis(\n  x = GDPPC_st,\n  .var = \"GDPPC\",\n  k = 1,\n  nsim = 99\n)"
  },
  {
    "objectID": "In-class_Ex/In-class_Ex07.html#visualising-ehsa",
    "href": "In-class_Ex/In-class_Ex07.html#visualising-ehsa",
    "title": "In-class Exercise 7",
    "section": "5.4 Visualising EHSA",
    "text": "5.4 Visualising EHSA"
  },
  {
    "objectID": "Take_home_ex/Data/Geospatial/stores.html",
    "href": "Take_home_ex/Data/Geospatial/stores.html",
    "title": "IS415 Geospatial-KMR",
    "section": "",
    "text": "<!DOCTYPE qgis PUBLIC ‘http://mrcc.com/qgis.dtd’ ‘SYSTEM’>     \n\n\n        0 0     false"
  },
  {
    "objectID": "Take_home_ex/Data/Geospatial/study_area.html",
    "href": "Take_home_ex/Data/Geospatial/study_area.html",
    "title": "IS415 Geospatial-KMR",
    "section": "",
    "text": "<!DOCTYPE qgis PUBLIC ‘http://mrcc.com/qgis.dtd’ ‘SYSTEM’>     dataset\n\n\n        0 0     false"
  },
  {
    "objectID": "Take_home_ex/Take_home_ex1.html",
    "href": "Take_home_ex/Take_home_ex1.html",
    "title": "Take Home Exercise 1",
    "section": "",
    "text": "Water is an important resource to mankind. Clean and accessible water is critical to human health. It provides a healthy environment, a sustainable economy, reduces poverty and ensures peace and security. Yet over 40% of the global population does not have access to sufficient clean water. By 2025, 1.8 billion people will be living in countries or regions with absolute water scarcity, according to UN-Water. The lack of water poses a major threat to several sectors, including food security. Agriculture uses about 70% of the world’s accessible freshwater.\nDeveloping countries are most affected by water shortages and poor water quality. Up to 80% of illnesses in the developing world are linked to inadequate water and sanitation. Despite technological advancement, providing clean water to the rural community is still a major development issues in many countries globally, especially countries in the Africa continent.\nTo address the issue of providing clean and sustainable water supply to the rural community, a global Water Point Data Exchange (WPdx) project has been initiated. The main aim of this initiative is to collect water point related data from rural areas at the water point or small water scheme level and share the data via WPdx Data Repository, a cloud-based data library. What is so special of this project is that data are collected based on WPDx Data Standard.\n\n\n\nGeospatial analytics hold tremendous potential to address complex problems facing society. In this study, application of appropriate spatial point patterns analysis methods to discover the geographical distribution of functional and non-function water points and their co-locations if any in Osun State, Nigeria.\n\n\n\nTo provide answers to the questions above, three data sets will be used. They are:\n\n\nFor the purpose of this assignment, data from WPdx Global Data Repositories will be used. There are two versions of the data. They are: WPdx-Basic and WPdx+. You are required to use WPdx+ data set.\n\n\n\nThis study will focus of Osun State, Nigeria. The state boundary GIS data of Nigeria can be downloaded either from The Humanitarian Data Exchange portal or geoBoundaries.\n\n\n\n\nIn this take home exercise 1, six R packages will be used, they are:\n\nsf, a relatively new R package specially designed to import, manage and process vector-based geospatial data in R.\nspatstat, which has a wide range of useful functions for point pattern analysis. In this hands-on exercise, it will be used to perform 1st- and 2nd-order spatial point patterns analysis and derive kernel density estimation (KDE) layer.\ntidyverse, tidyverse is an opinionated collection of R packages designed for data science. It is also coherent system of packages for data manipulation, exploration and visualization.\nraster which reads, writes, manipulates, analyses and model of gridded spatial data (i.e. raster). In this hands-on exercise, it will be used to convert image output generate by spatstat into raster format.\nmaptools which provides a set of tools for manipulating geographic data. In this hands-on exercise, we mainly use it to convert Spatial objects into ppp format of spatstat.\ntmap which provides functions for plotting cartographic quality static point patterns maps or interactive maps by using leaflet API.\nsfdep which is an interface to ‘spdep’ to integrate with ‘sf’ objects and the ‘tidyverse’.\nfunModeling This package contains a set of functions related to exploratory data analysis, data preparation, and model performance.\n\nUse the code chunk below to install and launch the seven R packages.\n\npacman::p_load(maptools, sf, tidyverse, raster, spatstat, tmap, sfdep, funModeling)\n\n\n\n\nIn this section, st_read of sf package will be used to import these data sets into R\nImporting the geoBoundaries Nigeria Level 2 Administrative Boundary Dataset\n\ngbnigeria = st_read(dsn = \"Data/Geospatial\", layer = \"nga_admbnda_adm2_osgof_20190417\")\n\nReading layer `nga_admbnda_adm2_osgof_20190417' from data source \n  `C:\\Users\\kwekm\\Desktop\\SMU Year 3 Semester 2\\IS415 Geospatial Analytics and Applications\\KMRCrazyDuck\\IS415-KMR\\Take_home_ex\\Data\\Geospatial' \n  using driver `ESRI Shapefile'\nSimple feature collection with 774 features and 16 fields\nGeometry type: MULTIPOLYGON\nDimension:     XY\nBounding box:  xmin: 2.668534 ymin: 4.273007 xmax: 14.67882 ymax: 13.89442\nGeodetic CRS:  WGS 84\n\n\n\n\n\nSince WPdx+ data set is in csv format, we will use read_csv() of readr package to import Water_Point_Data_Exchange_-_PlusWPdx.csv and output it to an R object called wpdx.\n\nwpdx <- read_csv(\"Data/Aspatial/Water_Point_Data_Exchange_-_Plus__WPdx__.csv\") %>% filter(`#clean_country_name` == \"Nigeria\")\n\n\n\n\nIn the code below, we will convert the Geographic Coordinate Reference System from WGS84 to EPSG:26391 Projected Coordinate System.\n\nnigeria26391 <- st_transform(gbnigeria, crs = 26391)\n\n\n\n\nAs the geometry is available in wkt in the column New Georeferenced Column, we can use st_as_sfc() to import the geomtry\n\nwpdx$Geometry <- st_as_sfc(wpdx$`New Georeferenced Column`)\n\nAs there is no spatial data information, firstly, we assign the original projection when converting the tibble dataframe to sf. The original is wgs84 which is EPSG:4326.\n\nwpdx_sf <- st_sf(wpdx, crs=4326)\nwpdx_sf\n\nSimple feature collection with 97478 features and 74 fields\nGeometry type: POINT\nDimension:     XY\nBounding box:  xmin: 2.707441 ymin: 4.301812 xmax: 14.21828 ymax: 13.86568\nGeodetic CRS:  WGS 84\n# A tibble: 97,478 × 75\n   row_id `#source`      #lat_…¹ #lon_…² #repo…³ #stat…⁴ #wate…⁵ #wate…⁶ #wate…⁷\n *  <dbl> <chr>            <dbl>   <dbl> <chr>   <chr>   <chr>   <chr>   <chr>  \n 1 158721 Federal Minis…    5.07    6.62 02/19/… Yes     Boreho… Well    Mechan…\n 2 158892 Federal Minis…    5.09    7.09 02/06/… Yes     Boreho… Well    Hand P…\n 3 323117 Federal Minis…    5.91    8.77 08/31/… Yes     Boreho… Well    Hand P…\n 4 300176 Federal Minis…    5.23    7.32 05/17/… Yes     Boreho… Well    Mechan…\n 5 324346 Federal Minis…    6.88    3.36 08/17/… Yes     Boreho… Well    Mechan…\n 6 297273 Federal Minis…    6.59    3.29 05/26/… Yes     Boreho… Well    Mechan…\n 7 296853 Federal Minis…    6.60    3.26 06/02/… Yes     Boreho… Well    Mechan…\n 8 323866 Federal Minis…    6.20    6.73 09/18/… Yes     Boreho… Well    Mechan…\n 9 297044 Federal Minis…    6.61    3.30 05/26/… Yes     Boreho… Well    Mechan…\n10 324321 Federal Minis…    6.96    3.60 08/16/… Yes     Boreho… Well    Mechan…\n# … with 97,468 more rows, 66 more variables: `#water_tech_category` <chr>,\n#   `#facility_type` <chr>, `#clean_country_name` <chr>, `#clean_adm1` <chr>,\n#   `#clean_adm2` <chr>, `#clean_adm3` <chr>, `#clean_adm4` <chr>,\n#   `#install_year` <dbl>, `#installer` <chr>, `#rehab_year` <lgl>,\n#   `#rehabilitator` <lgl>, `#management_clean` <chr>, `#status_clean` <chr>,\n#   `#pay` <chr>, `#fecal_coliform_presence` <chr>,\n#   `#fecal_coliform_value` <dbl>, `#subjective_quality` <chr>, …\n\n\nNext, we then convert the projection to the appropriate decimal based projection system.\n\nwpdx_sf <- wpdx_sf %>%\n  st_transform(crs = 26391)\n\n\n\n\nAs the wpdx sf dataframe consist of many redundant field, we use select() to select the fields which we want to retain.\n\nnigeria26391 <- nigeria26391 %>%\n  dplyr::select(c(3:4, 8:9))\n\n\n\n\nIt is important to check for duplicate name in the data main data fields. Using duplicated(), we can flag out LGA names that might be duplicated as shown below:\n\nnigeria26391$ADM2_EN[duplicated(nigeria26391$ADM2_EN) == TRUE]\n\n[1] \"Bassa\"    \"Ifelodun\" \"Irepodun\" \"Nasarawa\" \"Obi\"      \"Surulere\"\n\n\nTo reduce duplication of LGA names, we will put the state names behind to make it more specific.\n\nnigeria26391$ADM2_EN[94] <- \"Bassa, Kogi\"\nnigeria26391$ADM2_EN[95] <- \"Bassa, Plateau\"\nnigeria26391$ADM2_EN[304] <- \"Ifelodun, Kwara\"\nnigeria26391$ADM2_EN[305] <- \"Ifelodun, Osun\"\nnigeria26391$ADM2_EN[355] <- \"Irepodun, Kwara\"\nnigeria26391$ADM2_EN[356] <- \"Ireopodun, Osun\"\nnigeria26391$ADM2_EN[519] <- \"Nasarawa, Kano\"\nnigeria26391$ADM2_EN[520] <- \"Nasarawa, Nasarawa\"\nnigeria26391$ADM2_EN[546] <- \"Obi, Benue\"\nnigeria26391$ADM2_EN[547] <- \"Obi, Nasarawa\"\nnigeria26391$ADM2_EN[693] <- \"Surulere, Lagos\"\nnigeria26391$ADM2_EN[694] <- \"Surulere, Oyo\"\n\nChecking if the duplication has been resolved.\n\nnigeria26391$ADM2_EN[duplicated(nigeria26391$ADM2_EN) == TRUE]\n\ncharacter(0)\n\n\n\n\n\nFirst, let us have a look at the #status_clean column which stores the information about Functional and Non-Functional data points. The code below returns all values that were used in the column.\n\nfreq(data = wpdx_sf, input = '#status_clean')\n\n\n\n\n                      #status_clean frequency percentage cumulative_perc\n1                        Functional     47228      48.45           48.45\n2                    Non-Functional     30638      31.43           79.88\n3                              <NA>     10154      10.42           90.30\n4          Functional, needs repair      4792       4.92           95.22\n5               Non-Functional, dry      2473       2.54           97.76\n6            Functional, not in use      1775       1.82           99.58\n7          Abandoned/Decommissioned       321       0.33           99.91\n8         Functional but not in use        86       0.09          100.00\n9  Non-Functional due to dry season         7       0.01          100.01\n10      Functional but needs repair         4       0.00          100.00\n\n\nAs there might be issues performing mathematical calculations with NA labels, we will rename them to unknown.\nThe code below renames the column #status_clean to status_clean, select only the status_clean for manipulation and then replace all na values to unknown.\n\nwpdx_sf_nga <- wpdx_sf %>%\n  rename(status_clean = '#status_clean') %>%\n  dplyr::select(status_clean) %>%\n  mutate(status_clean = replace_na(status_clean, \"unknown\"))\n\n\n\n\nWith our previous knowledge, we can filter the data to obtain functional proportion counts in each LGA level. We will filter the wpdx_sf_nga dataframes to option functional and non-functional water points.\n\nwpdx_func <- wpdx_sf_nga %>% \n  filter(status_clean %in% \n           c(\"Functional\", \n             \"Functional but not in use\", \n             \"Functional but needs repair\"))\nwpdx_nonfunc <- wpdx_sf_nga %>% \n  filter(status_clean %in%\n          c(\"Abadoned/Decommissioned\", \n            \"Abandoned\",\n            \"Non-Functional due to dry season\",\n            \"Non-Functional\",\n            \"Non functional due to dry season\"))\nwpdx_unknown <- wpdx_sf_nga %>%\n  filter(status_clean == \"unknown\")\n\n\n\n\nUtilising st_intersects() of sf package and lengths, we check where each data point for the water point which fall inside each LGA. We do each calculation separation so we can cross check later to ensure all the values sum to the same total.\n\nnigeria_wp <- nigeria26391 %>%\n  mutate(`total_wp` = lengths(\n    st_intersects(nigeria26391, wpdx_sf_nga))) %>%\n  mutate(`wp_functional` = lengths(\n    st_intersects(nigeria26391, wpdx_func))) %>%\n  mutate(`wp_nonfunctional` = lengths(\n    st_intersects(nigeria26391, wpdx_nonfunc))) %>%\n  mutate(`wp_unknown` = lengths(\n    st_intersects(nigeria26391, wpdx_unknown)))\n\n\n\n\nIn order to retain the sf data structure for subsequent analysis, we should save the sf dataframe into rds format.\n\nwrite_rds(nigeria_wp, \"Data/rds/nigeria_wp.rds\")\n\n\n\n\nNext, we will use mutate() of dplyr package to compute the proportion of Functional and Non- water points.\nThis is given by Functional Proportion = Functional Count / Total Count.\n\nggplot(data = nigeria_wp,\n       aes(x = total_wp)) +\n  geom_histogram(bins = 20,\n                 color = \"black\",\n                 fill = \"light blue\") +\n  geom_vline(aes(xintercept = mean(\n    total_wp, na.rm = T)),\n    color = \"red\",\n    linetype = \"dashed\",\n    size = 0.8) +\n  ggtitle(\"Distribution of total water points by LGA\") +\n  xlab(\"No. of water points\") +\n  ylab(\"No of\\nLGAs\") +\n  theme(axis.title.y = element_text(angle = 0))\n\n\n\n\n\n\n\n\n\n\nWe want to import the sf dataframe we have cleaned and prepared earlier\n\nNGA_wp <- read_rds(\"Data/rds/nigeria_wp.rds\")\n\n\n\n\nHere, we will plot 2 maps, p1 which shows the functional water points and p2 by total number of water points for side-by-side visualization.\n\np1 <- tm_shape(NGA_wp) +\n  tm_fill(\"wp_functional\",\n          n = 10,\n          style = \"equal\",\n          palette = \"Blues\") +\n  tm_borders(lwd = 0.1,\n             alpha = 1) +\n  tm_layout(main.title = \"Distribution of functional water points by LGA\",\n            legend.outside = FALSE)\n\n\np2 <- tm_shape(NGA_wp) +\n  tm_fill(\"total_wp\",\n          n = 10,\n          style = \"equal\",\n          palette = \"Blues\") +\n  tm_borders(lwd = 0.1,\n             alpha = 1) +\n  tm_layout(main.title = \"Distribution of total water points by LGA\",\n            legend.outside = FALSE)\n\nUsing the tmap_arrange() function, we can arrange the two maps plotted on a single row for side-by-side comparison.\n\ntmap_arrange(p2, p1, nrow = 1)\n\n\n\n\n\n\n\nAlthough simple feature data frame is gaining popularity again sp’s Spatial* classes, there are, however, many geospatial analysis packages require the input geospatial data in sp’s Spatial* classes\nThe code chunk below uses as_Spatial() of sf package to convert the geospatial data from simple feature data frame to sp’s Spatial* class.\n\nwpdx_spdf <- as_Spatial(wpdx_sf)\nnga_wpspdf <- as_Spatial(NGA_wp) \n\n\nwpdx_spdf\n\nclass       : SpatialPointsDataFrame \nfeatures    : 97478 \nextent      : 32536.82, 1292096, 33461.24, 1091052  (xmin, xmax, ymin, ymax)\ncrs         : +proj=tmerc +lat_0=4 +lon_0=4.5 +k=0.99975 +x_0=230738.26 +y_0=0 +a=6378249.145 +rf=293.465 +towgs84=-92,-93,122,0,0,0,0 +units=m +no_defs \nvariables   : 74\nnames       : row_id,                                     X.source, X.lat_deg,  X.lon_deg,          X.report_date, X.status_id,    X.water_source_clean, X.water_source_category, X.water_tech_clean, X.water_tech_category, X.facility_type, X.clean_country_name, X.clean_adm1, X.clean_adm2, X.clean_adm3, ... \nmin values  :  29756, Federal Ministry of Water Resources, Nigeria, 4.3018117,   2.707441, 01/01/2010 12:00:00 AM,          No,                Borehole,             Piped Water,          Hand Pump,             Hand Pump,        Improved,              Nigeria,         Abia,    Aba North,           NA, ... \nmax values  : 685455,                                  WaterAid UK, 13.865675, 14.2182849, 12/31/2014 12:00:00 AM,         Yes, Undefined Hand Dug Well,                    Well,           Tapstand,              Tapstand,        Improved,              Nigeria,      Zamfara,         Zuru,           NA, ... \n\n\n\nnga_wpspdf\n\nclass       : SpatialPolygonsDataFrame \nfeatures    : 774 \nextent      : 28879.72, 1343798, 30292.37, 1094244  (xmin, xmax, ymin, ymax)\ncrs         : +proj=tmerc +lat_0=4 +lon_0=4.5 +k=0.99975 +x_0=230738.26 +y_0=0 +a=6378249.145 +rf=293.465 +towgs84=-92,-93,122,0,0,0,0 +units=m +no_defs \nvariables   : 8\nnames       :   ADM2_EN, ADM2_PCODE, ADM1_EN, ADM1_PCODE, total_wp, wp_functional, wp_nonfunctional, wp_unknown \nmin values  : Aba North,   NG001001,    Abia,      NG001,        0,             0,                0,          0 \nmax values  :      Zuru,   NG037014, Zamfara,      NG037,      907,           677,              252,        219 \n\n\n\n\n\nSpatstat requires the analytical data in ppp object form. There is no direct way to convert a Spatial* classes into ppp object. We need to convert the Spatial classes* into Spatial object first.\nThe codes chunk below converts the Spatial* classes into generic sp objects.\n\nwpdx_sp <- as(wpdx_spdf, \"SpatialPoints\")\nnga_wp_sp <- as(nga_wpspdf, \"SpatialPolygons\")\n\n\nwpdx_sp\n\nclass       : SpatialPoints \nfeatures    : 97478 \nextent      : 32536.82, 1292096, 33461.24, 1091052  (xmin, xmax, ymin, ymax)\ncrs         : +proj=tmerc +lat_0=4 +lon_0=4.5 +k=0.99975 +x_0=230738.26 +y_0=0 +a=6378249.145 +rf=293.465 +towgs84=-92,-93,122,0,0,0,0 +units=m +no_defs \n\n\n\nnga_wp_sp\n\nclass       : SpatialPolygons \nfeatures    : 774 \nextent      : 28879.72, 1343798, 30292.37, 1094244  (xmin, xmax, ymin, ymax)\ncrs         : +proj=tmerc +lat_0=4 +lon_0=4.5 +k=0.99975 +x_0=230738.26 +y_0=0 +a=6378249.145 +rf=293.465 +towgs84=-92,-93,122,0,0,0,0 +units=m +no_defs \n\n\n\n\n\nNow, we will use as.ppp() function of spatstat to convert the spatial data into spatstat’s ppp object format.\n\nwpdx_ppp <- as(wpdx_sp, \"ppp\")\nwpdx_ppp\n\nPlanar point pattern: 97478 points\nwindow: rectangle = [32536.8, 1292096.3] x [33461.2, 1091051.6] units\n\n\nTo check the summary\n\nsummary(wpdx_ppp)\n\nPlanar point pattern:  97478 points\nAverage intensity 7.31763e-08 points per square unit\n\nCoordinates are given to 2 decimal places\ni.e. rounded to the nearest multiple of 0.01 units\n\nWindow: rectangle = [32536.8, 1292096.3] x [33461.2, 1091051.6] units\n                    (1260000 x 1058000 units)\nWindow area = 1.3321e+12 square units\n\n\nIf newly created ppp object contain duplicate points\n\nany(duplicated(wpdx_ppp))\n\n[1] FALSE\n\n\n\n\n\nWhen analysing spatial point patterns, it is a good practice to confine the analysis with a geographical area like Nigeria boundary. In spatstat, an object called owin is specially designed to represent this polygonal region.\nThe code chunk below is used to covert nga_wp_sp SpatialPolygon object into owin object of spatstat.\n\nng_owin <- as(nga_wp_sp, \"owin\")\nplot(ng_owin)\n\n\n\n\n\n\n\nIn this last step of geospatial data wrangling, we will extract water points events that are located within Nigeria by using the code chunk below.\n\nnigeria_PPP = wpdx_ppp[ng_owin]\n\n\nplot(nigeria_PPP)\n\n\n\n\n\n\n\nIn this section, you will learn how to perform first-order SPPA by using spatstat package. The hands-on exercise will focus on:\n\nderiving kernel density estimation (KDE) layer for visualising and exploring the intensity of point processes,\nperforming Confirmatory Spatial Point Patterns Analysis by using Nearest Neighbour statistics.\n\n\n\n\nThe code chunk below computes a kernel density by using the following configurations of density() of spatstat:\n\nbw.diggle() automatic bandwidth selection method.\n\nThe smoothing kernel used is gaussian, which is the default.\n\nThe intensity estimate is corrected for edge effect bias by using method described by Jones (1993) and Diggle (2010, equation 18.9). The default is FALSE.\n\n\nkde_nigeria_bw <- density(nigeria_PPP,\n                              sigma=bw.diggle,\n                              edge=TRUE,\n                            kernel=\"gaussian\") \n\nPlot() function of Base R is used to display kernel density derived.\n\nplot(kde_nigeria_bw)\n\n\n\n\nThe density values of the output range is way too small to comprehend. This is because the default unit of measurement of svy21 is in meter. As a result, the density values computed is in “number of points per square meter”.\nBefore we move on to next section, it is good to know that you can retrieve the bandwidth used to compute the kde layer by using the code chunk below.\n\nbw <- bw.diggle(nigeria_PPP)\nbw\n\n   sigma \n213.9791 \n\n\nRescalling KDE values\nIn the code chunk below, rescale() is used to covert the unit of measurement from meter to kilometer.\n\nnigeria_PPP.km <- rescale(nigeria_PPP, 1000, \"km\")\n\nNow, we can re-run density() using the resale data set and plot the output kde map. The use of sigma = bw.diggle is used to detect a single tight cluster which would be the osun state to be extracted later\n\nkde_nigeria_PPP.bw <- density(nigeria_PPP.km, sigma=bw.diggle, edge=TRUE, kernel=\"gaussian\")\nplot(kde_nigeria_PPP.bw)\n\n\n\n\n\n\n\nIn this section, extraction of study of Nigeria, Osun will be done with the computation of KDE values.\n\nosun = nga_wpspdf[nga_wpspdf@data$ADM1_EN == \"Osun\",]\n\nPlotting of OSUN state of Nigeria\n\nplot(osun, main = \"Osun\")\n\n\n\n\nNext, conversion of SpatialPolygonsDataFrame layers into generic spatialpolygons layers\n\nosun_sp = as(osun, \"SpatialPolygons\")\n\nCreating owin object to convert the above spatialpolygons objects into owin objects that is required by spatstat\n\nosun_owin = as(osun_sp, \"owin\")\n\nCombining osun water points\n\nnigeria_osun_PPP = wpdx_ppp[osun_owin]\n\nrescale() function is used to transform the unit of measurement from metre to kilometre\n\nnigeria_osun_PPP.km = rescale(nigeria_osun_PPP, 1000, \"km\")\n\nPlotting the Osun water point study area\n\nplot(nigeria_osun_PPP.km, main = \"Osun\")\n\n\n\n\nVisualising the SF layer for Osun\n\ntmap_mode(\"view\")\ntm_shape(osun) +\n  tm_polygons() +\n  tm_dots(size = 0.01,\n          border.col = \"black\",\n          border.lwd = 0.5)\n\n\n\n\n\n\n\nosun1 <- tm_shape(osun) +\n  tm_fill(\"wp_functional\",\n          n = 10,\n          style = \"equal\",\n          palette = \"Blues\") +\n  tm_borders(lwd = 0.1,\n             alpha = 1) +\n  tm_layout(main.title = \"Distribution of functional water points by LGA\",\n            legend.outside = FALSE)\n\n\nosun2 <- tm_shape(osun) +\n  tm_fill(\"wp_nonfunctional\",\n          n = 10,\n          style = \"equal\",\n          palette = \"Blues\") +\n  tm_borders(lwd = 0.1,\n             alpha = 1) +\n  tm_layout(main.title = \"Distribution of total water points by LGA\",\n            legend.outside = FALSE)\n\nOsun State Functional and Non Functional Water Points\n\ntmap_arrange(osun2, osun1, nrow = 1)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nThe code below will be used to compute the KDE of Osun. bw.diggle method is used to derive the bandwidth\n\nplot(density(nigeria_osun_PPP.km, \n             sigma=bw.diggle, \n             edge=TRUE, \n             kernel=\"gaussian\"),\n     main=\"Osun\")\n\n\n\n\nComputing fixed bandwidth KDE\n\nplot(density(nigeria_osun_PPP.km,\n             sigma=0.25, \n             edge=TRUE, \n             kernel=\"gaussian\"),\n     main=\"Osun\")\n\n\n\n\nThe advantage of kernel density map over point map is that it spreads the known quantity in color shades of waterpoints for each location in Osun State of Nigeria. Whereas for Kernel point, it only provides the pointer of waterpoints for each location in Osun State and does not state the number unless clicked.\n\n\n\n\n\n\nIn this section, we will perform the Clark-Evans test of aggregation for a spatial point pattern by using clarkevans.test() of statspat.\nThe test hypotheses are:\nHo = The distribution of waterpoints are randomly distributed.\nH1= The distribution of waterpoints are not randomly distributed.\nThe 95% confidence interval will be used.\n\nclarkevans.test(nigeria_osun_PPP,\n                correction=\"none\",\n                clipregion=NULL,\n                alternative=c(\"two.sided\"),\n                nsim=999)\n\n\n    Clark-Evans test\n    No edge correction\n    Monte Carlo test based on 999 simulations of CSR with fixed n\n\ndata:  nigeria_osun_PPP\nR = 0.40793, p-value = 0.002\nalternative hypothesis: two-sided\n\n\nGiven the p value of 0.002 which is lesser than 0.05 of 95% confidence interval, H0 is rejected and we can conclude that the distribution of waterpoints are not randomly distributed\n\n\n\nIn this section, second spatial point L-function will be using Lest() of spatstat package. Monta carlo simulation test will be used using envelope() of spatstat package.\n\n\n\n\n#L_ck = Lest(nigeria_osun_PPP, correction = \"Ripley\")\n#plot(L_ck, . -r ~ r, \n#     ylab= \"L(d)-r\", xlab = \"d(m)\")\n\n\n\n\n\nTo confirm the observed spatial patterns above, a hypothesis test will be conducted. The hypothesis and test are as follows:\nHo = The distribution of Water Point at Osun State are randomly distributed.\nH1= The distribution of Water Point at Osun State are not randomly distributed.\nThe null hypothesis will be rejected if p-value if smaller than alpha value of 0.001.\nThe code chunk below is used to perform the hypothesis testing\n\n#L_ck.csr <- envelope(nigeria_osun_PPP, Lest, nsim = 39, rank = 1, glocal=TRUE)\n\n\n#plot(L_ck.csr, . - r ~ r, xlab=\"d\", ylab=\"L(d)-r\")\n\nFrom the chart, given the L_ck.csr plotted graph exited the envelope this shows that it is lesser than 0.05 of 95% confidence interval, H0 is rejected and we can conclude that the distribution of waterpoints are not randomly distributed\n\n\n\n\n\n\n\n# wpdx_lclq <- wpdx_sf_nga %>% \n#   filter(status_clean %in% \n#            c(\"Functional\",\n#             \"Non-Functional\"))\n\n\n# tmap_mode(\"view\")\n# tm_shape(osun) +\n#   tm_polygons() +\n#   tm_shape(wpdx_lclq) +\n#   tm_dots(col = \"status_clean\", \n#           size = 0.01,\n#           border.col = \"black\",\n#           border.lwd = 0.5)\n\n\n\n\n\n\n# nb <- include_self(\n#   st_knn(st_geometry(wpdx_lclq), 6))\n\n\n# wt <- st_kernel_weights(nb,\n#                         wpdx_lclq, \n#                         \"gaussian\",\n#                         adaptive = TRUE)\n\n\n# Functional <- wpdx_lclq %>%\n#   filter(status_clean == \"Functional\")\n# A <- Functional$status_clean\n\n\n# NonFunctional <- wpdx_lclq %>%\n#   filter(status_clean == \"Non-Functional\")\n# B <- NonFunctional$status_clean\n\n\n# LCLQ <- local_colocation(A, B, nb, wt , 20)\n\n\n# LCLQ_sf_nga <- cbind(wpdx_lclq, LCLQ)\n\n\n# tmap_mode(\"view\")\n# tm_shape(osun) +\n#   tm_polygons() +\n#   tm_shape(LCLQ_sf_nga) +\n#   tm_dots(col = \"Non.Functional\", \n#           size = 0.01,\n#           border.col = \"black\",\n#           border.lwd = 0.5)\n\n\nAfter applying the local colocation quotients for the Osun State, we can conclude that there are many ‘missing’ water points in Osun which implies that there is no correlation between the water points.\nAfter performing the ‘clark evans test’, ‘L function’ and the ‘local colocation quotients’, we can conclude that H0 is rejected, the functional and non functional water points in Osun State are not randomly distributed. In addition, there is no correlation between the water points.\nNote to Professor,\nI have commented out the code chunks above after section 6.2 because when rendered, the take_home_ex1.html file is over 70mb which I have issue committing and pushing to my github repository. The images from section 6.2 onwards are statically pasted to reduce the file size.\nEnd of take home exercise 1"
  },
  {
    "objectID": "Take_home_ex/Take_home_ex2.html",
    "href": "Take_home_ex/Take_home_ex2.html",
    "title": "Take Home Exercise 2",
    "section": "",
    "text": "Since late December 2019, an outbreak of a novel coronavirus disease (COVID-19; previously known as 2019-nCoV) was reported in Wuhan, China, which had subsequently affected 210 countries worldwide. In general, COVID-19 is an acute resolved disease but it can also be deadly, with a 2% case fatality rate.\nThe COVID-19 vaccination in Indonesia is an ongoing mass immunisation in response to the COVID-19 pandemic in Indonesia. On 13 January 2021, the program commenced when President Joko Widodo was vaccinated at the presidential palace. In terms of total doses given, Indonesia ranks third in Asia and fifth in the world.\nAccording to wikipedia, as of 5 February 2023 at 18:00 WIB (UTC+7), 204,266,655 people had received the first dose of the vaccine and 175,131,893 people had been fully vaccinated; 69,597,474 of them had been inoculated with the booster or the third dose, while 1,585,164 had received the fourth dose. Jakarta has the highest percentage of population fully vaccinated with 103.46%, followed by Bali and Special Region of Yogyakarta with 85.45% and 83.02% respectively.\nDespite its compactness, the cumulative vaccination rate are not evenly distributed within DKI Jakarta. The question is where are the sub-districts with relatively higher number of vaccination rate and how they changed over time.\n\n\n\nExploratory Spatial Data Analysis (ESDA) hold tremendous potential to address complex problems facing society. In this study, you are tasked to apply appropriate Local Indicators of Spatial Association (LISA) and Emerging Hot Spot Analysis (EHSA) to undercover the spatio-temporal trends of COVID-19 vaccination in DKI Jakarta.\nf## The Task\nThe specific tasks of this take-home exercise are as follows:\n\n\n\nCompute the monthly vaccination rate from July 2021 to June 2022 at sub-district (also known as kelurahan in Bahasa Indonesia) level,\nPrepare the monthly vaccination rate maps by using appropriate tmap functions,\nDescribe the spatial patterns revealed by the choropleth maps (not more than 200 words).\n\n\n\n\nWith reference to the vaccination rate maps prepared in ESDA:\n\nCompute local Gi* values of the monthly vaccination rate,\nDisplay the Gi* maps of the monthly vaccination rate. The maps should only display the significant (i.e. p-value < 0.05)\nWith reference to the analysis results, draw statistical conclusions (not more than 250 words).\n\n\n\n\nWith reference to the local Gi* values of the vaccination rate maps prepared in the previous section:\n\nPerform Mann-Kendall Test by using the spatio-temporal local Gi* values,\nSelect three sub-districts and describe the temporal trends revealed (not more than 250 words), and\nPrepared a EHSA map of the Gi* values of vaccination rate. The maps should only display the significant (i.e. p-value < 0.05).\nWith reference to the EHSA map prepared, describe the spatial patterns revealed. (not more than 250 words).\n\n\n\n\n\n\n\nFor the purpose of this assignment, data from Riwayat File Vaksinasi DKI Jakarta will be used. Daily vaccination data are provides. You are only required to download either the first day of the month or last day of the month of the study period.\n\n\n\nFor the purpose of this study, DKI Jakarta administration boundary 2019 will be used. The data set can be downloaded at Indonesia Geospatial portal, specifically at this page.\nNote\n\nThe national Projected Coordinates Systems of Indonesia is DGN95 / Indonesia TM-3 zone 54.1.\nExclude all the outer islands from the DKI Jakarta sf data frame, and\nRetain the first nine fields in the DKI Jakarta sf data frame. The ninth field JUMLAH_PEN = Total Population.\nReference was taken from the senior sample submissions for the code for this section, with credit to Megan - https://is415-msty.netlify.app/posts/2021-09-10-take-home-exercise-1/"
  },
  {
    "objectID": "Take_home_ex/Take_home_ex2.html#importing-the-geospatial-data",
    "href": "Take_home_ex/Take_home_ex2.html#importing-the-geospatial-data",
    "title": "Take Home Exercise 2",
    "section": "1.1 Importing the Geospatial Data",
    "text": "1.1 Importing the Geospatial Data\nThe code chunk below uses st_read() of sf package to import BATAS_DESA_DESEMBER_2019_DUKCAPIL_DKI_JAKARTA shapefile into R. The imported shapefile will be simple features Object of sf. As we can see, the assigned coordinates system is WGS 84, the ‘World Geodetic System 1984’. In the context of this dataset, this isn’t appropriate: as this is an Indonesian-specific geospatial dataset, we should be using the national CRS of Indonesia, DGN95, the ‘Datum Geodesi Nasional 1995’, ESPG code 23845. st_transform will be used to rectify the coordinate system\n\nbd_jakarta <- st_read(dsn = \"Data/Geospatial\", \n                 layer = \"BATAS_DESA_DESEMBER_2019_DUKCAPIL_DKI_JAKARTA\")\n\nReading layer `BATAS_DESA_DESEMBER_2019_DUKCAPIL_DKI_JAKARTA' from data source \n  `C:\\Users\\kwekm\\Desktop\\SMU Year 3 Semester 2\\IS415 Geospatial Analytics and Applications\\KMRCrazyDuck\\IS415-KMR\\Take_home_ex\\Data\\Geospatial' \n  using driver `ESRI Shapefile'\nSimple feature collection with 269 features and 161 fields\nGeometry type: MULTIPOLYGON\nDimension:     XY\nBounding box:  xmin: 106.3831 ymin: -6.370815 xmax: 106.9728 ymax: -5.184322\nGeodetic CRS:  WGS 84\n\n\nFrom the output message we can see that there are 269 features and 161 fields. The assigned CRS is WGS 84, the ‘World Geodetic System 1984’. This is not right, and will be rectify that later."
  },
  {
    "objectID": "Take_home_ex/Take_home_ex2.html#data-pre-processing",
    "href": "Take_home_ex/Take_home_ex2.html#data-pre-processing",
    "title": "Take Home Exercise 2",
    "section": "1.2 Data Pre-Processing",
    "text": "1.2 Data Pre-Processing\n\n1.2.1 Check for Missing Values\nNow lets check if there are any missing values\n\nbd_jakarta[rowSums(is.na(bd_jakarta))!=0,]\n\nSimple feature collection with 2 features and 161 fields\nGeometry type: MULTIPOLYGON\nDimension:     XY\nBounding box:  xmin: 106.8412 ymin: -6.154036 xmax: 106.8612 ymax: -6.144973\nGeodetic CRS:  WGS 84\n    OBJECT_ID KODE_DESA             DESA   KODE    PROVINSI KAB_KOTA KECAMATAN\n243     25645  31888888     DANAU SUNTER 318888 DKI JAKARTA     <NA>      <NA>\n244     25646  31888888 DANAU SUNTER DLL 318888 DKI JAKARTA     <NA>      <NA>\n    DESA_KELUR JUMLAH_PEN JUMLAH_KK LUAS_WILAY KEPADATAN PERPINDAHA JUMLAH_MEN\n243       <NA>          0         0          0         0          0          0\n244       <NA>          0         0          0         0          0          0\n    PERUBAHAN WAJIB_KTP SILAM KRISTEN KHATOLIK HINDU BUDHA KONGHUCU KEPERCAYAA\n243         0         0     0       0        0     0     0        0          0\n244         0         0     0       0        0     0     0        0          0\n    PRIA WANITA BELUM_KAWI KAWIN CERAI_HIDU CERAI_MATI U0 U5 U10 U15 U20 U25\n243    0      0          0     0          0          0  0  0   0   0   0   0\n244    0      0          0     0          0          0  0  0   0   0   0   0\n    U30 U35 U40 U45 U50 U55 U60 U65 U70 U75 TIDAK_BELU BELUM_TAMA TAMAT_SD SLTP\n243   0   0   0   0   0   0   0   0   0   0          0          0        0    0\n244   0   0   0   0   0   0   0   0   0   0          0          0        0    0\n    SLTA DIPLOMA_I DIPLOMA_II DIPLOMA_IV STRATA_II STRATA_III BELUM_TIDA\n243    0         0          0          0         0          0          0\n244    0         0          0          0         0          0          0\n    APARATUR_P TENAGA_PEN WIRASWASTA PERTANIAN NELAYAN AGAMA_DAN PELAJAR_MA\n243          0          0          0         0       0         0          0\n244          0          0          0         0       0         0          0\n    TENAGA_KES PENSIUNAN LAINNYA GENERATED KODE_DES_1 BELUM_ MENGUR_ PELAJAR_\n243          0         0       0      <NA>       <NA>      0       0        0\n244          0         0       0      <NA>       <NA>      0       0        0\n    PENSIUNA_1 PEGAWAI_ TENTARA KEPOLISIAN PERDAG_ PETANI PETERN_ NELAYAN_1\n243          0        0       0          0       0      0       0         0\n244          0        0       0          0       0      0       0         0\n    INDUSTR_ KONSTR_ TRANSP_ KARYAW_ KARYAW1 KARYAW1_1 KARYAW1_12 BURUH BURUH_\n243        0       0       0       0       0         0          0     0      0\n244        0       0       0       0       0         0          0     0      0\n    BURUH1 BURUH1_1 PEMBANT_ TUKANG TUKANG_1 TUKANG_12 TUKANG__13 TUKANG__14\n243      0        0        0      0        0         0          0          0\n244      0        0        0      0        0         0          0          0\n    TUKANG__15 TUKANG__16 TUKANG__17 PENATA PENATA_ PENATA1_1 MEKANIK SENIMAN_\n243          0          0          0      0       0         0       0        0\n244          0          0          0      0       0         0       0        0\n    TABIB PARAJI_ PERANCA_ PENTER_ IMAM_M PENDETA PASTOR WARTAWAN USTADZ JURU_M\n243     0       0        0       0      0       0      0        0      0      0\n244     0       0        0       0      0       0      0        0      0      0\n    PROMOT ANGGOTA_ ANGGOTA1 ANGGOTA1_1 PRESIDEN WAKIL_PRES ANGGOTA1_2\n243      0        0        0          0        0          0          0\n244      0        0        0          0        0          0          0\n    ANGGOTA1_3 DUTA_B GUBERNUR WAKIL_GUBE BUPATI WAKIL_BUPA WALIKOTA WAKIL_WALI\n243          0      0        0          0      0          0        0          0\n244          0      0        0          0      0          0        0          0\n    ANGGOTA1_4 ANGGOTA1_5 DOSEN GURU PILOT PENGACARA_ NOTARIS ARSITEK AKUNTA_\n243          0          0     0    0     0          0       0       0       0\n244          0          0     0    0     0          0       0       0       0\n    KONSUL_ DOKTER BIDAN PERAWAT APOTEK_ PSIKIATER PENYIA_ PENYIA1 PELAUT\n243       0      0     0       0       0         0       0       0      0\n244       0      0     0       0       0         0       0       0      0\n    PENELITI SOPIR PIALAN PARANORMAL PEDAGA_ PERANG_ KEPALA_ BIARAW_ WIRASWAST_\n243        0     0      0          0       0       0       0       0          0\n244        0     0      0          0       0       0       0       0          0\n    LAINNYA_12 LUAS_DESA KODE_DES_3 DESA_KEL_1 KODE_12\n243          0         0       <NA>       <NA>       0\n244          0         0       <NA>       <NA>       0\n                          geometry\n243 MULTIPOLYGON (((106.8612 -6...\n244 MULTIPOLYGON (((106.8504 -6...\n\n\nThere are 2 rows containing ‘NA’ values. However, the data is big, we need to find columns with missing NA values to remove it.\n\nnames(which(colSums(is.na(bd_jakarta))>0))\n\n[1] \"KAB_KOTA\"   \"KECAMATAN\"  \"DESA_KELUR\" \"GENERATED\"  \"KODE_DES_1\"\n[6] \"KODE_DES_3\" \"DESA_KEL_1\"\n\n\nWe can see that there are two particular rows with missing values for KAB_KOTA (City), KECAMATAN (District) and DESA_KELUR (Village).\nHence, we remove rows with NA value in DESA_KELUR. There are other columns with NA present as well, however, since we are only looking at the sub-district level, it is most appropriate to remove DESA_KELUR.\n\nbd_jakarta <- na.omit(bd_jakarta,c(\"DESA_KELUR\"))\n\nTo double check if the rows with missing values are removed\n\nbd_jakarta[rowSums(is.na(bd_jakarta))!=0,]\n\nSimple feature collection with 0 features and 161 fields\nBounding box:  xmin: NA ymin: NA xmax: NA ymax: NA\nGeodetic CRS:  WGS 84\n  [1] OBJECT_ID  KODE_DESA  DESA       KODE       PROVINSI   KAB_KOTA  \n  [7] KECAMATAN  DESA_KELUR JUMLAH_PEN JUMLAH_KK  LUAS_WILAY KEPADATAN \n [13] PERPINDAHA JUMLAH_MEN PERUBAHAN  WAJIB_KTP  SILAM      KRISTEN   \n [19] KHATOLIK   HINDU      BUDHA      KONGHUCU   KEPERCAYAA PRIA      \n [25] WANITA     BELUM_KAWI KAWIN      CERAI_HIDU CERAI_MATI U0        \n [31] U5         U10        U15        U20        U25        U30       \n [37] U35        U40        U45        U50        U55        U60       \n [43] U65        U70        U75        TIDAK_BELU BELUM_TAMA TAMAT_SD  \n [49] SLTP       SLTA       DIPLOMA_I  DIPLOMA_II DIPLOMA_IV STRATA_II \n [55] STRATA_III BELUM_TIDA APARATUR_P TENAGA_PEN WIRASWASTA PERTANIAN \n [61] NELAYAN    AGAMA_DAN  PELAJAR_MA TENAGA_KES PENSIUNAN  LAINNYA   \n [67] GENERATED  KODE_DES_1 BELUM_     MENGUR_    PELAJAR_   PENSIUNA_1\n [73] PEGAWAI_   TENTARA    KEPOLISIAN PERDAG_    PETANI     PETERN_   \n [79] NELAYAN_1  INDUSTR_   KONSTR_    TRANSP_    KARYAW_    KARYAW1   \n [85] KARYAW1_1  KARYAW1_12 BURUH      BURUH_     BURUH1     BURUH1_1  \n [91] PEMBANT_   TUKANG     TUKANG_1   TUKANG_12  TUKANG__13 TUKANG__14\n [97] TUKANG__15 TUKANG__16 TUKANG__17 PENATA     PENATA_    PENATA1_1 \n[103] MEKANIK    SENIMAN_   TABIB      PARAJI_    PERANCA_   PENTER_   \n[109] IMAM_M     PENDETA    PASTOR     WARTAWAN   USTADZ     JURU_M    \n[115] PROMOT     ANGGOTA_   ANGGOTA1   ANGGOTA1_1 PRESIDEN   WAKIL_PRES\n[121] ANGGOTA1_2 ANGGOTA1_3 DUTA_B     GUBERNUR   WAKIL_GUBE BUPATI    \n[127] WAKIL_BUPA WALIKOTA   WAKIL_WALI ANGGOTA1_4 ANGGOTA1_5 DOSEN     \n[133] GURU       PILOT      PENGACARA_ NOTARIS    ARSITEK    AKUNTA_   \n[139] KONSUL_    DOKTER     BIDAN      PERAWAT    APOTEK_    PSIKIATER \n[145] PENYIA_    PENYIA1    PELAUT     PENELITI   SOPIR      PIALAN    \n[151] PARANORMAL PEDAGA_    PERANG_    KEPALA_    BIARAW_    WIRASWAST_\n[157] LAINNYA_12 LUAS_DESA  KODE_DES_3 DESA_KEL_1 KODE_12    geometry  \n<0 rows> (or 0-length row.names)\n\n\n\n\n1.2.2 Transforming Coordinates\nPreviously as mentioned it uses the WGS 84 coordinate system. The data is using a Geographic projected system, however, this is system is not appropriate since we need to use distance and area measures.\n\nst_crs(bd_jakarta)\n\nCoordinate Reference System:\n  User input: WGS 84 \n  wkt:\nGEOGCRS[\"WGS 84\",\n    DATUM[\"World Geodetic System 1984\",\n        ELLIPSOID[\"WGS 84\",6378137,298.257223563,\n            LENGTHUNIT[\"metre\",1]]],\n    PRIMEM[\"Greenwich\",0,\n        ANGLEUNIT[\"degree\",0.0174532925199433]],\n    CS[ellipsoidal,2],\n        AXIS[\"latitude\",north,\n            ORDER[1],\n            ANGLEUNIT[\"degree\",0.0174532925199433]],\n        AXIS[\"longitude\",east,\n            ORDER[2],\n            ANGLEUNIT[\"degree\",0.0174532925199433]],\n    ID[\"EPSG\",4326]]\n\n\nTherefore, we use st_transform() and not st_set_crs() as st_set_crs() assigns the EPSG code to the data frame. And we need to transform the data frame from geographic to projected coordinate system. We will be using crs=23845 (found from the EPSG for Indonesia).\n\nbd_jakarta <- st_transform(bd_jakarta, 23845)\n\nCheck if CRS has been assigned\n\nst_crs(bd_jakarta)\n\nCoordinate Reference System:\n  User input: EPSG:23845 \n  wkt:\nPROJCRS[\"DGN95 / Indonesia TM-3 zone 54.1\",\n    BASEGEOGCRS[\"DGN95\",\n        DATUM[\"Datum Geodesi Nasional 1995\",\n            ELLIPSOID[\"WGS 84\",6378137,298.257223563,\n                LENGTHUNIT[\"metre\",1]]],\n        PRIMEM[\"Greenwich\",0,\n            ANGLEUNIT[\"degree\",0.0174532925199433]],\n        ID[\"EPSG\",4755]],\n    CONVERSION[\"Indonesia TM-3 zone 54.1\",\n        METHOD[\"Transverse Mercator\",\n            ID[\"EPSG\",9807]],\n        PARAMETER[\"Latitude of natural origin\",0,\n            ANGLEUNIT[\"degree\",0.0174532925199433],\n            ID[\"EPSG\",8801]],\n        PARAMETER[\"Longitude of natural origin\",139.5,\n            ANGLEUNIT[\"degree\",0.0174532925199433],\n            ID[\"EPSG\",8802]],\n        PARAMETER[\"Scale factor at natural origin\",0.9999,\n            SCALEUNIT[\"unity\",1],\n            ID[\"EPSG\",8805]],\n        PARAMETER[\"False easting\",200000,\n            LENGTHUNIT[\"metre\",1],\n            ID[\"EPSG\",8806]],\n        PARAMETER[\"False northing\",1500000,\n            LENGTHUNIT[\"metre\",1],\n            ID[\"EPSG\",8807]]],\n    CS[Cartesian,2],\n        AXIS[\"easting (X)\",east,\n            ORDER[1],\n            LENGTHUNIT[\"metre\",1]],\n        AXIS[\"northing (Y)\",north,\n            ORDER[2],\n            LENGTHUNIT[\"metre\",1]],\n    USAGE[\n        SCOPE[\"Cadastre.\"],\n        AREA[\"Indonesia - onshore east of 138°E.\"],\n        BBOX[-9.19,138,-1.49,141.01]],\n    ID[\"EPSG\",23845]]\n\n\n\n\n1.2.3 Removal of the Outer Island\nWe have done our basic pre-processing, lets quickly visualize the data\n\nplot(st_geometry(bd_jakarta))\n\n\n\n\nAs we can see from the diagram, bd_jakarta includes both mainland and outer islands. And since we don’t require the outer islands (as per the requirements), we can remove them.\nWe know that the date is grouped by KAB_KOTA (City), KECAMATAN (Sub-District) and DESA_KELUR (Village). Now, lets plot the map and see how we can use KAB_KOTA to remove the outer islands.\n\ntm_shape(bd_jakarta) + \n  tm_polygons(\"KAB_KOTA\")\n\n\n\n\nFrom the map, we can see that all the cities in Jakarta start with ‘Jakarta’ as their prefix and hence, ‘Kepulauan Seribu’ are the other outer islands. When translated in English, the name means ‘Thousand Islands’. Now we know what to remove, and we shall proceed with that.\n\nbd_jakarta <- filter(bd_jakarta, KAB_KOTA != \"KEPULAUAN SERIBU\")\n\nNow, lets double check if the outer islands have been removed.\n\ntm_shape(bd_jakarta) + \n  tm_polygons(\"KAB_KOTA\")\n\n\n\n\n\n\n1.2.4 To retain the first 9 columns as requested\n\nbd_jakarta <- bd_jakarta[, 0:9]\n\n\n\n1.2.5 Renaming columns to English\n\nbd_jakarta <- bd_jakarta %>% \n  dplyr::rename(\n    Object_ID=OBJECT_ID,\n    Village_Code=KODE_DESA, \n    Village=DESA,\n    Code=KODE,\n    Province=PROVINSI, \n    City=KAB_KOTA, \n    District=KECAMATAN, \n    Sub_District=DESA_KELUR,\n    Total_Population=JUMLAH_PEN\n    )"
  },
  {
    "objectID": "Take_home_ex/Take_home_ex2.html#importing-eda",
    "href": "Take_home_ex/Take_home_ex2.html#importing-eda",
    "title": "Take Home Exercise 2",
    "section": "2.1 Importing EDA",
    "text": "2.1 Importing EDA\nFor this take home exercise 2, we will be working on data from July 2021 to June 2022, as a result we will be having several excel files.\n\njul2021 <- read_xlsx(\"Data/Aspatial/Data Vaksinasi Berbasis Kelurahan (31 Juli 2021).xlsx\")\n\nglimpse(jul2021)\n\nRows: 268\nColumns: 27\n$ `KODE KELURAHAN`                             <chr> NA, \"3172051003\", \"317304…\n$ `WILAYAH KOTA`                               <chr> NA, \"JAKARTA UTARA\", \"JAK…\n$ KECAMATAN                                    <chr> NA, \"PADEMANGAN\", \"TAMBOR…\n$ KELURAHAN                                    <chr> \"TOTAL\", \"ANCOL\", \"ANGKE\"…\n$ SASARAN                                      <dbl> 8941211, 23947, 29381, 29…\n$ `BELUM VAKSIN`                               <dbl> 4441501, 12333, 13875, 18…\n$ `JUMLAH\\r\\nDOSIS 1`                          <dbl> 4499710, 11614, 15506, 10…\n$ `JUMLAH\\r\\nDOSIS 2`                          <dbl> 1663218, 4181, 4798, 3658…\n$ `TOTAL VAKSIN\\r\\nDIBERIKAN`                  <dbl> 6162928, 15795, 20304, 14…\n$ `LANSIA\\r\\nDOSIS 1`                          <dbl> 502579, 1230, 2012, 865, …\n$ `LANSIA\\r\\nDOSIS 2`                          <dbl> 440910, 1069, 1729, 701, …\n$ `LANSIA TOTAL \\r\\nVAKSIN DIBERIKAN`          <dbl> 943489, 2299, 3741, 1566,…\n$ `PELAYAN PUBLIK\\r\\nDOSIS 1`                  <dbl> 1052883, 3333, 2586, 2837…\n$ `PELAYAN PUBLIK\\r\\nDOSIS 2`                  <dbl> 666009, 2158, 1374, 1761,…\n$ `PELAYAN PUBLIK TOTAL\\r\\nVAKSIN DIBERIKAN`   <dbl> 1718892, 5491, 3960, 4598…\n$ `GOTONG ROYONG\\r\\nDOSIS 1`                   <dbl> 56660, 78, 122, 174, 71, …\n$ `GOTONG ROYONG\\r\\nDOSIS 2`                   <dbl> 38496, 51, 84, 106, 57, 7…\n$ `GOTONG ROYONG TOTAL\\r\\nVAKSIN DIBERIKAN`    <dbl> 95156, 129, 206, 280, 128…\n$ `TENAGA KESEHATAN\\r\\nDOSIS 1`                <dbl> 76397, 101, 90, 215, 73, …\n$ `TENAGA KESEHATAN\\r\\nDOSIS 2`                <dbl> 67484, 91, 82, 192, 67, 3…\n$ `TENAGA KESEHATAN TOTAL\\r\\nVAKSIN DIBERIKAN` <dbl> 143881, 192, 172, 407, 14…\n$ `TAHAPAN 3\\r\\nDOSIS 1`                       <dbl> 2279398, 5506, 9012, 5408…\n$ `TAHAPAN 3\\r\\nDOSIS 2`                       <dbl> 446028, 789, 1519, 897, 4…\n$ `TAHAPAN 3 TOTAL\\r\\nVAKSIN DIBERIKAN`        <dbl> 2725426, 6295, 10531, 630…\n$ `REMAJA\\r\\nDOSIS 1`                          <dbl> 531793, 1366, 1684, 1261,…\n$ `REMAJA\\r\\nDOSIS 2`                          <dbl> 4291, 23, 10, 1, 1, 8, 6,…\n$ `REMAJA TOTAL\\r\\nVAKSIN DIBERIKAN`           <dbl> 536084, 1389, 1694, 1262,…\n\n\nFrom opening up the excel file till February 2022, the number of columns is 27. However, from March 2022 the number of columns is 34. Upon identifying the difference between the number of columns, the data files from March 2022 has a separate column for 3rd dosage, where has all the data files before that don’t have 3rd dosage column."
  },
  {
    "objectID": "Take_home_ex/Take_home_ex2.html#creating-aspatial-data-pre-processing-function",
    "href": "Take_home_ex/Take_home_ex2.html#creating-aspatial-data-pre-processing-function",
    "title": "Take Home Exercise 2",
    "section": "2.2 Creating Aspatial Data Pre-Processing Function",
    "text": "2.2 Creating Aspatial Data Pre-Processing Function\nFor take home exercise 2, we don’t require all the columns. Only the following columns are required -\nKODE KELURAHAN (Sub-District Code)\nKELURAHAN (Sub-District)\nSASARAN (Target)\nBELUM VASKIN (Yet to be vaccinated / Not yet vaccinated)\nThis solves the issue of some months having extra columns. However, we need to create an ‘Date’ column that shows the month and year of the observation, which is originally the file name. Each file has the naming convention ’Data Vaksinasi Berbasis Keluarahan (DD Month YYYY).\nWe will be combining the mentioned steps into a function\n\n# takes in an aspatial data filepath and returns a processed output\naspatial_preprocess <- function(filepath){\n  # We have to remove the first row of the file (subheader row) and hence, we use [-1,] to remove it.\n  result_file <- read_xlsx(filepath)[-1,]\n  \n  # We then create the Date Column, the format of our files is: Data Vaksinasi Berbasis Kelurahan (DD Month YYYY)\n  # While the start is technically \"(\", \"(\" is part of a regular expression and leads to a warning message, so we'll use \"Kelurahan\" instead. The [[1]] refers to the first element in the list.\n  # We're loading it as DD-Month-YYYY format\n  # We use the length of the filepath '6' to get the end index (which has our Date)\n  # as such, the most relevant functions are substr (returns a substring) and either str_locate (returns location of substring as an integer matrix) or gregexpr (returns a list of locations of substring)\n  # reference https://stackoverflow.com/questions/14249562/find-the-location-of-a-character-in-string\n  startpoint <- gregexpr(pattern=\"Kelurahan\", filepath)[[1]] + 11\n  \n  result_file$Date <- substr(filepath, startpoint, nchar(filepath)-6)\n  \n  # Retain the Relevant Columns\n  result_file <- result_file %>% \n    select(\"Date\", \n           \"KODE KELURAHAN\", \n           \"KELURAHAN\", \n           \"SASARAN\", \n           \"BELUM VAKSIN\")\n  return(result_file)\n}"
  },
  {
    "objectID": "Take_home_ex/Take_home_ex2.html#feed-files-into-aspatial-function",
    "href": "Take_home_ex/Take_home_ex2.html#feed-files-into-aspatial-function",
    "title": "Take Home Exercise 2",
    "section": "2.3 Feed files into Aspatial function",
    "text": "2.3 Feed files into Aspatial function\nInstead of manually feeding the files, line by line, we will be using the function list.files() and lapply() to get our process done quicker.\n\n# in the folder 'data/aspatial', find files with the extension '.xlsx' and add it to our fileslist \n# the full.names=TRUE prepends the directory path to the file names, giving a relative file path - otherwise, only the file names (not the paths) would be returned \n# reference: https://stat.ethz.ch/R-manual/R-devel/library/base/html/list.files.html\nfileslist <-list.files(path = \"data/aspatial\", pattern = \"*.xlsx\", full.names=TRUE)\n\n# afterwards, for every element in fileslist, apply aspatial_process function\ndflist <- lapply(seq_along(fileslist), function(x) aspatial_preprocess(fileslist[x]))\n\nWe will then convert the dflist into an actual dataframe with ldply() using the below code\n\nvaccination_jakarta <- ldply(dflist, data.frame)\n\nNow, lets take a look into our data\n\nglimpse(vaccination_jakarta)\n\nRows: 3,204\nColumns: 5\n$ Date           <chr> \"27 Februari 2022\", \"27 Februari 2022\", \"27 Februari 20…\n$ KODE.KELURAHAN <chr> \"3172051003\", \"3173041007\", \"3175041005\", \"3175031003\",…\n$ KELURAHAN      <chr> \"ANCOL\", \"ANGKE\", \"BALE KAMBANG\", \"BALI MESTER\", \"BAMBU…\n$ SASARAN        <dbl> 23947, 29381, 29074, 9752, 26285, 21566, 23886, 47898, …\n$ BELUM.VAKSIN   <dbl> 4592, 5319, 5903, 1649, 4030, 3950, 3344, 9382, 3772, 7…"
  },
  {
    "objectID": "Take_home_ex/Take_home_ex2.html#formatting-date-column",
    "href": "Take_home_ex/Take_home_ex2.html#formatting-date-column",
    "title": "Take Home Exercise 2",
    "section": "2.4 Formatting Date Column",
    "text": "2.4 Formatting Date Column\nThe Dates are in Bahasa Indonesia, and hence, we need to translate them to English for ease of use. However, since the values in Date column were derived from sub-strings, they are in a string format and thus, first need to be converted to datetime.\n\n# parses the 'Date' column into Month(Full Name)-YYYY datetime objects\n# reference: https://stackoverflow.com/questions/53380650/b-y-date-conversion-gives-na\n\n# locale=\"ind\" means that the locale has been set as Indonesia\nSys.setlocale(locale=\"ind\")\n\n[1] \"LC_COLLATE=Indonesian_Indonesia.1252;LC_CTYPE=Indonesian_Indonesia.1252;LC_MONETARY=Indonesian_Indonesia.1252;LC_NUMERIC=C;LC_TIME=Indonesian_Indonesia.1252\"\n\n\n\nvaccination_jakarta$Date <- c(vaccination_jakarta$Date) %>% \n  as.Date(vaccination_jakarta$Date, format =\"%d %B %Y\")\n\nglimpse(vaccination_jakarta)\n\nRows: 3,204\nColumns: 5\n$ Date           <date> 2022-02-27, 2022-02-27, 2022-02-27, 2022-02-27, 2022-0~\n$ KODE.KELURAHAN <chr> \"3172051003\", \"3173041007\", \"3175041005\", \"3175031003\",~\n$ KELURAHAN      <chr> \"ANCOL\", \"ANGKE\", \"BALE KAMBANG\", \"BALI MESTER\", \"BAMBU~\n$ SASARAN        <dbl> 23947, 29381, 29074, 9752, 26285, 21566, 23886, 47898, ~\n$ BELUM.VAKSIN   <dbl> 4592, 5319, 5903, 1649, 4030, 3950, 3344, 9382, 3772, 7~"
  },
  {
    "objectID": "Take_home_ex/Take_home_ex2.html#rename-columns-into-english",
    "href": "Take_home_ex/Take_home_ex2.html#rename-columns-into-english",
    "title": "Take Home Exercise 2",
    "section": "2.5 Rename columns into English",
    "text": "2.5 Rename columns into English\n\n# renames the columns in the style New_Name = OLD_NAME\nvaccination_jakarta <- vaccination_jakarta %>% \n  dplyr::rename(\n    Date=Date,\n    Sub_District_Code=KODE.KELURAHAN,\n    Sub_District=KELURAHAN, \n    Target=SASARAN, \n    Not_Yet_Vaccinated=BELUM.VAKSIN\n    )\n\n\nglimpse(vaccination_jakarta)\n\nRows: 3,204\nColumns: 5\n$ Date               <date> 2022-02-27, 2022-02-27, 2022-02-27, 2022-02-27, 20~\n$ Sub_District_Code  <chr> \"3172051003\", \"3173041007\", \"3175041005\", \"31750310~\n$ Sub_District       <chr> \"ANCOL\", \"ANGKE\", \"BALE KAMBANG\", \"BALI MESTER\", \"B~\n$ Target             <dbl> 23947, 29381, 29074, 9752, 26285, 21566, 23886, 478~\n$ Not_Yet_Vaccinated <dbl> 4592, 5319, 5903, 1649, 4030, 3950, 3344, 9382, 377~"
  },
  {
    "objectID": "Take_home_ex/Take_home_ex2.html#further-data-processing",
    "href": "Take_home_ex/Take_home_ex2.html#further-data-processing",
    "title": "Take Home Exercise 2",
    "section": "2.6 Further data processing",
    "text": "2.6 Further data processing\nFurther perform any pre-processing to check out for anything we might have missed.\n\nvaccination_jakarta[rowSums(is.na(vaccination_jakarta))!=0,]\n\n[1] Date               Sub_District_Code  Sub_District       Target            \n[5] Not_Yet_Vaccinated\n<0 rows> (or 0-length row.names)\n\n\nFrom the output, we can see there are no missing values."
  },
  {
    "objectID": "Take_home_ex/Take_home_ex2.html#initial-exploratory-data-analysis",
    "href": "Take_home_ex/Take_home_ex2.html#initial-exploratory-data-analysis",
    "title": "Take Home Exercise 2",
    "section": "3.1 Initial Exploratory Data Analysis",
    "text": "3.1 Initial Exploratory Data Analysis\nWe have both our Geospatial and Aspatial data, we need to join them. However, we need to first find a common header to join them.\n\ncolnames(bd_jakarta)\n\n [1] \"Object_ID\"        \"Village_Code\"     \"Village\"          \"Code\"            \n [5] \"Province\"         \"City\"             \"District\"         \"Sub_District\"    \n [9] \"Total_Population\" \"geometry\"        \n\n\n\ncolnames(vaccination_jakarta)\n\n[1] \"Date\"               \"Sub_District_Code\"  \"Sub_District\"      \n[4] \"Target\"             \"Not_Yet_Vaccinated\"\n\n\nWe can see that both have Sub_District and hence we can join them by the Sub_District and Sub_District_Code.\n\n# joins vaccination_jakarta to jakarta based on Sub_District and  Sub_District_Code\ncombined_jakarta <- left_join(bd_jakarta, vaccination_jakarta,\n                              by=c(\n                                \"Village_Code\"=\"Sub_District_Code\", \n                                \"Sub_District\"=\"Sub_District\")\n                              )\n\nSubcategorize the data into ‘Target population to be Vaccinated’ , ‘Not Yet Vaccinated Population’ and ‘Total Population’\n\ntarget = tm_shape(combined_jakarta)+\n  tm_fill(\"Target\") +\n  tm_borders(alpha = 0.5) +\n  tm_layout(main.title=\"Target Count\")\n\nnot_yet_vaccinated = tm_shape(combined_jakarta)+\n  tm_fill(\"Not_Yet_Vaccinated\") +\n  tm_borders(alpha = 0.5) +\n  tm_layout(main.title=\"Not Yet Vaccinated Count\")\n\ntotal_population = tm_shape(combined_jakarta)+\n  tm_fill(\"Total_Population\") +\n  tm_borders(alpha = 0.5) +\n  tm_layout(main.title=\"Total Population\")\n\ntmap_arrange(target, not_yet_vaccinated, total_population)\n\n\n\n\nThere seems to be still be a ‘Missing’ value in the Target and Not_Yet_Vaccinated maps. Even though, when we had previously checked for missing values, it didn’t show any missing values. However, we shall double check again.\n\nbd_jakarta[rowSums(is.na(bd_jakarta))!=0,]\n\nSimple feature collection with 0 features and 9 fields\nBounding box:  xmin: NA ymin: NA xmax: NA ymax: NA\nProjected CRS: DGN95 / Indonesia TM-3 zone 54.1\n [1] Object_ID        Village_Code     Village          Code            \n [5] Province         City             District         Sub_District    \n [9] Total_Population geometry        \n<0 rows> (or 0-length row.names)\n\n\n\nvaccination_jakarta[rowSums(is.na(vaccination_jakarta))!=0,]\n\n[1] Date               Sub_District_Code  Sub_District       Target            \n[5] Not_Yet_Vaccinated\n<0 rows> (or 0-length row.names)\n\n\nThere are no missing values in our dataframes. Therefore, the most likely reasons for the missing values must be due to mismatched values when we perform the left-join of the Geospatial and Aspatial data."
  },
  {
    "objectID": "Take_home_ex/Take_home_ex2.html#finding-mismatched-sub-district-records",
    "href": "Take_home_ex/Take_home_ex2.html#finding-mismatched-sub-district-records",
    "title": "Take Home Exercise 2",
    "section": "3.2 Finding mismatched sub-district records",
    "text": "3.2 Finding mismatched sub-district records\nSince, we had conducted left-join using the Sub-District, there must be a mismatch in the naming of the subdistricts. Lets check it by looking at the unique subdistrict names in both bd_jakarta and vaccination_jakarta\n\njakarta_subdistrict <- c(bd_jakarta$Sub_District)\nvaccination_subdistrict <- c(vaccination_jakarta$Sub_District)\n\nunique(jakarta_subdistrict[!(jakarta_subdistrict %in% vaccination_subdistrict)])\n\n[1] \"KRENDANG\"             \"RAWAJATI\"             \"TENGAH\"              \n[4] \"BALEKAMBANG\"          \"PINANGRANTI\"          \"JATIPULO\"            \n[7] \"PALMERIAM\"            \"KRAMATJATI\"           \"HALIM PERDANA KUSUMA\"\n\n\n\nunique(vaccination_subdistrict[!(vaccination_subdistrict %in% jakarta_subdistrict)])\n\n [1] \"BALE KAMBANG\"          \"HALIM PERDANA KUSUMAH\" \"JATI PULO\"            \n [4] \"KAMPUNG TENGAH\"        \"KERENDANG\"             \"KRAMAT JATI\"          \n [7] \"PAL MERIAM\"            \"PINANG RANTI\"          \"PULAU HARAPAN\"        \n[10] \"PULAU KELAPA\"          \"PULAU PANGGANG\"        \"PULAU PARI\"           \n[13] \"PULAU TIDUNG\"          \"PULAU UNTUNG JAWA\"     \"RAWA JATI\"            \n\n\nFrom above there are same names in both but are just written in different ways. However, there are 6 words in the vaccination_subdistrict which are not in the jakarta_subdistrict. We need to take a look into that after we first correct the mismatched values.\n\n# initialise a dataframe of our cases vs bd subdistrict spelling\nspelling <- data.frame(\n  Aspatial_Cases=c(\"BALE KAMBANG\", \"HALIM PERDANA KUSUMAH\", \"JATI PULO\", \"KAMPUNG TENGAH\", \"KERENDANG\", \"KRAMAT JATI\", \"PAL MERIAM\", \"PINANG RANTI\", \"RAWA JATI\"),\n  Geospatial_BD=c(\"BALEKAMBAG\", \"HALIM PERDANA KUSUMA\", \"JATIPULO\", \"TENGAH\", \"KRENDANG\", \"KRAMATJATI\", \"PALMERIAM\", \"PINANGRANTI\", \"RAWAJATI\")\n  )\n\n# with dataframe a input, outputs a kable\nlibrary(knitr)\nlibrary(kableExtra)\nkable(spelling, caption=\"Mismatched Records\") %>%\n  kable_material(\"hover\", latex_options=\"scale_down\")\n\n\n\nMismatched Records\n \n  \n    Aspatial_Cases \n    Geospatial_BD \n  \n \n\n  \n    BALE KAMBANG \n    BALEKAMBAG \n  \n  \n    HALIM PERDANA KUSUMAH \n    HALIM PERDANA KUSUMA \n  \n  \n    JATI PULO \n    JATIPULO \n  \n  \n    KAMPUNG TENGAH \n    TENGAH \n  \n  \n    KERENDANG \n    KRENDANG \n  \n  \n    KRAMAT JATI \n    KRAMATJATI \n  \n  \n    PAL MERIAM \n    PALMERIAM \n  \n  \n    PINANG RANTI \n    PINANGRANTI \n  \n  \n    RAWA JATI \n    RAWAJATI \n  \n\n\n\n\n\nAs we can see these records have the same name, except that there is no standardization. Therefore, there is a mismatch between them. Let’s correct this mismatch\n\n# We are replacing the mistmatched values in jakarta with the correct value\nbd_jakarta$Sub_District[bd_jakarta$Sub_District == 'BALEKAMBANG'] <- 'BALE KAMBANG'\nbd_jakarta$Sub_District[bd_jakarta$Sub_District == 'HALIM PERDANA KUSUMA'] <- 'HALIM PERDANA KUSUMAH'\nbd_jakarta$Sub_District[bd_jakarta$Sub_District == 'JATIPULO'] <- 'JATI PULO'\nbd_jakarta$Sub_District[bd_jakarta$Sub_District == 'KALI BARU'] <- 'KALIBARU'\nbd_jakarta$Sub_District[bd_jakarta$Sub_District == 'TENGAH'] <- 'KAMPUNG TENGAH'\nbd_jakarta$Sub_District[bd_jakarta$Sub_District == 'KRAMATJATI'] <- 'KRAMAT JATI'\nbd_jakarta$Sub_District[bd_jakarta$Sub_District == 'KRENDANG'] <- 'KERENDANG'\nbd_jakarta$Sub_District[bd_jakarta$Sub_District == 'PALMERIAM'] <- 'PAL MERIAM'\nbd_jakarta$Sub_District[bd_jakarta$Sub_District == 'PINANGRANTI'] <- 'PINANG RANTI'\nbd_jakarta$Sub_District[bd_jakarta$Sub_District == 'RAWAJATI'] <- 'RAWA JATI'\n\nThere are 6 subdistrict names that we say in vaccination_jakarta which were not present in jakarta. This ideally suggests that these districts are not a part of Jakarta, Therefore we need to remove them.\n\nvaccination_jakarta <- vaccination_jakarta[!(vaccination_jakarta$Sub_District==\"PULAU HARAPAN\" | vaccination_jakarta$Sub_District==\"PULAU KELAPA\" | vaccination_jakarta$Sub_District==\"PULAU PANGGANG\" | vaccination_jakarta$Sub_District==\"PULAU PARI\" | vaccination_jakarta$Sub_District==\"PULAU TIDUNG\" | vaccination_jakarta$Sub_District==\"PULAU UNTUNG JAWA\"), ]"
  },
  {
    "objectID": "Take_home_ex/Take_home_ex2.html#rejoin-exploratory-data-analysis",
    "href": "Take_home_ex/Take_home_ex2.html#rejoin-exploratory-data-analysis",
    "title": "Take Home Exercise 2",
    "section": "3.3 Rejoin Exploratory Data Analysis",
    "text": "3.3 Rejoin Exploratory Data Analysis\n\n# joins vaccination_jakarta to bd_jakarta based on Sub_District and  Sub_District_Code\ncombined_jakarta <- left_join(bd_jakarta, vaccination_jakarta,\n                              by=c(\n                                \"Village_Code\"=\"Sub_District_Code\", \n                                \"Sub_District\"=\"Sub_District\")\n                              )\n\nCheck if there are any further NA values\n\ncombined_jakarta[rowSums(is.na(combined_jakarta))!=0,]\n\nSimple feature collection with 0 features and 12 fields\nBounding box:  xmin: NA ymin: NA xmax: NA ymax: NA\nProjected CRS: DGN95 / Indonesia TM-3 zone 54.1\n [1] Object_ID          Village_Code       Village            Code              \n [5] Province           City               District           Sub_District      \n [9] Total_Population   Date               Target             Not_Yet_Vaccinated\n[13] geometry          \n<0 rows> (or 0-length row.names)\n\n\nRelook the data into ‘Target population to be Vaccinated’ , ‘Not Yet Vaccinated Population’ and ‘Total Population’\n\ntarget = tm_shape(combined_jakarta)+\n  tm_fill(\"Target\") +\n  tm_borders(alpha = 0.5) +\n  tm_layout(main.title=\"Target Count\")\n\nnot_yet_vaccinated = tm_shape(combined_jakarta)+\n  tm_fill(\"Not_Yet_Vaccinated\") +\n  tm_borders(alpha = 0.5) +\n  tm_layout(main.title=\"Not Yet Vaccinated Count\")\n\ntotal_population = tm_shape(combined_jakarta)+\n  tm_fill(\"Total_Population\") +\n  tm_borders(alpha = 0.5) +\n  tm_layout(main.title=\"Total Population\")\n\ntmap_arrange(target, not_yet_vaccinated, total_population)"
  },
  {
    "objectID": "Take_home_ex/Take_home_ex2.html#convert-dataframe-to-sf",
    "href": "Take_home_ex/Take_home_ex2.html#convert-dataframe-to-sf",
    "title": "Take Home Exercise 2",
    "section": "4.1 Convert dataframe to SF",
    "text": "4.1 Convert dataframe to SF\n\ncombined_jakarta <- st_as_sf(combined_jakarta)\n\n# need to join our previous dataframes with the geospatial data to ensure that geometry column is present\nvaccination_rate <- vaccination_rate%>% left_join(bd_jakarta, by=c(\"Sub_District\"=\"Sub_District\"))\nvaccination_rate <- st_as_sf(vaccination_rate)"
  },
  {
    "objectID": "Take_home_ex/Take_home_ex2.html#jenks-choropleth-mapping",
    "href": "Take_home_ex/Take_home_ex2.html#jenks-choropleth-mapping",
    "title": "Take Home Exercise 2",
    "section": "5.1 Jenks Choropleth Mapping",
    "text": "5.1 Jenks Choropleth Mapping\n\n# using the jenks method, with 6 classes for human eye\ntmap_mode(\"plot\")\ntm_shape(vaccination_rate)+\n  tm_fill(\"2021-07-31\", \n          n= 6,\n          style = \"jenks\", \n          title = \"Vaccination Rate\") +\n  tm_layout(main.title = \"Vaccination Rate in July 2021\",\n            main.title.position = \"center\",\n            main.title.size = 1,\n            legend.height = 0.5, \n            legend.width = 0.4,\n            frame = TRUE) +\n  tm_borders(alpha = 0.5)\n\n\n\n\nPlot for all 12 months. Adopt a helper function to help us do it.\n\n# input: the dataframe and the variable name - in this case, the month \n# with style=\"jenks\" for the jenks classification method\njenks_plot <- function(df, varname) {\n  tm_shape(vaccination_rate) +\n    tm_polygons() +\n  tm_shape(df) +\n    tm_fill(varname, \n          n= 6,\n          style = \"jenks\", \n          title = \"Vaccination Rate\") +\n    tm_layout(main.title = varname,\n          main.title.position = \"center\",\n          main.title.size = 1.2,\n          legend.height = 0.45, \n          legend.width = 0.35,\n          frame = TRUE) +\n    tm_borders(alpha = 0.5)\n}\n\n\ntmap_mode(\"plot\")\ntmap_arrange(jenks_plot(vaccination_rate, \"2021-07-31\"),\n             jenks_plot(vaccination_rate, \"2021-08-31\"),\n             jenks_plot(vaccination_rate, \"2021-09-30\"),\n             jenks_plot(vaccination_rate, \"2021-10-31\"))\n\n\n\n\n\ntmap_mode(\"plot\")\ntmap_arrange(jenks_plot(vaccination_rate, \"2021-11-30\"),\n             jenks_plot(vaccination_rate, \"2021-12-31\"),\n             jenks_plot(vaccination_rate, \"2022-01-31\"),\n             jenks_plot(vaccination_rate, \"2022-02-27\"))\n\n\n\n\n\ntmap_mode(\"plot\")\ntmap_arrange(jenks_plot(vaccination_rate, \"2022-03-31\"),\n             jenks_plot(vaccination_rate, \"2022-04-30\"),\n             jenks_plot(vaccination_rate, \"2022-05-31\"),\n             jenks_plot(vaccination_rate, \"2022-06-30\"))\n\n\n\n\nObservations from the plotted map\nEach map has its own relative vaccination rate: the ranges gradually grow larger over time with the greater number of people getting vaccinated. By comparing the increasing rates over the months, there are a number of observations we can make\nIn the early stages (July 2021 ~ October 2021), there is a visible darkly-coloured cluster around the north of Jakarta. In the section below, we learned that this is the KAMAL MUARA and HALIM PERDANA KUSUMAH sub-district with the highest vaccination rate.\nBetween (November 2021 ~ February 2022, other sub districts have darken in colour and the HALIM PERDANA KUSUMAH still remains the sub-district with the highest vaccination rate.\nIn the later stages of vaccination from March 2022, based on the observation there were more sub-districts with lower vaccination rate (lighter colour). Especially the majority of sub-districts in the North and West seems to have a low vaccination rate in comparison to others. However, HALIM PERDANA KUSUMAH still remains the sub-district with the highest vaccination rate.\nChecking for sub-districts with highest vaccination rate according to month\n\nvaccination_rate$Sub_District[which.max(vaccination_rate$`2021-07-31`)]\n\n[1] \"KAMAL MUARA\"\n\nvaccination_rate$Sub_District[which.max(vaccination_rate$`2021-08-31`)]\n\n[1] \"KAMAL MUARA\"\n\nvaccination_rate$Sub_District[which.max(vaccination_rate$`2021-09-30`)]\n\n[1] \"HALIM PERDANA KUSUMAH\"\n\nvaccination_rate$Sub_District[which.max(vaccination_rate$`2021-10-31`)]\n\n[1] \"HALIM PERDANA KUSUMAH\"\n\nvaccination_rate$Sub_District[which.max(vaccination_rate$`2021-11-30`)]\n\n[1] \"HALIM PERDANA KUSUMAH\"\n\nvaccination_rate$Sub_District[which.max(vaccination_rate$`2021-12-31`)]\n\n[1] \"HALIM PERDANA KUSUMAH\"\n\nvaccination_rate$Sub_District[which.max(vaccination_rate$`2022-01-31`)]\n\n[1] \"HALIM PERDANA KUSUMAH\"\n\nvaccination_rate$Sub_District[which.max(vaccination_rate$`2022-02-27`)]\n\n[1] \"HALIM PERDANA KUSUMAH\"\n\nvaccination_rate$Sub_District[which.max(vaccination_rate$`2022-03-31`)]\n\n[1] \"HALIM PERDANA KUSUMAH\"\n\nvaccination_rate$Sub_District[which.max(vaccination_rate$`2022-04-30`)]\n\n[1] \"HALIM PERDANA KUSUMAH\"\n\nvaccination_rate$Sub_District[which.max(vaccination_rate$`2022-05-31`)]\n\n[1] \"HALIM PERDANA KUSUMAH\"\n\nvaccination_rate$Sub_District[which.max(vaccination_rate$`2022-06-30`)]\n\n[1] \"HALIM PERDANA KUSUMAH\""
  },
  {
    "objectID": "Take_home_ex/Take_home_ex2.html#calculation-of-local-gi-of-monthly-vaccination-rate",
    "href": "Take_home_ex/Take_home_ex2.html#calculation-of-local-gi-of-monthly-vaccination-rate",
    "title": "Take Home Exercise 2",
    "section": "6.1 Calculation of Local GI* of monthly vaccination rate",
    "text": "6.1 Calculation of Local GI* of monthly vaccination rate\n\n# Make new vaccination attribute table with Date, Sub_District, Target, Not_Yet_Vaccinated\nvaccination_table <- combined_jakarta %>% select(10, 8, 11, 12) %>% st_drop_geometry()\n\n# Adding a new field for Vaccination_Rate\nvaccination_table$Vaccination_Rate <- ((vaccination_table$Target - vaccination_table$Not_Yet_Vaccinated) / vaccination_table$Target) *100\n\n# Vaccination attribute table with just Date, Sub_District, Vaccination_Rate\nvaccination_table <- tibble(vaccination_table %>% select(1,2,5))"
  },
  {
    "objectID": "Take_home_ex/Take_home_ex2.html#create-time-series-cube",
    "href": "Take_home_ex/Take_home_ex2.html#create-time-series-cube",
    "title": "Take Home Exercise 2",
    "section": "6.2 Create Time Series Cube",
    "text": "6.2 Create Time Series Cube\n\nvaccination_rate_st <- spacetime(vaccination_table, bd_jakarta,\n                          .loc_col = \"Sub_District\",\n                          .time_col = \"Date\")\n\nVerify if vaccination_rate_st is indeed a space-time cube by using the is_spacetime_cube() of sfdep package.\n\nis_spacetime_cube(vaccination_rate_st)\n\n[1] TRUE"
  },
  {
    "objectID": "Take_home_ex/Take_home_ex2.html#deriving-spatial-weights",
    "href": "Take_home_ex/Take_home_ex2.html#deriving-spatial-weights",
    "title": "Take Home Exercise 2",
    "section": "6.3 Deriving Spatial Weights",
    "text": "6.3 Deriving Spatial Weights\nCalculation of local Gi* weights will be done. However, before that we need derive the spatial weights. The below code chunk is used to identify neighbors and derive an inverse distance weights.\n\nvaccination_rate_nb <- vaccination_rate_st %>%\n  activate(\"geometry\") %>%\n  mutate(nb = include_self(st_contiguity(geometry)),\n         wt = st_inverse_distance(nb, geometry,\n                                  scale=1,\n                                  alpha=1),\n         .before=1) %>%\n  set_nbs(\"nb\") %>%\n  set_wts(\"wt\")\n\nNote that\n\nactivate() is used to activate the geometry context\nmutate() is used to create two new columns nb and wt.\nThen we will activate the data context again and copy over the nb and wt columns to each time-slice using set_nbs() and set_wts()\n\nrow order is very important so do not rearrange the observations after using set_nbs() or set_wts().\n\n\nThe dataset provided has neighbours and weights for each time slicing\n\nhead(vaccination_rate_nb)\n\n# A tibble: 6 x 5\n  Date       Sub_District  Vaccination_Rate nb        wt       \n  <date>     <chr>                    <dbl> <list>    <list>   \n1 2021-07-31 KEAGUNGAN                 53.3 <int [6]> <dbl [6]>\n2 2021-07-31 GLODOK                    61.6 <int [7]> <dbl [7]>\n3 2021-07-31 HARAPAN MULIA             49.7 <int [6]> <dbl [6]>\n4 2021-07-31 CEMPAKA BARU              46.7 <int [7]> <dbl [7]>\n5 2021-07-31 PASAR BARU                59.3 <int [9]> <dbl [9]>\n6 2021-07-31 KARANG ANYAR              52.2 <int [7]> <dbl [7]>\n\n\nset.seed() will be use before performing simulation to ensure that the computation is reproducible. When a random number generator is used, the results can be different each time the code is run, which makes it difficult to reproduce results. By setting the seed to a specific value (e.g., set.seed(1234)), the same random numbers will be generated each time the code is run, making the results reproducible and consistent.\n\nset.seed(1234)"
  },
  {
    "objectID": "Take_home_ex/Take_home_ex2.html#calculation-of-gi-value",
    "href": "Take_home_ex/Take_home_ex2.html#calculation-of-gi-value",
    "title": "Take Home Exercise 2",
    "section": "6.4 Calculation of GI* value",
    "text": "6.4 Calculation of GI* value\nThe calculation of the Gi* value for each sub-district where we group by date\n\ngi_values <- vaccination_rate_nb |>\n  group_by(Date) |>\n  mutate(gi_values = local_gstar_perm(\n    Vaccination_Rate, nb, wt, nsim=99)) |>\n      tidyr::unnest(gi_values)\n\ngi_values\n\n# A tibble: 3,132 x 13\n# Groups:   Date [12]\n   Date       Sub_District   Vacci~1 nb    wt    gi_star    e_gi  var_gi p_value\n   <date>     <chr>            <dbl> <lis> <lis>   <dbl>   <dbl>   <dbl>   <dbl>\n 1 2021-07-31 KEAGUNGAN         53.3 <int> <dbl>   2.54  0.00387 1.24e-8   2.87 \n 2 2021-07-31 GLODOK            61.6 <int> <dbl>   3.71  0.00403 1.19e-8   2.65 \n 3 2021-07-31 HARAPAN MULIA     49.7 <int> <dbl>   0.280 0.00380 1.66e-8   0.511\n 4 2021-07-31 CEMPAKA BARU      46.7 <int> <dbl>  -0.961 0.00376 1.29e-8  -0.465\n 5 2021-07-31 PASAR BARU        59.3 <int> <dbl>   2.70  0.00395 1.15e-8   1.79 \n 6 2021-07-31 KARANG ANYAR      52.2 <int> <dbl>   1.82  0.00385 1.45e-8   1.86 \n 7 2021-07-31 MANGGA DUA SE~    51.6 <int> <dbl>   1.66  0.00385 1.49e-8   1.43 \n 8 2021-07-31 PETOJO UTARA      47.2 <int> <dbl>  -0.236 0.00375 1.16e-8   0.487\n 9 2021-07-31 SENEN             54.4 <int> <dbl>   1.37  0.00386 1.04e-8   1.18 \n10 2021-07-31 BUNGUR            52.8 <int> <dbl>   0.922 0.00387 1.04e-8   0.703\n# ... with 3,122 more rows, 4 more variables: p_sim <dbl>, p_folded_sim <dbl>,\n#   skewness <dbl>, kurtosis <dbl>, and abbreviated variable name\n#   1: Vaccination_Rate"
  },
  {
    "objectID": "Take_home_ex/Take_home_ex2.html#visualise-the-monthly-values-of-gi",
    "href": "Take_home_ex/Take_home_ex2.html#visualise-the-monthly-values-of-gi",
    "title": "Take Home Exercise 2",
    "section": "6.5 Visualise the monthly values of GI*",
    "text": "6.5 Visualise the monthly values of GI*\nTo be able to visualise the Gi* values of the monthly vaccination rate, we need to join it with combined_jakarta, to be able to plot the Gi* values on the map. As the gi_values do not have any coordinates\n\njakarta_gi_values <- combined_jakarta %>%\n  left_join(gi_values)\n\njakarta_gi_values\n\nSimple feature collection with 3132 features and 23 fields\nGeometry type: MULTIPOLYGON\nDimension:     XY\nBounding box:  xmin: -3644275 ymin: 663887.8 xmax: -3606237 ymax: 701380.1\nProjected CRS: DGN95 / Indonesia TM-3 zone 54.1\nFirst 10 features:\n   Object_ID Village_Code   Village   Code    Province          City   District\n1      25477   3173031006 KEAGUNGAN 317303 DKI JAKARTA JAKARTA BARAT TAMAN SARI\n2      25477   3173031006 KEAGUNGAN 317303 DKI JAKARTA JAKARTA BARAT TAMAN SARI\n3      25477   3173031006 KEAGUNGAN 317303 DKI JAKARTA JAKARTA BARAT TAMAN SARI\n4      25477   3173031006 KEAGUNGAN 317303 DKI JAKARTA JAKARTA BARAT TAMAN SARI\n5      25477   3173031006 KEAGUNGAN 317303 DKI JAKARTA JAKARTA BARAT TAMAN SARI\n6      25477   3173031006 KEAGUNGAN 317303 DKI JAKARTA JAKARTA BARAT TAMAN SARI\n7      25477   3173031006 KEAGUNGAN 317303 DKI JAKARTA JAKARTA BARAT TAMAN SARI\n8      25477   3173031006 KEAGUNGAN 317303 DKI JAKARTA JAKARTA BARAT TAMAN SARI\n9      25477   3173031006 KEAGUNGAN 317303 DKI JAKARTA JAKARTA BARAT TAMAN SARI\n10     25477   3173031006 KEAGUNGAN 317303 DKI JAKARTA JAKARTA BARAT TAMAN SARI\n   Sub_District Total_Population       Date Target Not_Yet_Vaccinated\n1     KEAGUNGAN            21609 2022-02-27  17387               2755\n2     KEAGUNGAN            21609 2022-04-30  17387               2593\n3     KEAGUNGAN            21609 2022-06-30  17387               2553\n4     KEAGUNGAN            21609 2021-11-30  17387               3099\n5     KEAGUNGAN            21609 2021-09-30  17387               4203\n6     KEAGUNGAN            21609 2021-08-31  17387               6054\n7     KEAGUNGAN            21609 2021-12-31  17387               2924\n8     KEAGUNGAN            21609 2022-01-31  17387               2783\n9     KEAGUNGAN            21609 2021-07-31  17387               8126\n10    KEAGUNGAN            21609 2022-03-31  17387               2675\n   Vaccination_Rate                      nb\n1          84.15483 1, 2, 39, 152, 158, 166\n2          85.08656 1, 2, 39, 152, 158, 166\n3          85.31662 1, 2, 39, 152, 158, 166\n4          82.17634 1, 2, 39, 152, 158, 166\n5          75.82677 1, 2, 39, 152, 158, 166\n6          65.18088 1, 2, 39, 152, 158, 166\n7          83.18284 1, 2, 39, 152, 158, 166\n8          83.99379 1, 2, 39, 152, 158, 166\n9          53.26393 1, 2, 39, 152, 158, 166\n10         84.61494 1, 2, 39, 152, 158, 166\n                                                                             wt\n1  0.000000000, 0.001071983, 0.001039284, 0.001417870, 0.001110612, 0.001297268\n2  0.000000000, 0.001071983, 0.001039284, 0.001417870, 0.001110612, 0.001297268\n3  0.000000000, 0.001071983, 0.001039284, 0.001417870, 0.001110612, 0.001297268\n4  0.000000000, 0.001071983, 0.001039284, 0.001417870, 0.001110612, 0.001297268\n5  0.000000000, 0.001071983, 0.001039284, 0.001417870, 0.001110612, 0.001297268\n6  0.000000000, 0.001071983, 0.001039284, 0.001417870, 0.001110612, 0.001297268\n7  0.000000000, 0.001071983, 0.001039284, 0.001417870, 0.001110612, 0.001297268\n8  0.000000000, 0.001071983, 0.001039284, 0.001417870, 0.001110612, 0.001297268\n9  0.000000000, 0.001071983, 0.001039284, 0.001417870, 0.001110612, 0.001297268\n10 0.000000000, 0.001071983, 0.001039284, 0.001417870, 0.001110612, 0.001297268\n    gi_star        e_gi       var_gi  p_value       p_sim p_folded_sim skewness\n1  2.458185 0.003844465 4.719854e-10 2.698959 0.006955666         0.02     0.01\n2  2.617693 0.003844331 5.194092e-10 2.637680 0.008347537         0.04     0.02\n3  2.647271 0.003847590 5.436358e-10 2.412370 0.015849175         0.02     0.01\n4  2.667891 0.003849749 1.087432e-09 2.413818 0.015786353         0.02     0.01\n5  2.381935 0.003842751 1.634566e-09 2.495261 0.012586449         0.02     0.01\n6  2.920481 0.003872762 3.342052e-09 2.532910 0.011311991         0.04     0.02\n7  2.594238 0.003844674 6.984154e-10 2.665185 0.007694606         0.02     0.01\n8  2.525165 0.003848330 6.200440e-10 2.332546 0.019671973         0.04     0.02\n9  2.535091 0.003866740 1.239652e-08 2.874848 0.004042217         0.04     0.02\n10 2.546472 0.003845743 5.032007e-10 2.577723 0.009945355         0.02     0.01\n      kurtosis                       geometry\n1   0.01180204 MULTIPOLYGON (((-3626874 69...\n2   0.05312438 MULTIPOLYGON (((-3626874 69...\n3   0.20010357 MULTIPOLYGON (((-3626874 69...\n4  -0.97424506 MULTIPOLYGON (((-3626874 69...\n5  -0.15423806 MULTIPOLYGON (((-3626874 69...\n6   0.01839206 MULTIPOLYGON (((-3626874 69...\n7  -0.34317063 MULTIPOLYGON (((-3626874 69...\n8  -0.13637911 MULTIPOLYGON (((-3626874 69...\n9   0.17937665 MULTIPOLYGON (((-3626874 69...\n10  0.01941702 MULTIPOLYGON (((-3626874 69...\n\n\nWe will proceed with visualizing the first month (July 2021). We will be plotting both the Gi* value and the p-value of Gi* for the Vaccination Rates.\nAs per take home exercise 2 requirement, we will only be plotting the significant p-value < 0.05\n\ngi_value_plot <- function(date, title) {\n  gi_star_map = tm_shape(filter(jakarta_gi_values, Date == date)) +\n    tm_fill(\"gi_star\") +\n    tm_borders(alpha=0.5) +\n    tm_view(set.zoom.limits = c(6,8)) +\n    tm_layout(main.title = paste(\"Gi* values for vaccination rates in\", title), main.title.size=0.8)\n\n  p_value_map = tm_shape(filter(jakarta_gi_values, Date == date)) +\n    tm_fill(\"p_value\",\n          breaks = c(0, 0.001, 0.01, 0.05, 1),\n              labels = c(\"0.001\", \"0.01\", \"0.05\", \"Not sig\")) +\n    tm_borders(alpha=0.5) + \n    tm_layout(main.title = paste(\"p-values of Gi* for vaccination rates in\", title), main.title.size=0.8)\n\n  tmap_arrange(gi_star_map, p_value_map)\n}\n\nPlotting Gi* for all 12 months\n\ntmap_mode(\"plot\")\ngi_value_plot(\"2021-07-31\", \"July 2021\")\n\n\n\n\n\ntmap_mode(\"plot\")\ngi_value_plot(\"2021-08-31\", \"August 2021\")\n\n\n\n\n\ntmap_mode(\"plot\")\ngi_value_plot(\"2021-09-30\", \"September 2021\")\n\n\n\n\n\ntmap_mode(\"plot\")\ngi_value_plot(\"2021-10-31\", \"October 2021\")\n\n\n\n\n\ntmap_mode(\"plot\")\ngi_value_plot(\"2021-11-30\", \"November 2021\")\n\n\n\n\n\ntmap_mode(\"plot\")\ngi_value_plot(\"2021-12-31\", \"December 2021\")\n\n\n\n\n\ntmap_mode(\"plot\")\ngi_value_plot(\"2022-01-31\", \"January 2022\")\n\n\n\n\n\ntmap_mode(\"plot\")\ngi_value_plot(\"2022-02-27\", \"February 2022\")\n\n\n\n\n\ntmap_mode(\"plot\")\ngi_value_plot(\"2022-03-31\", \"March 2022\")\n\n\n\n\n\ntmap_mode(\"plot\")\ngi_value_plot(\"2022-04-30\", \"April 2022\")\n\n\n\n\n\ntmap_mode(\"plot\")\ngi_value_plot(\"2022-05-31\", \"May 2022\")\n\n\n\n\n\ntmap_mode(\"plot\")\ngi_value_plot(\"2022-06-30\", \"June 2022\")\n\n\n\n\nStatistical conclusion\nThe p-value represents the probability of observing a clustering. A significant p-value < 0.05 suggests that an observed pattern is unlikely to have occurred by chance and may indicate the presence of a spatial process. When Gi* value > 0, it indicates sub-districts with a higher vaccination rate than average. We can view the number of sub districts with p-value < 0.05 with the code below\n\nno_of_subdistricts_freq = filter(jakarta_gi_values, p_sim < 0.05)\nas.data.frame(table(no_of_subdistricts_freq$Sub_District))\n\n                     Var1 Freq\n1                   ANCOL    2\n2            BALE KAMBANG    3\n3             BALI MESTER    7\n4                    BARU    8\n5              BATU AMPAR   12\n6         BENDUNGAN HILIR   10\n7             BIDARA CINA    4\n8     CEMPAKA PUTIH BARAT    1\n9                CIGANJUR    9\n10              CIJANTUNG    9\n11                 CIKINI    7\n12                 CIKOKO    2\n13         CILANDAK TIMUR    3\n14              CILILITAN    3\n15              CILINCING    9\n16               CIPAYUNG    1\n17                CIPEDAK    9\n18 CIPINANG BESAR SELATAN    2\n19   CIPINANG BESAR UTARA    5\n20      CIPINANG CEMPEDAK   12\n21         CIPINANG MUARA    1\n22                  GALUR    1\n23                 GAMBIR    4\n24                 GEDONG    6\n25                 GELORA    7\n26                 GLODOK    4\n27             GONDANGDIA    5\n28         GROGOL SELATAN    1\n29           GROGOL UTARA    1\n30  GUNUNG SAHARI SELATAN    2\n31              JAGAKARSA    9\n32        JATINEGARA KAUM    1\n33               KALISARI    7\n34                  KAMAL    1\n35            KAMAL MUARA    3\n36           KAMPUNG BALI   10\n37           KAMPUNG RAWA    1\n38         KAMPUNG TENGAH    4\n39                  KAPUK    2\n40          KARET TENGSIN    8\n41                KARTINI    1\n42              KEAGUNGAN   12\n43              KEBAGUSAN    7\n44           KEBON KACANG   12\n45           KEBON MELATI   11\n46            KEBON SIRIH    8\n47             KELAPA DUA    4\n48       KELAPA DUA WETAN    3\n49    KELAPA GADING TIMUR    1\n50                KLENDER    2\n51            KRAMAT JATI    3\n52            KRAMAT PELA    1\n53                  LAGOA    6\n54          LENTENG AGUNG   10\n55            MALAKA SARI    2\n56           MANGGA BESAR   12\n57      MANGGARAI SELATAN    4\n58                 MAPHAR    3\n59                MELAWAI    2\n60                MENTENG    9\n61          MENTENG DALAM    1\n62                 MUNJUL    9\n63              PEJAGALAN    1\n64          PEJATEN TIMUR    1\n65                PEKAYON    3\n66                PEKOJAN    1\n67            PENJARINGAN    2\n68             PETAMBURAN    9\n69              PETOGOGAN    2\n70           PINANG RANTI   11\n71          PISANGAN BARU    1\n72         PISANGAN TIMUR    2\n73           PONDOK BAMBU    2\n74          PONDOK KELAPA    1\n75            PONDOK LABU    4\n76         PONDOK RANGGON    1\n77            PULO GADUNG   12\n78       RAWA BADAK UTARA    4\n79             RAWA BUNGA    5\n80                ROROTAN    1\n81           SEMPER BARAT   10\n82           SEMPER TIMUR    9\n83                SENAYAN    2\n84                   SETU    1\n85                  SLIPI    1\n86        SRENGSENG SAWAH    9\n87       SUKABUMI SELATAN    3\n88               SUKAPURA    1\n89                TAMBORA    1\n90           TANAH SEREAL    1\n91           TANAH TINGGI    1\n92                 TANGKI    3\n93            TEBET BARAT    9\n94            TEBET TIMUR    3\n95             TEGAL ALUR    1\n96             TUGU UTARA    2\n97                ULUJAMI    1\n\n\nFrom the table above, there are 128 sub-districts who have a significant vaccination rate p-value < 0.05 at least once during the period of 12 months. Those sub-districts that have double digits frequency have a significant p value throughout the entire 12 months."
  },
  {
    "objectID": "Take_home_ex/Take_home_ex2.html#mann-kendall-test",
    "href": "Take_home_ex/Take_home_ex2.html#mann-kendall-test",
    "title": "Take Home Exercise 2",
    "section": "7.1 Mann-Kendall Test",
    "text": "7.1 Mann-Kendall Test\n\n7.1.1 HALIM PERDANA KUSUMAH\n\nhalim <- gi_values |>\n  ungroup() |>\n  filter(Sub_District == \"HALIM PERDANA KUSUMAH\") |>\n  select(Sub_District, Date, gi_star)\n\nPlotting the result by using ggplotly\n\np <- ggplot(data = halim, \n       aes(x = Date, \n           y = gi_star)) +\n  geom_line() +\n  theme_light()\n\nggplotly(p)\n\n\n\n\n\n\nhalim %>%\n  summarise(mk = list(\n    unclass(\n      Kendall::MannKendall(gi_star)))) %>% \n  tidyr::unnest_wider(mk)\n\n# A tibble: 1 x 5\n    tau        sl     S     D  varS\n  <dbl>     <dbl> <dbl> <dbl> <dbl>\n1 0.909 0.0000521    60  66.0  213.\n\n\nThe p-value is 0.086 which is > 0.05 hence p-value is not significant. Therefore, this is an upward but insignificant trend.\n\n\n7.1.2 TUGU UTARA\n\ntugu <- gi_values |>\n  ungroup() |>\n  filter(Sub_District == \"TUGU UTARA\") |>\n  select(Sub_District, Date, gi_star)\n\nPlotting the result by using ggplotly\n\np <- ggplot(data = tugu, \n       aes(x = Date, \n           y = gi_star)) +\n  geom_line() +\n  theme_light()\n\nggplotly(p)\n\n\n\n\n\n\ntugu %>%\n  summarise(mk = list(\n    unclass(\n      Kendall::MannKendall(gi_star)))) %>% \n  tidyr::unnest_wider(mk)\n\n# A tibble: 1 x 5\n    tau    sl     S     D  varS\n  <dbl> <dbl> <dbl> <dbl> <dbl>\n1 0.364 0.115    24  66.0  213.\n\n\nThe p-value is 0.149 which is > 0.05 hence p-value is not significant. Initially, the line chart is a downtrend but from January 2022 onward it begins its upward trend but it is still insignificant.\n\n\n7.1.3 ULUJAMI\n\nulujami <- gi_values |>\n  ungroup() |>\n  filter(Sub_District == \"ULUJAMI\") |>\n  select(Sub_District, Date, gi_star)\n\nPlotting the result by using ggplotly\n\np <- ggplot(data = ulujami, \n       aes(x = Date, \n           y = gi_star)) +\n  geom_line() +\n  theme_light()\n\nggplotly(p)\n\n\n\n\n\n\nulujami %>%\n  summarise(mk = list(\n    unclass(\n      Kendall::MannKendall(gi_star)))) %>% \n  tidyr::unnest_wider(mk)\n\n# A tibble: 1 x 5\n    tau       sl     S     D  varS\n  <dbl>    <dbl> <dbl> <dbl> <dbl>\n1 0.848 0.000162    56  66.0  213.\n\n\nThe p-value is 0.003 which is < 0.05 hence p-value is significant. Therefore, this is an upward but significant trend."
  },
  {
    "objectID": "Take_home_ex/Take_home_ex2.html#ehsa-map-of-the-gi-value",
    "href": "Take_home_ex/Take_home_ex2.html#ehsa-map-of-the-gi-value",
    "title": "Take Home Exercise 2",
    "section": "7.2 EHSA map of the Gi* value",
    "text": "7.2 EHSA map of the Gi* value\nFor us to find the significant hot and cold spots, there is a need to conduct the Mann Kendall test on all the subdistricts out there. Therefore, the group_by() function will be used for all subdistricts.\n\nehsa <- gi_values %>%\n  group_by(Sub_District) %>%\n  summarise(mk = list(\n    unclass(\n      Kendall::MannKendall(gi_star)))) %>%\n  tidyr::unnest_wider(mk)\n\nShow significant top 10 emerging hot/cold spots area\n\nemerging <- ehsa %>% \n  arrange(sl, abs(tau)) %>% \n  slice(1:10)\nemerging\n\n# A tibble: 10 x 6\n   Sub_District         tau         sl     S     D  varS\n   <chr>              <dbl>      <dbl> <dbl> <dbl> <dbl>\n 1 KEMBANGAN UTARA   -1.00  0.00000830   -66  66.0  213.\n 2 PASAR BARU        -1.00  0.00000830   -66  66.0  213.\n 3 CIPINANG MUARA     1.00  0.00000834    66  66.0  213.\n 4 KAMPUNG MELAYU     1.00  0.00000834    66  66.0  213.\n 5 MANGGARAI          1.00  0.00000834    66  66.0  213.\n 6 PISANGAN BARU      1.00  0.00000834    66  66.0  213.\n 7 UTAN KAYU SELATAN  1.00  0.00000834    66  66.0  213.\n 8 GAMBIR            -0.970 0.0000156    -64  66.0  213.\n 9 KAMAL             -0.970 0.0000156    -64  66.0  213.\n10 KAMAL MUARA       -0.970 0.0000156    -64  66.0  213.\n\n\nemerging_hotspot_analysis() of sfdep package will be used to perform EHSA analysis. It takes a spacetime object x (i.e. vaccination_rate_st), and the quoted name of the variable of interest (i.e. Vaccinaton Rate) for .var argument. The k argument is used to specify the number of time lags which is set to 1 by default.\n\nehsa <- emerging_hotspot_analysis(\n  x = vaccination_rate_st,\n  .var = \"Vaccination_Rate\",\n  k = 1,\n  nsim = 99\n)\n\nVisualisation of distribution\n\nggplot(data = ehsa,\n       aes(x=classification, fill=classification)) + \n  geom_bar()\n\n\n\n\nThe barchart above shows that sporadic hot spots class has the highest numbers.\nLeft join of combine jakarta and ehsa together\n\njakarta_ehsa <- bd_jakarta %>%\n  left_join(ehsa, by = c(\"Sub_District\" = \"location\"))\n\nVisualisation of classification using tmap\n\n# We use the filter to filter out values with p-value < 0.05\njakarta_ehsa_sig <- jakarta_ehsa  %>%\n  filter(p_value < 0.05)\ntmap_mode(\"plot\")\ntm_shape(jakarta_ehsa) +\n  tm_polygons() +\n  tm_borders(alpha = 0.5) +\ntm_shape(jakarta_ehsa_sig) +\n  tm_fill(\"classification\") + \n  tm_borders(alpha = 0.4)\n\n\n\n\nThe maps shows spordiac coldspot is spread evenly out in Jakarta. For oscillating hotspot it is lesser than spordiac coldspot. Oscilating coldspot can be found to be more around the border and in the central of Jakarta. There is also a large number of oscilating hotspot spread out evenly around Jakarta. From the map, there is no obvious pattern and lastly, sub districts shaded in grey are of p value > 0.05 which represents that the sub districts are insignificant.\nEnd of take home exercise 2\n*Note to Professor*\nSome of the code chunks above are referenced from\n\nMegan’s IS415 Journey https://is415-msty.netlify.app/posts/2021-09-10-take-home-exercise-1/\nProfessor In Class Exercises 7 https://is415-gaa-tskam.netlify.app/in-class_ex/in-class_ex07/in-class_ex07-glsa and https://is415-gaa-tskam.netlify.app/in-class_ex/in-class_ex07/in-class_ex07_ehsa"
  }
]