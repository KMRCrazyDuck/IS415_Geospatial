---
title: "Take Home Exercise 3: Predicting HDB Public Housing Resale Pricies using Geographically Weighted Methods"
author: "Kwek Ming Rong"
date: "09 March 2023"
date-modified: "20 March 2023"
format: html
execute:
  eval: true
  echo: true
  warning: false
editor: visual
---

![](housing_image.jpg)

Source: https://singaplex.com/magazine/singles-hdb

# Background Information

Housing is an essential component of household wealth worldwide. Be it for a couple or individual, buying a house has always been a dream or goal to stay for most people. The price of housing is affected by many factors. Some of them are global in nature such as the general economy of a country or inflation rate. Others can be more specific to the properties themselves. These factors can be further divided to structural and locational factors. Structural factors are variables related to the property themselves such as the size, fitting, and tenure of the property. Locational factors are variables related to the neighbourhood of the properties such as proximity to childcare centre, public transport service and shopping centre.

Conventional, housing resale prices predictive models were built by using [**Ordinary Least Square (OLS)**](https://en.wikipedia.org/wiki/Ordinary_least_squares) method. However, this method failed to take into consideration that spatial autocorrelation and spatial heterogeneity exist in geographic data sets such as housing transactions. With the existence of spatial autocorrelation, the OLS estimation of predictive housing resale pricing models could lead to biased, inconsistent, or inefficient results (Anselin 1998). In view of this limitation, **Geographical Weighted Models** were introduced for calibrating predictive model for housing resale prices.

# The Objective

In this take-home exercise, I am tasked to predict HDB resale prices at the sub-market level (i.e. HDB 3-room, HDB 4-room and HDB 5-room) for the month of January and February 2023 in Singapore. The predictive models must be built by using by using conventional OLS method and GWR methods. I am also required to compare the performance of the conventional OLS method versus the geographical weighted methods.

# 1. Datasets

-   **Aspatial dataset**:

    -   HDB Resale data: a list of HDB resale transacted prices in Singapore from Jan 2017 onwards. It is in csv format which can be downloaded from Data.gov.sg.

-   **Geospatial dataset**:

    -   *MP14_SUBZONE_WEB_PL*: a polygon feature data providing information of URA 2014 Master Plan Planning Subzone boundary data. It is in ESRI shapefile format. This data set was also downloaded from Data.gov.sg

-   **Location factors with regards to the geographic coordinates**:

    -   Downloaded from **Data.gov.sg**.

        -   **Eldercare** data is a list of eldercare in Singapore. It is in shapefile format.

        -   **Hawker Centre** data is a list of hawker centres in Singapore. It is in geojson format.

        -   **Parks** data is a list of parks in Singapore. It is in geojson format.

        -   **Supermarket** data is a list of supermarkets in Singapore. It is in geojson format.

        -   **CHAS clinics** data is a list of CHAS clinics in Singapore. It is in geojson format.

        -   **Childcare service** data is a list of childcare services in Singapore. It is in geojson format.

        -   **Kindergartens** data is a list of kindergartens in Singapore. It is in geojson format.

    -   Downloaded from **Datamall.lta.gov.sg**.

        -   **MRT** data is a list of MRT/LRT stations in Singapore with the station names and codes. It is in shapefile format.\

        -   **Bus stops** data is a list of bus stops in Singapore. It is in shapefile format.

-   **Location factors without geographic coordinates**:

    -   Downloaded from **Data.gov.sg**.

        -   **Primary school** data is extracted from the list on General information of schools from data.gov portal. It is in csv format.

    -   Retrieved/Scraped from **other sources**

        -   **CBD** coordinates obtained from Google.

        -   **Shopping malls** data is a list of Shopping malls in Singapore obtained from [Wikipedia](https://en.wikipedia.org/wiki/List_of_shopping_malls_in_Singapore).

        -   **Good primary schools** is a list of primary schools that are ordered in ranking in terms of popularity and this can be found at [Local Salary Forum](https://www.salary.sg/2021/best-primary-schools-2021-by-popularity).

# 2. Loading the R packages

The following code chunk will perform the following task. A list called packages will be created and will consists of all the R packages required to accomplish this exercise. There will be a check to see if R packages on package have been installed in R and if not, they will be installed. After which when all the R packages have been installed, the packages will then be loaded

```{r}
packages <- c('sf', 'tidyverse', 'tmap', 'httr', 'jsonlite', 'rvest', 
              'sp', 'ggpubr', 'corrplot', 'broom',  'olsrr', 'spdep', 
              'GWmodel', 'devtools', 'rgeos', 'lwgeom', 'maptools', 'rsample', 'Metrics', 'SpatialML')

for(p in packages){
  if(!require(p, character.only = T)){
    install.packages(p, repos = "http://cran.us.r-project.org")
  }
  library(p, character.only = T)
}
```

```{r}
devtools::install_github("gadenbuie/xaringanExtra")
library(xaringanExtra)
```

More on the packages used:

-   sf: used for importing, managing, and processing geospatial data

-   tidyverse: used for importing, wrangling and visualising data. It consists of a family of R packages, such as:

    -   **readr** for importing csv data,

    -   **readxl** for importing Excel worksheet,

    -   **tidyr** for manipulating data,

    -   **dplyr** for transforming data, and

    -   **ggplot2** for visualising data

-   **tmap:** provides functions for plotting cartographic quality *static* inpoint patterns maps or *interactive* maps by using leaflet API.

-   **httr:** Useful tools for working with HTTP organised by HTTP verbs (GET(), POST(), etc). Configuration functions make it easy to control additional request components (authenticate(), add_headers() and so on).

    -   In this analysis, it will be used to send GET requests to OneMapAPI SG to retrieve the coordinates of addresses.

-   **jsonlite:** A simple and robust JSON parser and generator for R. It offers simple, flexible tools for working with JSON in R, and is particularly powerful for building pipelines and interacting with a web API.

-   **rvest:** A new package that makes it easy to scrape (or harvest) data from html web pages, inspired by libraries like beautiful soup.

    -   In this analysis, it will be used to scrape data for **shopping malls** and **good primary schools**

-   **sp:** provides classes and methods for dealing with spatial data in R.

-   **ggpubr:** provides some easy-to-use functions for creating and customizing ggplot2 based publication ready plots

    -   In this analysis, it will be used to arrange multiple ggplots.

-   **corrplot:** For Multivariate data visualisation and analysis

-   **broom:** Takes the messy output of built-in functions in R, such as lm, nls, or t.test, and turns them into tidy tibble.

    -   In this analysis, functions like tidy and glance will be used to construct a tibble / summmary of the model which is easier to look at.

-   **oslrr:** Used to build OLD and performing diagnostic tests.

-   **spdep:** For spatial dependence statistics.

-   **GWmodel:** Calibrate geographical weighted family of modes.

-   **devtools:** used for installing any R packages which is not available in RCRAN. In this exercise, I will be installing using devtools to install the package xaringanExtra which is still under development stage.

-   **xaringanExtra:** is an enhancement of xaringan package. As it is still under development stage, we can still install the current version using install_github function of devtools. This package will be used to add Panelsets to contain both the r code chunk and results whereever applicable.

# 3. Importing of Aspatial Data & Wrangling

read_csv() function of readr package will be used to import resale-flat-prices into R as a tibble data frame called resale. glimpse() function of dplyr package is used to display the data structure

```{r}
resale <- read_csv("data/aspatial/resale-flat-prices.csv")
```

```{r}
glimpse(resale)
```

When we load in the dataset for the first time, we can see that:

The dataset contains 11 columns with 148,373 rows. The columns that are present in the data are: month, town, flat_type, block, street_name, storey_range, floor_area_sqm, flat_model, lease_commence_date, remaining_lease, resale_price.

For this take home exercise 3, we are allowed the option to choose to perform our analysis between either 3, 4 or 5 room flat transactions. Therefore, I will be selecting the 3 room flat transactions during the transaction period from 1st January 2021 to 31st December 2022. Test data should be included for January and February 2023 resale prices.

## 3.1 Filtering HDB Resale Data

filter() function of dplyr package will be used to select the desired flat_type and dates which will be stored in rs_subset.

```{r}
rs_subset <-  filter(resale,flat_type == "3 ROOM") %>% 
              filter(month >= "2021-01" & month <= "2023-02")
```

unique() function of R package will be used to check whether flat_type and month have been extracted successfully.

```{r}
unique(rs_subset$month)
```

```{r}
unique(rs_subset$flat_type)
```

glimpse() function will be used to view the overall resale transactions available for 3 room flat in Singapore.

```{r}
glimpse(rs_subset)
```

From the glimpse() function result, we can see that from Jan 2021 to December 2022, there are 23,656 transactions for 3 room flat in Singapore.

## 3.2 Transforming HDB Resale Data Columns

Here, *mutate* function of dplyr package will be used to create columns such as:

-   **`address`**: concatenation of the **`block`** and **`street_name`** columns using *paste()* function of **base R** package. **`remaining_lease_yr`** & **`remaining_lease_mth`**: split the **year** and **months** part of the **`remaining_lease`** respectively using *str_sub()* function of **stringr** package then converting the character to integer using *as.integer()* function of **base R** package. After performing mutate function, we will store the new data in **`rs_transform`**

```{r}
rs_transform <- rs_subset %>%
  mutate(rs_subset, address = paste(block,street_name)) %>%
  mutate(rs_subset, remaining_lease_yr = as.integer(str_sub(remaining_lease, 0, 2)))%>%
  mutate(rs_subset, remaining_lease_mth = as.integer(str_sub(remaining_lease, 9, 11)))
```

```{r}
head(rs_transform)
```

### 3.2.1 Sum remaining lease in months

There are some NA values in remaining lease months with value of 0. We need to multiply the **remaining_lease_yr** by 12 to convert into months. The **remaining_lease_mths** column will be created using mutate function of dplyr package which contains the summation of the remaining_lease_yr and remaining_lease_mths using rowSums() of R package.

```{r}
rs_transform$remaining_lease_mth[is.na(rs_transform$remaining_lease_mth)] <- 0
rs_transform$remaining_lease_yr <- rs_transform$remaining_lease_yr * 12
rs_transform <- rs_transform %>% 
  mutate(rs_transform, remaining_lease_mths = rowSums(rs_transform[, c("remaining_lease_yr", "remaining_lease_mth")])) %>%
  select(month, town, address, block, street_name, flat_type, storey_range, floor_area_sqm, flat_model, 
         lease_commence_date, remaining_lease_mths, resale_price)
```

```{r}
head(rs_transform)
```

## 3.3 Retrieval of Addresses & its Coordinates

Data such as postal codes and coordinates of the addresses is required to get the proximity to various location factors later on.

### 3.3.1 Creaion of unique list of addresses

Unique addresses will be created and stored in **add_list**. unique() function of base R package will be used to extract the unique addresses and sort() function of R package to sort the unique vector.

```{r}
add_list <- sort(unique(rs_transform$address))
```

### 3.3.2 Creation of function to retrieve Coordinates from OneMapSG API

A dataframe **postal_coords** will be created to store all final retrieved coordinates. The GET() function of **httr** package will be used to perform a GET request to https://developers.onemap.sg/commonapi/search. There are a few search arguments variables and information we have to take note of

-   searchVal: Unique keywords that user will enter to filter results

-   returnGeom {Y/N}: Yes or No to check if user want to return the geometry

-   getAddrDetails {Y/N}: Yes or No to check if user want to return address details for a point

Return JSON response will contain many fields but we are only interested in postal code and coordinates like Longitude and Latitude. A new dataframe new_row will be created and is used to store each final set of coordinates retrieved. There is also the need to check the number of responses because some searched location have 0, some only have 1 result and others have many. Finally, the JSON result will be appended to the dataframe postal_coords using rbind() function of R.

```{r}
get_coords <- function(add_list){
  
  # Create a data frame to store all retrieved coordinates
  postal_coords <- data.frame()
    
  for (i in add_list){
    #print(i)

    r <- GET('https://developers.onemap.sg/commonapi/search?',
           query=list(searchVal=i,
                     returnGeom='Y',
                     getAddrDetails='Y'))
    data <- fromJSON(rawToChar(r$content))
    found <- data$found
    res <- data$results
    
    # Create a new data frame for each address
    new_row <- data.frame()
    
    # If single result, append 
    if (found == 1){
      postal <- res$POSTAL 
      lat <- res$LATITUDE
      lng <- res$LONGITUDE
      new_row <- data.frame(address= i, postal = postal, latitude = lat, longitude = lng)
    }
    
    # If multiple results, drop NIL and append top 1
    else if (found > 1){
      # Remove those with NIL as postal
      res_sub <- res[res$POSTAL != "NIL", ]
      
      # Set as NA first if no Postal
      if (nrow(res_sub) == 0) {
          new_row <- data.frame(address= i, postal = NA, latitude = NA, longitude = NA)
      }
      
      else{
        top1 <- head(res_sub, n = 1)
        postal <- top1$POSTAL 
        lat <- top1$LATITUDE
        lng <- top1$LONGITUDE
        new_row <- data.frame(address= i, postal = postal, latitude = lat, longitude = lng)
      }
    }

    else {
      new_row <- data.frame(address= i, postal = NA, latitude = NA, longitude = NA)
    }
    
    # Add the row
    postal_coords <- rbind(postal_coords, new_row)
  }
  return(postal_coords)
}
```

### 3.3.3 Retrieve resale coordinates using the get_coords function

```{r}
coords <- get_coords(add_list)
```

### 3.3.4 Check results

Check if columns contain any Nil or NA values with is.na() function of R

```{r}
coords[(is.na(coords$postal) | is.na(coords$latitude) | is.na(coords$longitude) | coords$postal=="NIL"), ]
```

From the result, it can be seen that 215 CHOA CHU KANG CTRL has nil postal code but there are coordinates available. When performing a search on gothere.sg, the postal code should be 680215.

### 3.3.5 Combination of HDB resale and coordinate data

After retrieving the coordinates, we need to combine with the HDB resale dataset using left_join() function of dplyr package. The data will be stored in rs_coords.

```{r}
rs_coords <- left_join(rs_transform, coords, by = c('address' = 'address'))
```

```{r}
head(rs_coords)
```

## 3.4 Writing file to rds

```{r}
rs_coords_rds <- write_rds(rs_coords, "Data/rds/rs_coords.rds")
```

## 3.5 Reading the rs_coords_rds file

```{r}
rs_coords <- read_rds("Data/rds/rs_coords.rds")
glimpse(rs_coords)
```

### 3.5.1 Transform and Assign CRS

The Latitude & Longitude are in decimal, therefore the projected CRS will be WGS84. We will need to assign the CRS of 4326 first before transforming it to 3414 which is the EPSG code for Singapore SVY21

```{r}
rs_coords_sf <- st_as_sf(rs_coords,
                    coords = c("longitude", 
                               "latitude"),
                    crs=4326) %>%
  st_transform(crs = 3414)
```

```{r}
st_crs(rs_coords_sf)
```

### 3.5.2 Plotting HDB resale points

```{r}
tmap_mode("view")
tm_shape(rs_coords_sf)+
  tm_dots(col="blue", size = 0.02)
tmap_mode("plot")
```

# 4. Importing Geospatial Locational Factors

## 4.1 Location factors with coordinates

### 4.1.1 Reading of location factors

```{r}
elder_sf <- st_read(dsn = "Data/Geospatial", layer="ELDERCARE")
mrtlrt_sf <- st_read(dsn = "Data/Geospatial", layer="MRTLRTStnPtt")
bus_sf <- st_read(dsn = "Data/Geospatial", layer="BusStop")
hawker_sf <- st_read("Data/Geospatial/hawker-centres-geojson.geojson") 
supermkt_sf <- st_read("Data/Geospatial/supermarkets-geojson.geojson") 
chas_sf <- st_read("Data/Geospatial/moh-chas-clinics.geojson")
childcare_sf <- st_read("Data/Geospatial/childcare.geojson") 
kind_sf <- st_read("Data/Geospatial/preschools-location.geojson") 
parks_sf <- st_read("Data/Geospatial/parks-geojson.geojson")
```

```{r}
st_crs(elder_sf)
st_crs(mrtlrt_sf)
st_crs(bus_sf)
st_crs(hawker_sf)
st_crs(supermkt_sf)
st_crs(chas_sf)
st_crs(childcare_sf)
st_crs(kind_sf)
st_crs(parks_sf)
```

### 4.1.2 Assign EPSG code to sf dataframes

```{r}
elder_sf <- st_set_crs(elder_sf, 3414)
mrtlrt_sf <- st_set_crs(mrtlrt_sf, 3414)
bus_sf <- st_set_crs(bus_sf, 3414)
hawker_sf <- hawker_sf %>%
  st_transform(crs = 3414)
supermkt_sf <- supermkt_sf %>%
  st_transform(crs = 3414)
chas_sf <- chas_sf %>%
  st_transform(crs = 3414)
childcare_sf <- childcare_sf %>%
  st_transform(crs = 3414)
kind_sf <- kind_sf %>%
  st_transform(crs = 3414)
parks_sf <- parks_sf %>%
  st_transform(crs = 3414)
```

```{r}
st_crs(elder_sf)
st_crs(mrtlrt_sf)
st_crs(bus_sf)
st_crs(hawker_sf)
st_crs(supermkt_sf)
st_crs(chas_sf)
st_crs(childcare_sf)
st_crs(kind_sf)
#st_crs(parks_sf)
```

```{r}
length(which(st_is_valid(elder_sf) == FALSE))
length(which(st_is_valid(mrtlrt_sf) == FALSE))
length(which(st_is_valid(hawker_sf) == FALSE))
length(which(st_is_valid(parks_sf) == FALSE))
length(which(st_is_valid(supermkt_sf) == FALSE))
length(which(st_is_valid(chas_sf) == FALSE))
length(which(st_is_valid(childcare_sf) == FALSE))
length(which(st_is_valid(kind_sf) == FALSE))
length(which(st_is_valid(bus_sf) == FALSE))
```

### 4.1.3 Proximity Calculation of location

get_prox function will be created to perform calculation of distances between the HDB resale and location factors

```{r}
get_prox <- function(origin_df, dest_df, col_name){
  
  # creates a matrix of distances
  dist_matrix <- st_distance(origin_df, dest_df)           
  
  # find the nearest location_factor and create new data frame
  near <- origin_df %>% 
    mutate(PROX = apply(dist_matrix, 1, function(x) min(x)) / 1000) 
  
  # rename column name according to input parameter
  names(near)[names(near) == 'PROX'] <- col_name

  return(near)
}
```

get_prox function will be called to get the proximity of the resale HDB and location factors such as:

-   Eldercare

-   MRT

-   Hawker

-   Parks

-   Supermarkets

-   CHAS clinics

```{r}
rs_coords_sf <- get_prox(rs_coords_sf, elder_sf, "PROX_ELDERLYCARE") 
rs_coords_sf <- get_prox(rs_coords_sf, mrtlrt_sf, "PROX_MRT") 
rs_coords_sf <- get_prox(rs_coords_sf, hawker_sf, "PROX_HAWKER") 
rs_coords_sf <- get_prox(rs_coords_sf, parks_sf, "PROX_PARK") 
rs_coords_sf <- get_prox(rs_coords_sf, supermkt_sf, "PROX_SUPERMARKET")
rs_coords_sf <- get_prox(rs_coords_sf, chas_sf, "PROX_CHAS")
```

### 4.1.4 Calculation of location factors within distance

get_within function will create a matrix of distances between the HDB and the location factor using st_distance of sf package. It will also get the sum of points of the location factor that are within the threshold distance using sum function of base R package then add it to HDB resale data under a new column using mutate() function of dpylr package. Lastly, it will rename the column name according to input given by user so that the columns have appropriate and distinct names that are different from one another.

```{r}
get_within <- function(origin_df, dest_df, threshold_dist, col_name){
  
  # creates a matrix of distances
  dist_matrix <- st_distance(origin_df, dest_df)   
  
  # count the number of location_factors within threshold_dist and create new data frame
  wdist <- origin_df %>% 
    mutate(WITHIN_DT = apply(dist_matrix, 1, function(x) sum(x <= threshold_dist)))
  
  # rename column name according to input parameter
  names(wdist)[names(wdist) == 'WITHIN_DT'] <- col_name

  # Return df
  return(wdist)
}
```

#### 4.1.4.1 Calling the function for all location factors

In this case, the threshold we set it to will be Within 350m for location factors such as, kindergartens, childcare and bus stops.

```{r}
rs_coords_sf <- get_within(rs_coords_sf, kind_sf, 350, "WITHIN_350M_KINDERGARTEN")
```

```{r}
rs_coords_sf <- get_within(rs_coords_sf, childcare_sf, 350, "WITHIN_350M_CHILDCARE")
```

```{r}
rs_coords_sf <- get_within(rs_coords_sf, bus_sf, 350, "WITHIN_350M_BUS")
```

## 4.2 Location factors without coordinates

### 4.2.1 CBD Town Area

Central Business District is an area where majority of the citizens travel to daily for work. Therefore it is an important measure. With a quick google search, the latitude and longitude of Downtown Core also known as CBD, are 1.287953 and 103.851784 respectively.

```{r}
# Store CBD Coordinates in Dataframe
name <- c('CBD Area')
latitude= c(1.287953)
longitude= c(103.851784)
cbd_coords <- data.frame(name, latitude, longitude)
```

Conversion of data frame into sf object and perform transformation of crs

```{r}
cbd_coords_sf <- st_as_sf(cbd_coords,
                    coords = c("longitude", 
                               "latitude"),
                    crs=4326) %>%
  st_transform(crs = 3414)

st_crs(cbd_coords_sf)
```

get_prox function will be called to get the proximity between the HDB resale flats and CBD area

```{r}
rs_coords_sf <- get_prox(rs_coords_sf, cbd_coords_sf, "PROX_CBD")
```

### 4.2.2 Shopping Malls Places

There are multiple lists of shopping malls classified by the regions in Singapore, we are able to check the XPaths of the various lists on the wikipedia page.

```{r}
url <- "https://en.wikipedia.org/wiki/List_of_shopping_malls_in_Singapore"
malls_list <- list()

for (i in 1:7){
  malls <- read_html(url) %>%
    html_nodes(xpath = paste('//*[@id="mw-content-text"]/div[1]/div[',as.character(i),']/ul/li',sep="") ) %>%
    html_text()
  malls_list <- append(malls_list, malls)
}

#malls_list
```

From the malls_list result, there are 164 shopping malls. As these malls does not have their coordinates, we need to use the get_coords function created previously to search the names of these shopping malls.

```{r}
malls_list_coords <- get_coords(malls_list) %>% 
  rename("mall_name" = "address")
```

```{r}
malls_list_coords <- subset(malls_list_coords, mall_name!= "Yew Tee Shopping Centre")
```

We need to correct the mall names that are no longer relevant and also malls with an index beside it due to Wikipedia page numbering as shown above output

```{r}
invalid_malls<- subset(malls_list_coords, is.na(malls_list_coords$postal))
invalid_malls_list <- unique(invalid_malls$mall_name)
corrected_malls <- c("Clarke Quay", "City Gate", "Raffles Holland V", "Knightsbridge", "Mustafa Centre", "GR.ID", "Shaw House",
                     "The Poiz Centre", "Velocity @ Novena Square", "Singapore Post Centre", "PLQ Mall", "KINEX", "The Grandstand")

for (i in 1:length(invalid_malls_list)) {
  malls_list_coords <- malls_list_coords %>% 
    mutate(mall_name = ifelse(as.character(mall_name) == invalid_malls_list[i], corrected_malls[i], as.character(mall_name)))
}
```

```{r}
malls_list <- sort(unique(malls_list_coords$mall_name))
```

```{r}
malls_coords <- get_coords(malls_list)
```

To check if the malls still have any NA values

```{r}
malls_coords[(is.na(malls_coords$postal) | is.na(malls_coords$latitude) | is.na(malls_coords$longitude)), ]
```

To convert the mall dataframe into SF object and perform transformation of CRS

```{r}
malls_sf <- st_as_sf(malls_coords,
                    coords = c("longitude", 
                               "latitude"),
                    crs=4326) %>%
  st_transform(crs = 3414)
```

get_prox function will be called to get the proximity between the HDB resale flats and Shopping Malls

```{r}
rs_coords_sf <- get_prox(rs_coords_sf, malls_sf, "PROX_MALL") 
```

### 4.2.3 Primary Schools

To extract and retrieve only primary schools data

```{r}
pri_sch <- read_csv("Data/Aspatial/general-information-of-schools.csv")

pri_sch <- pri_sch %>%
  filter(mainlevel_code == "PRIMARY") %>%
  select(school_name, address, postal_code, mainlevel_code)

glimpse(pri_sch)
```

There are 183 Primary Schools in Singapore.

To create a list to store postal code of the schools and retrieve the coordinates

```{r}
prisch_list <- sort(unique(pri_sch$postal_code))
prisch_coords <- get_coords(prisch_list)
```

To check if the primary schools still have any NA values

```{r}
prisch_coords[(is.na(prisch_coords$postal) | is.na(prisch_coords$latitude) | is.na(prisch_coords$longitude)), ]
```

To combine coordinates with the schools

```{r}
prisch_coords = prisch_coords[c("postal","latitude", "longitude")]
pri_sch <- left_join(pri_sch, prisch_coords, by = c('postal_code' = 'postal'))
```

To convert primary schools dataframe into SF object and perform transformation of CRS

```{r}
prisch_sf <- st_as_sf(pri_sch,
                    coords = c("longitude", 
                               "latitude"),
                    crs=4326) %>%
  st_transform(crs = 3414)
```

To call get_within function to obtain schools within a proximity of 1km

```{r}
rs_coords_sf <- get_within(rs_coords_sf, prisch_sf, 1000, "WITHIN_1KM_PRISCH")
```

### 4.2.4 Proximity to Good Primary Schools

To identify good primary schools in Singapore, one particular forum that we can use is www.salary.sg where it provides ranking of schools according to their popularity. Similarly, we can reuse the code that we extract the shopping malls in Singapore to extract the ranking of primary schools.

```{r}
url <- "https://www.salary.sg/2021/best-primary-schools-2021-by-popularity/"

good_pri <- data.frame()

schools <- read_html(url) %>%
  html_nodes(xpath = paste('//*[@id="post-3068"]/div[3]/div/div/ol/li') ) %>%
  html_text() 

for (i in (schools)){
  sch_name <- toupper(gsub(" – .*","",i))
  sch_name <- gsub("\\(PRIMARY SECTION)","",sch_name)
  sch_name <- trimws(sch_name)
  new_row <- data.frame(pri_sch_name=sch_name)
  # Add the row
  good_pri <- rbind(good_pri, new_row)
}

top_good_pri <- head(good_pri, 10)

head(top_good_pri)
```

To check if the extracted top primary schools' name matches the existing names in the primary school dataframe

```{r}
top_good_pri$pri_sch_name[!top_good_pri$pri_sch_name %in% prisch_sf$school_name]

#Unique list to store good school names
good_pri_list <- unique(top_good_pri$pri_sch_name)

```

To obtain the coordinates of good primary schools

```{r}
goodprisch_coords <- get_coords(good_pri_list)
```

To check for NA values

```{r}
goodprisch_coords[(is.na(goodprisch_coords$postal) | is.na(goodprisch_coords$latitude) | is.na(goodprisch_coords$longitude)), ]
```

Replacing the names to match the primary school dataframe names

```{r}
top_good_pri$pri_sch_name[top_good_pri$pri_sch_name == "CHIJ ST. NICHOLAS GIRLS’ SCHOOL"] <- "CHIJ SAINT NICHOLAS GIRLS' SCHOOL"
top_good_pri$pri_sch_name[top_good_pri$pri_sch_name == "ST. HILDA’S PRIMARY SCHOOL"] <- "SAINT HILDA'S PRIMARY SCHOOL"

#Unique list to store good school names
good_pri_list <- unique(top_good_pri$pri_sch_name)

#To obtain the coordinates of good primary schools
goodprisch_coords <- get_coords(good_pri_list)
```

Final inspection to check for NA values

```{r}
goodprisch_coords[(is.na(goodprisch_coords$postal) | is.na(goodprisch_coords$latitude) | is.na(goodprisch_coords$longitude)), ]
```

To convert the good primary schools dataframe into SF object and perform transformation of CRS

```{r}
goodpri_sf <- st_as_sf(goodprisch_coords,
                    coords = c("longitude", 
                               "latitude"),
                    crs=4326) %>%
  st_transform(crs = 3414)

st_crs(goodpri_sf)
```

get_prox function will be called to get the proximity between the HDB resale flats and good primary schools

```{r}
rs_coords_sf <- get_prox(rs_coords_sf, goodpri_sf, "PROX_GOOD_PRISCH")
```

## 4.3 Filtering dataset

```{r}
rs_coords_sf_training <-  filter(rs_coords_sf ,flat_type == "3 ROOM") %>% 
              filter(month >= "2021-01" & month <= "2023-02")
```

### 4.3.1 Writing to RDS file

```{r}
rs_factors_rds_training <- write_rds(rs_coords_sf_training, "Data/rds/rs_factors_training.rds")
```

# 5. Import Data for Analysis

## 5.1 Geospatial Data

Here, st_read of sf package to read simple features or layers from file.

```{r}
mpsz_sf <- st_read(dsn = "Data/Geospatial", layer="MPSZ-2019")
mpsz_sf <- st_transform(mpsz_sf, 3414)
```

R object used to contain the imported MPSZ-2019 shapefile is called mpsz_sf and it is a simple feature object. The correct EPSG code for SVY21 should be 3414 therefore a transformation is done above.

### 5.1.1 Check for invalid geometry and handle it

```{r}
length(which(st_is_valid(mpsz_sf) == FALSE))
```

Making the invalid geometry valid by using st_make_valid function

```{r}
mpsz_sf <- st_make_valid(mpsz_sf)
length(which(st_is_valid(mpsz_sf) == FALSE))
```

Now the geometry is valid.

## 5.2 Aspatial Data for HDB resale location factors

```{r}
rs_factors_training <- read_rds("Data/rds/rs_factors_training.rds")
```

## 5.3 Extract storey_order due to character type

```{r}
storeys <- sort(unique(rs_factors_training$storey_range))
# Create dataframe storey_range_order to store order of storey_range
storey_order <- 1:length(storeys)
storey_range_order <- data.frame(storeys, storey_order)

# Combine storey_order with resale dataframe for training and testing data
rs_factors_training <- left_join(rs_factors_training, storey_range_order, by= c("storey_range" = "storeys"))
```

## 5.4 Selecting required columns for analysis

```{r}
rs_factors_training <- rs_factors_training %>%
  select(resale_price, floor_area_sqm, storey_order, remaining_lease_mths,
         PROX_CBD, PROX_ELDERLYCARE, PROX_HAWKER, PROX_MRT, PROX_PARK, PROX_GOOD_PRISCH, PROX_MALL, PROX_CHAS,
         PROX_SUPERMARKET, WITHIN_350M_KINDERGARTEN, WITHIN_350M_CHILDCARE, WITHIN_350M_BUS, WITHIN_1KM_PRISCH)
```

# 6. Exploratory Data Analysis

## 6.1 HDB 3 room resale prices in Histogram

```{r}
ggplot(data=rs_factors_training, aes(x=`resale_price`)) +
  geom_histogram(bins=20, color="black", fill="light blue")
```

The results shown above reveals a right skewed distribution. This means that more resale HDB units were transacted at relative lower prices about \$500,000 majority. Statistically, the skewed distribution can be normalised by using log transformation which we will be doing in the next section.

## 6.2 Normalise using Log Transformation of HDB 3 room resale prices in Histogram

A new variable called LOG_RESALE_PRICE by using a log transformation on the variable resale_price

```{r}
rs_factors_training <- rs_factors_training %>%
  mutate(`LOG_SELLING_PRICE` = log(resale_price))
```

Replot Histogram of LOG_RESALE_PRICE

```{r}
ggplot(data=rs_factors_training, aes(x=`LOG_SELLING_PRICE`)) +
  geom_histogram(bins=20, color="black", fill="green")
```

The distribution is now less skewed after the transformation.

## 6.3 Various histogram distribution of structural variables

Here we will consider some structural factors involved such as floor_area_sqm, storey_order, remaining_lease_mths

```{r}
s_factor <- c("floor_area_sqm","remaining_lease_mths", "storey_order")
```

A list will be created to store the histogram structural variables

```{r}
s_factor_hist_list <- vector(mode = "list", length = length(s_factor))
for (i in 1:length(s_factor)) {
  hist_plot <- ggplot(rs_factors_training, aes_string(x = s_factor[[i]])) +
    geom_histogram(color="firebrick", fill = "light coral") +
    labs(title = s_factor[[i]]) +
    theme(plot.title = element_text(size = 10),
          axis.title = element_blank())
  
  s_factor_hist_list[[i]] <- hist_plot
}
```

Plotting histogram to visualise distribution of structural variables

```{r}
ggarrange(plotlist = s_factor_hist_list,
          ncol = 2,
          nrow = 2)
```

From the histogram results, only floor_area_sqm seem to resemble a normal distribution. The histogram of storey_order is skewed towards the right which means that the resale HDBs in this period and flat_type are generally on the lower levels. For the remaining_lease_mths, there are a few pattern found of popular transactions involving \~ 600 to 800 months and \~ 1100 months. This shows that more transactions are done on resale flats that have minimum 66 years lease remaining.

## 6.4 Various histogram of location variables

Extraction of location factors column names to plot

```{r}
l_factor <- c("PROX_CBD", "PROX_ELDERLYCARE", "PROX_HAWKER", "PROX_MRT", "PROX_PARK", "PROX_GOOD_PRISCH", "PROX_MALL", "PROX_CHAS",
              "PROX_SUPERMARKET", "WITHIN_350M_KINDERGARTEN", "WITHIN_350M_CHILDCARE", "WITHIN_350M_BUS", "WITHIN_1KM_PRISCH")
```

Similarly, a list will be created to store the histogram location variables

```{r}
l_factor_hist_list <- vector(mode = "list", length = length(l_factor))
for (i in 1:length(l_factor)) {
  hist_plot <- ggplot(rs_factors_training, aes_string(x = l_factor[[i]])) +
    geom_histogram(color="midnight blue", fill = "light sky blue") +
    labs(title = l_factor[[i]]) +
    theme(plot.title = element_text(size = 10),
          axis.title = element_blank())
  
  l_factor_hist_list[[i]] <- hist_plot
}
```

Plotting histogram to visualise distribution of location variables

```{r}
ggarrange(plotlist = l_factor_hist_list,
          ncol = 3,
          nrow = 5)
```

PROX_GOOD_PRISCH have some patterns involved whereby there more transactions happening within the proximity of approximately 3km. PROX_CBD resemble somewhat a normal distribution.

Other variables like PROX_SUPERMARKET, PROX_HAWKER, PROX_MRT, PROX_MALL, PROX_CHAS, WITHIN_350M_KINDERGARTEN, WITHIN_350M_CHILDCARE is skewed towards the right. This shows that there are more resale transactions involved and residents prefer to stay near resale flats with these amenities.

## 6.5 Statistical Point Map

```{r}
tmap_mode("view")
tm_shape(rs_factors_training) +  
  tm_dots(col = "resale_price",
          alpha = 0.6,
          style="quantile") +
  tm_view(set.zoom.limits = c(11,14)) +
tm_basemap("OpenStreetMap")

tmap_mode("plot")
```

From the interactive map, we can see that 3 room resale HDBs around the North-East, South and Central tend to have higher resale prices which is indicated by the darker orange points. This is in comparison to the lighter yellow points concentrated around the North and East area.

# 7. Data Sampling of Training and Testing Data

The entire data are split into training and test data sets with 2021-01 to 2022-12 being the training set and 2023-01 to 2023-02 being the testing set. This can be done by using initial_split() of rsample package. Rsample is one of the package of tidymodels.

Due to time constraint, 6 months (2022-07 to 2022-12) of dataset will be used to perform the training instead.s

```{r}
set.seed(1234)
rs_coords_sf <-  filter(rs_factors_rds_training,flat_type == "3 ROOM") %>% 
              filter(month >= "2022-07" & month <= "2023-02")
resale_split <- initial_split(rs_coords_sf[,8:26], 
                              prop = 7.5/10,)

train_data <- training(resale_split)
test_data <- testing(resale_split)
```

## 7.1 Computing Correlation Matrix

```{r}
rs_coords_sf_nogeo <- rs_coords_sf %>%
  st_drop_geometry()
corrplot::corrplot(cor(rs_coords_sf_nogeo[, 14:26]), 
                   diag = FALSE, 
                   order = "AOE",
                   tl.pos = "td", 
                   tl.cex = 0.5, 
                   method = "number", 
                   type = "upper")
```

The correlation matrix above shows that all the correlation values are below 0.8 with the exception of WITHIN_350M_CHILDCARE that has a value of 0.92. This shows that WITHIN_350M_CHILDCARE has a sign of multicolinearity with WTIHIN_350M_KINDERGARTEN

## 7.2 Building a non-spatial multiple linear regression

```{r}
price_mlr <- lm(resale_price ~ floor_area_sqm +
                  remaining_lease_mths +
                  PROX_CBD + PROX_ELDERLYCARE + PROX_HAWKER +
                  PROX_MRT + PROX_PARK + PROX_MALL + 
                  PROX_SUPERMARKET + WITHIN_350M_KINDERGARTEN +
                  WITHIN_350M_CHILDCARE + WITHIN_350M_BUS +
                  WITHIN_1KM_PRISCH,
                data=train_data)
summary(price_mlr)
```

## 7.3 Geographically Weighted Regression Predictive Method

Calibrating a model to predict HDB resale price by using geographically weighted regression method of GWmodel package.

Converting the sf data.frame to SpatialPointDataFrame

```{r}
train_data_sp <- as_Spatial(train_data)
train_data_sp
```

Computing adaptive bandwidth

Next, bw.gwr() of GWmodel package will be used to determine the optimal bandwidth to be used

```{r}
bw_adaptive <- bw.gwr(resale_price ~ floor_area_sqm +
                  remaining_lease_mths +
                  PROX_CBD + PROX_ELDERLYCARE + PROX_HAWKER +
                  PROX_MRT + PROX_PARK + PROX_MALL + 
                  PROX_SUPERMARKET + WITHIN_350M_KINDERGARTEN +
                  WITHIN_350M_CHILDCARE + WITHIN_350M_BUS +
                  WITHIN_1KM_PRISCH,
                  data=train_data_sp,
                  approach="CV",
                  kernel="gaussian",
                  adaptive=TRUE,
                  longlat=FALSE)
```

The result shows that 29 neighbour points will be the optimal bandwidth to be used if adaptive bandwidth is used for this data set.

Constructing the adaptive bandwidth GWR model

```{r}
gwr_adaptive <- gwr.basic(formula = resale_price ~
                            floor_area_sqm +
                            remaining_lease_mths + PROX_CBD + 
                            PROX_ELDERLYCARE + PROX_HAWKER +
                            PROX_MRT + PROX_PARK + PROX_MALL + 
                            PROX_SUPERMARKET + WITHIN_350M_KINDERGARTEN +
                            WITHIN_350M_CHILDCARE + WITHIN_350M_BUS +
                            WITHIN_1KM_PRISCH,
                          data=train_data_sp,
                          bw=bw_adaptive, 
                          kernel = 'gaussian', 
                          adaptive=TRUE,
                          longlat = FALSE)

gwr_adaptive
```

## 7.4 Preparing Coordinates Data

The code chunk below extract the x,y coordinates of the full, training and test data sets.

```{r}
coords <- st_coordinates(rs_coords_sf)
coords_train <- st_coordinates(train_data)
coords_test <- st_coordinates(test_data)
```

Dropping Geometry Field

We will drop geometry column of the sf data.frame by using st_drop_geometry() of sf package.

```{r}
train_data <- train_data %>% 
  st_drop_geometry()
```

## 7.5 Calibrating Random Forest Model

In this section, calibration of a model to predict HDB resale price will be done by using random forest function of ranger package.

```{r}
set.seed(1234)
rf <- ranger(resale_price ~ floor_area_sqm + 
               remaining_lease_mths + PROX_CBD + PROX_ELDERLYCARE + 
               PROX_HAWKER + PROX_MRT + PROX_PARK + PROX_MALL + 
               PROX_SUPERMARKET + WITHIN_350M_KINDERGARTEN +
               WITHIN_350M_CHILDCARE + WITHIN_350M_BUS + 
               WITHIN_1KM_PRISCH,
             data=train_data)
```

```{r}
print(rf)
```

## 7.6 Calibrating Geographical Random Forest Model

In this section, calibration of a model to predict HDB resale price will be done by using grf() of SpatialML package.

### 7.6.1 Calibrating using training data.

The code chunk below calibrate a geographic random forest model by using grf() of SpatialML package.

```{r}
set.seed(1234)
gwRF_adaptive <- grf(formula = resale_price ~ floor_area_sqm +
                       remaining_lease_mths + PROX_CBD + PROX_ELDERLYCARE +
                       PROX_HAWKER + PROX_MRT + PROX_PARK + PROX_MALL +
                       PROX_SUPERMARKET + WITHIN_350M_KINDERGARTEN +
                       WITHIN_350M_CHILDCARE + WITHIN_350M_BUS +
                       WITHIN_1KM_PRISCH,
                     dframe=train_data, 
                     bw=29,
                     kernel="adaptive",
                     coords=coords_train)
```

```{r}
write_rds(gwRF_adaptive, "Data/rds/gwRF_adaptive.rds")
gwRF_adaptive <- read_rds("Data/rds/gwRF_adaptive.rds")
```

### 7.6.2 Predicting using Test Data

The code chunk below will be used to combine the test data with its corresponding coordinates data.

Preparing the test data

```{r}
test_data <- cbind(test_data, coords_test) %>%
  st_drop_geometry()
```

Predicting with test data

Next, predict.grf() of spatialML package will be used to predict the resale value by using the test data and gwRF_adaptive model calibrated earlier.

```{r}
gwRF_pred <- predict.grf(gwRF_adaptive, 
                           test_data, 
                           x.var.name="X",
                           y.var.name="Y", 
                           local.w=1,
                           global.w=0)
```

Before moving on, let us save the output into rds file for future use.

```{r}
GRF_pred <- write_rds(gwRF_pred, "Data/rds/GRF_pred.rds")
GRF_pred
```

Converting the predicting output into a data frame

The output of the predict.grf() is a vector of predicted values. It is wiser to convert it into a data frame for further visualisation and analysis.

```{r}
GRF_pred <- read_rds("Data/rds/GRF_pred.rds")
GRF_pred_df <- as.data.frame(GRF_pred)
```

In the code chunk below, cbind() is used to append the predicted values onto test_data

```{r}
test_data_p <- cbind(test_data, GRF_pred_df)
```

```{r}
write_rds(test_data_p, "Data/rds/test_data_p.rds")
```

### 7.6.3 Calculating Root Mean Square Error

The root mean square error (RMSE) allows us to measure how far predicted values are from observed values in a regression analysis. In the code chunk below, rmse() of Metrics package is used to compute the RMSE.

```{r}
rmse(test_data_p$resale_price, 
     test_data_p$GRF_pred)
```

### 7.6.4 Visualising the predicted values

Alternatively, scatterplot can be used to visualise the actual resale price and the predicted resale price by using the code chunk below.

```{r}
ggplot(data = test_data_p,
       aes(x = GRF_pred,
           y = resale_price)) +
  geom_point()
```

# 8. References

-   Take Home Exercise 3 (Nor Aisyah Binte Ajit) - https://aisyahajit2018-is415.netlify.app/posts/2021-11-07-take-home-exercise-3/

-   In Class Exercise 8 (Professor Kam) - https://is415-gaa-tskam.netlify.app/in-class_ex/in-class_ex08/in-class_ex08_gwr

-   In Class Exercise 9 (Professor Kam) - https://is415-gaa-tskam.netlify.app/in-class_ex/in-class_ex09/in-class_ex09_gwml

Code chunks are reviewed and referenced from senior Nor Aisyah and Professor Kam in class exercises. End of take home exercise 3.
